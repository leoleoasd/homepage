@inproceedings{ubicomp2024,
  title     = {RECOVER: A Large Language Model-based Remote Patient Monitoring System for Postoperative GI Cancer Patients},
  author    = {
               Ziqi Yang* and
               Yuxuan Lu* and
               Jennifer Bagdasarian and
               Vedant Das Swain and
               Ritu Agarwal and
               Collin Campbell and
               Waddah Al-Refaire and
               Dr Jehan El-Bayoumi and
               Guodong (Gordon) Gao and
               nawar shara and
               Dakuo Wang and
               Bingsheng Yao
               },
  year      = {2024},
  booktitle = {Submission to IMWUT 2024},
  abbr      = {In Submission}
}


@@inproceedings{scisparkstory2024,
  title     = {SciSpark: An Interactive Storytelling System for Young Children's Science Education with Large Language Model (LLM) and Retrieval-Augmented Generation (RAG)},
  author    = {Jiaju Chen and
               Yuxuan Lu and
               Bingsheng Yao and
               Qianwen Wang and
               Ying Xu and
               Dakuo Wang and
               Yuling Sun},
  year      = {2024},
  booktitle = {Submission to UIST 2024},
  abbr      = {In Submission}
}

@article{lu2023human,
  title    = {Human Still Wins over LLM: An Empirical Study of Active Learning on Domain-Specific Annotation Tasks},
  author   = {Yuxuan Lu and Bingsheng Yao and Shao Zhang and Yun Wang and Peng Zhang and Tun Lu and Toby Jia-Jun Li and Dakuo Wang},
  year     = {2023},
  journal  = {arXiv preprint arXiv:2311.09825},
  abstract = {Large Language Models (LLMs) have demonstrated considerable advances, and several claims have been made about their exceeding human performance. However, in real-world tasks, domain knowledge is often required. Low-resource learning methods like Active Learning (AL) have been proposed to tackle the cost of domain expert annotation, raising this question: Can LLMs surpass compact models trained with expert annotations in domain-specific tasks? In this work, we conduct an empirical experiment on four datasets from three different domains comparing SOTA LLMs with small models trained on expert annotations with AL. We found that small models can outperform GPT-3.5 with a few hundreds of labeled data, and they achieve higher or similar performance with GPT-4 despite that they are hundreds time smaller. Based on these findings, we posit that LLM predictions can be used as a warmup method in real-world applications and human experts remain indispensable in tasks involving data annotation driven by domain-specific knowledge.},
  pdf      = {https://arxiv.org/abs/2311.09825},
  abbr     = {arXiv},
  arxiv={2311.09825}
}

@article{chen2023fairytalecqa,
  title    = {FairytaleCQA: Integrating a Commonsense Knowledge Graph into Children's Storybook Narratives},
  author   = {Jiaju Chen and Yuxuan Lu and Shao Zhang and Bingsheng Yao and Yuanzhe Dong and Ying Xu and Yunyao Li and Qianwen Wang and Dakuo Wang and Yuling Sun},
  year     = {2023},
  journal  = {arXiv preprint arXiv:2311.09756},
  abstract = {AI models (including LLM) often rely on narrative question-answering (QA) datasets to provide customized QA functionalities to support downstream children education applications; however, existing datasets only include QA pairs that are grounded within the given storybook content, but children can learn more when teachers refer the storybook content to real-world knowledge (e.g., commonsense knowledge). We introduce the FairytaleCQA dataset, which is annotated by children education experts, to supplement 278 storybook narratives with educationally appropriate commonsense knowledge. The dataset has 5,868 QA pairs that not only originate from the storybook narrative but also contain the commonsense knowledge grounded by an external knowledge graph (i.e., ConceptNet). A follow-up experiment shows that a smaller model (T5-large) fine-tuned with FairytaleCQA reliably outperforms much larger prompt-engineered LLM (e.g., GPT-4) in this new QA-pair generation task (QAG). This result suggests that: 1) our dataset brings novel challenges to existing LLMs, and 2) human experts' data annotation are still critical as they have much nuanced knowledge that LLMs do not know in the children educational domain.},
  pdf      = {https://arxiv.org/abs/2311.09756},
  abbr     = {arXiv},
  arxiv={2311.09756}
}

@inproceedings{geoscience2024,
  title     = {From Dark Data to Open Data: Challenges and Practices for Data Integrators of Data-Driven Open Science Projects in Geoscience},
  author    = {Shao Zhang and
               Shihan Fu and
               Bin Lu and
               Yuxuan Lu and
               Toby Jia-Jun Li and
               Dakuo Wang and
               Ying Wen and
               Xinbing Wang and
               Chenghu Zhou},
  year      = {2024},
  booktitle = {Submission to CSCW 2025},
  abbr      = {In Submission}
}

@inproceedings{anonymous2024storyspark,
  title     = {StorySpark: Expert-Annotated {QA} Pairs with Real-World Knowledge for Children Storytelling},
  author    = {Jiaju Chen and Yuxuan Lu and Shao Zhang and Bingsheng Yao and Yuanzhe Dong and Ying Xu and Yunyao Li and Qianwen Wang and Dakuo Wang and Yuling Sun},
  booktitle = {Submission to EMNLP 2024},
  year      = {2024},
  abbr      = {In Submission}
}

@inproceedings{anonymous2024exploring,
  title     = {Exploring Domain Adaptation with {LLM}s for Real-World Augmented Question Answer Generation ({RA}-{QAG}) in Children Storytelling},
  author    = {Jiaju Chen and Yuxuan Lu and Shao Zhang and Bingsheng Yao and Yuanzhe Dong and Ying Xu and Yunyao Li and Qianwen Wang and Dakuo Wang and Yuling Sun},
  booktitle = {Submission to EMNLP 2024},
  year      = {2024},
  abbr      = {In Submission}
}

@inproceedings{anonymous2024alerts,
  title     = {{ALERTS}: Active Learning and Ensemble {LLM} Real-Time Switch for Real-World Data Drift Challenges},
  author    = {Yuxuan Lu and Bingsheng Yao and Shao Zhang and Yisi Sang and Yun Wang and Hansu Gu and Peng Zhang and Tun Lu and Toby Jia-Jun Li and Dakuo Wang },
  booktitle = {Submission to EMNLP 2024},
  year      = {2024},
  abbr      = {In Submission}
}