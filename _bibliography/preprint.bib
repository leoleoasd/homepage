@article{lu2023human,
  title    = {Human Still Wins over LLM: An Empirical Study of Active Learning on Domain-Specific Annotation Tasks},
  author   = {Yuxuan Lu and Bingsheng Yao and Shao Zhang and Yun Wang and Peng Zhang and Tun Lu and Toby Jia-Jun Li and Dakuo Wang},
  year     = {2023},
  journal  = {arXiv preprint arXiv:2311.09825},
  abstract = {Large Language Models (LLMs) have demonstrated considerable advances, and several claims have been made about their exceeding human performance. However, in real-world tasks, domain knowledge is often required. Low-resource learning methods like Active Learning (AL) have been proposed to tackle the cost of domain expert annotation, raising this question: Can LLMs surpass compact models trained with expert annotations in domain-specific tasks? In this work, we conduct an empirical experiment on four datasets from three different domains comparing SOTA LLMs with small models trained on expert annotations with AL. We found that small models can outperform GPT-3.5 with a few hundreds of labeled data, and they achieve higher or similar performance with GPT-4 despite that they are hundreds time smaller. Based on these findings, we posit that LLM predictions can be used as a warmup method in real-world applications and human experts remain indispensable in tasks involving data annotation driven by domain-specific knowledge.},
  pdf      = {https://arxiv.org/abs/2311.09825},
  abbr     = {arXiv},
  arxiv={2311.09825}
}

@inproceedings{geoscience2024,
  title     = {From Dark Data to Open Data: Challenges and Practices for Data Integrators of Data-Driven Open Science Projects in Geoscience},
  author    = {Shao Zhang and
               Shihan Fu and
               Bin Lu and
               Yuxuan Lu and
               Toby Jia-Jun Li and
               Dakuo Wang and
               Ying Wen and
               Xinbing Wang and
               Chenghu Zhou},
  year      = {2024},
  booktitle = {Submission to CSCW 2025},
  abbr      = {In Submission}
}

@inproceedings{anonymous2024exploring,
  title     = {Exploring Domain Adaptation with {LLM}s for Real-World Augmented Question Answer Generation ({RA}-{QAG}) in Children Storytelling},
  author    = {Jiaju Chen and Yuxuan Lu and Shao Zhang and Bingsheng Yao and Yuanzhe Dong and Ying Xu and Yunyao Li and Qianwen Wang and Dakuo Wang and Yuling Sun},
  booktitle = {Submission to EMNLP 2024},
  year      = {2024},
  abbr      = {In Submission}
}

@inproceedings{anonymous2024alerts,
  title     = {{ALERTS}: Active Learning and Ensemble {LLM} Real-Time Switch for Real-World Data Drift Challenges},
  author    = {Yuxuan Lu and Bingsheng Yao and Shao Zhang and Yisi Sang and Yun Wang and Hansu Gu and Peng Zhang and Tun Lu and Toby Jia-Jun Li and Dakuo Wang },
  booktitle = {Submission to EMNLP 2024},
  year      = {2024},
  abbr      = {In Submission}
}

@misc{luUXAgentSystemSimulating2025,
  title         = {{{UXAgent}}: {{A System}} for {{Simulating Usability Testing}} of {{Web Design}} with {{LLM Agents}}},
  shorttitle    = {{{UXAgent}}},
  author        = {Lu, Yuxuan and Yao, Bingsheng and Gu, Hansu and Huang, Jing and Wang, Jessie and Li, Yang and Gesi, Jiri and He, Qi and Li, Toby Jia-Jun and Wang, Dakuo},
  year          = {2025},
  month         = apr,
  number        = {arXiv:2504.09407},
  eprint        = {2504.09407},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2504.09407},
  urldate       = {2025-04-30},
  abstract      = {Usability testing is a fundamental research method that user experience (UX) researchers use to evaluate and iterate a web design, but{\textbackslash}textbf\{ how to evaluate and iterate the usability testing study design \} itself? Recent advances in Large Language Model-simulated Agent ({\textbackslash}textbf\{LLM Agent\}) research inspired us to design {\textbackslash}textbf\{UXAgent\} to support UX researchers in evaluating and reiterating their usability testing study design before they conduct the real human-subject study. Our system features a Persona Generator module, an LLM Agent module, and a Universal Browser Connector module to automatically generate thousands of simulated users to interactively test the target website. The system also provides an Agent Interview Interface and a Video Replay Interface so that the UX researchers can easily review and analyze the generated qualitative and quantitative log data. Through a heuristic evaluation, five UX researcher participants praised the innovation of our system but also expressed concerns about the future of LLM Agent usage in UX studies.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Computation and Language,Computer Science - Human-Computer Interaction}
}

@misc{wangAgentAAutomatedScalable2025,
  title         = {{{AgentA}}/{{B}}: {{Automated}} and {{Scalable Web A}}/{{BTesting}} with {{Interactive LLM Agents}}},
  shorttitle    = {{{AgentA}}/{{B}}},
  author        = {Wang, Dakuo and Hsu, Ting-Yao and Lu, Yuxuan and Gu, Hansu and Cui, Limeng and Xie, Yaochen and Headean, William and Yao, Bingsheng and Veeragouni, Akash and Liu, Jiapeng and Nag, Sreyashi and Wang, Jessie},
  year          = {2025},
  month         = apr,
  number        = {arXiv:2504.09723},
  eprint        = {2504.09723},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2504.09723},
  urldate       = {2025-04-30},
  abstract      = {A/B testing experiment is a widely adopted method for evaluating UI/UX design decisions in modern web applications. Yet, traditional A/B testing remains constrained by its dependence on the large-scale and live traffic of human participants, and the long time of waiting for the testing result. Through formative interviews with six experienced industry practitioners, we identified critical bottlenecks in current A/B testing workflows. In response, we present AgentA/B, a novel system that leverages Large Language Model-based autonomous agents (LLM Agents) to automatically simulate user interaction behaviors with real webpages. AgentA/B enables scalable deployment of LLM agents with diverse personas, each capable of navigating the dynamic webpage and interactively executing multi-step interactions like search, clicking, filtering, and purchasing. In a demonstrative controlled experiment, we employ AgentA/B to simulate a between-subject A/B testing with 1,000 LLM agents Amazon.com, and compare agent behaviors with real human shopping behaviors at a scale. Our findings suggest AgentA/B can emulate human-like behavior patterns.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Computation and Language,Computer Science - Human-Computer Interaction}
}

@misc{yangRECOVERDesigningLarge2025,
  title         = {{{RECOVER}}: {{Designing}} a {{Large Language Model-based Remote Patient Monitoring System}} for {{Postoperative Gastrointestinal Cancer Care}}},
  shorttitle    = {{{RECOVER}}},
  author        = {Yang, Ziqi and Lu, Yuxuan and Bagdasarian, Jennifer and Swain, Vedant Das and Agarwal, Ritu and Campbell, Collin and {Al-Refaire}, Waddah and {El-Bayoumi}, Jehan and Gao, Guodong and Wang, Dakuo and Yao, Bingsheng and Shara, Nawar},
  year          = {2025},
  month         = feb,
  number        = {arXiv:2502.05740},
  eprint        = {2502.05740},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2502.05740},
  urldate       = {2025-04-30},
  abstract      = {Cancer surgery is a key treatment for gastrointestinal (GI) cancers, a group of cancers that account for more than 35\% of cancer-related deaths worldwide, but postoperative complications are unpredictable and can be life-threatening. In this paper, we investigate how recent advancements in large language models (LLMs) can benefit remote patient monitoring (RPM) systems through clinical integration by designing RECOVER, an LLM-powered RPM system for postoperative GI cancer care. To closely engage stakeholders in the design process, we first conducted seven participatory design sessions with five clinical staff and interviewed five cancer patients to derive six major design strategies for integrating clinical guidelines and information needs into LLM-based RPM systems. We then designed and implemented RECOVER, which features an LLM-powered conversational agent for cancer patients and an interactive dashboard for clinical staff to enable efficient postoperative RPM. Finally, we used RECOVER as a pilot system to assess the implementation of our design strategies with four clinical staff and five patients, providing design implications by identifying crucial design elements, offering insights on responsible AI, and outlining opportunities for future LLM-powered RPM systems.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction}
}
