@article{abdelghaniGPT3drivenPedagogicalAgents2023,
  title = {{{GPT-3-driven}} Pedagogical Agents for Training Children's Curious Question-Asking Skills},
  author = {Abdelghani, Rania and Wang, Yen-Hsiang and Yuan, Xingdi and Wang, Tong and Lucas, Pauline and Sauz{\'e}on, H{\'e}l{\`e}ne and Oudeyer, Pierre-Yves},
  year = {2023},
  month = jun,
  journal = {International Journal of Artificial Intelligence in Education},
  eprint = {2211.14228},
  primaryclass = {cs},
  issn = {1560-4292, 1560-4306},
  doi = {10.1007/s40593-023-00340-7},
  urldate = {2023-07-24},
  abstract = {In order to train children's ability to ask curiosity-driven questions, previous research has explored designing specific exercises relying on providing semantic and linguistic cues to help formulate such questions. But despite showing pedagogical efficiency, this method is still limited as it relies on generating the said cues by hand, which can be a very costly process. In this context, we propose to leverage advances in the natural language processing field (NLP) and investigate the efficiency of using a large language model (LLM) for automating the production of the pedagogical content of a curious question-asking (QA) training. We study generating the said content using the "prompt-based" method that consists of explaining the task to the LLM in natural text. We evaluate the output using human experts annotations and comparisons with hand-generated content. Results suggested indeed the relevance and usefulness of this content. We also conduct a field study in primary school (75 children aged 9-10), where we evaluate children's QA performance when having this training. We compare 3 types of content : 1) hand-generated content that proposes "closed" cues leading to predefined questions; 2) GPT-3-generated content that proposes the same type of cues; 3) GPT-3-generated content that proposes "open" cues leading to several possible questions. We see a similar QA performance between the two "closed" trainings (showing the scalability of the approach using GPT-3), and a better one for participants with the "open" training. These results suggest the efficiency of using LLMs to support children in generating more curious questions, using a natural language prompting approach that affords usability by teachers and other users not specialists of AI techniques. Furthermore, results also show that open-ended content may be more suitable for training curious question-asking skills.},
  archiveprefix = {arXiv}
}

@misc{ACL2025Demo,
  title = {{{ACL}} 2025 {{Demo Reviewers}}},
  journal = {OpenReview},
  url = {https://openreview.net/group?id=aclweb.org/ACL/2025/Demo/Reviewers#assigned-submissions},
  urldate = {2025-04-27},
  abstract = {Welcome to the OpenReview homepage for ACL 2025 Demo Reviewers},
  langid = {english}
}

@inproceedings{agarwalProcessAwareDecisionSupport2022,
  title = {A {{Process-Aware Decision Support System}} for {{Business Processes}}},
  booktitle = {Proceedings of the 28th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Agarwal, Prerna and Gao, Buyu and Huo, Siyu and Reddy, Prabhat and Dechu, Sampath and Obeidi, Yazan and Muthusamy, Vinod and Isahagian, Vatche and Carbajales, Sebastian},
  year = {2022},
  month = aug,
  series = {{{KDD}} '22},
  pages = {2673--2681},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3534678.3539088},
  urldate = {2023-02-01},
  abstract = {Business processes in workflows comprise of an ordered sequence of tasks and decisions to accomplish certain business goals. Each decision point requires the input of a decision-maker to distill complex case information and make an optimal decision given their experience, organizational policy, and external contexts. Overlooking some of the essential factors or lack of knowledge can impact the throughput and business outcomes. Therefore, we propose an end-to-end automated decision support system with explanation for business processes. The system uses the proposed process-aware feature engineering methodology that extracts features from process and business data attributes. The system helps a decision-maker to make quick and quality decisions by predicting the decision and providing an explanation of the factors which led to the prediction. We provide offline and online training methods robust to data drift that can also incorporate user feedback. The system also support predictions with live instance data i.e., allow decision-makers to conduct trials on current data instance by modifying its business data attribute values. We evaluate our system on real-world and synthetic datasets and benchmark the performance, achieving an average of 15\% improvement over baselines.},
  isbn = {978-1-4503-9385-0}
}

@inproceedings{aggarwalExplanationsCommonsenseQANew2021,
  title = {Explanations for {{CommonsenseQA}}: {{New Dataset}} and {{Models}}},
  shorttitle = {Explanations for {{CommonsenseQA}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Aggarwal, Shourya and Mandowara, Divyanshu and Agrawal, Vishwajeet and Khandelwal, Dinesh and Singla, Parag and Garg, Dinesh},
  editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
  year = {2021},
  month = aug,
  pages = {3050--3065},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.acl-long.238},
  urldate = {2023-11-07},
  abstract = {CommonsenseQA (CQA) (Talmor et al., 2019) dataset was recently released to advance the research on common-sense question answering (QA) task. Whereas the prior work has mostly focused on proposing QA models for this dataset, our aim is to retrieve as well as generate explanation for a given (question, correct answer choice, incorrect answer choices) tuple from this dataset. Our explanation definition is based on certain desiderata, and translates an explanation into a set of positive and negative common-sense properties (aka facts) which not only explain the correct answer choice but also refute the incorrect ones. We human-annotate a first-of-its-kind dataset (called ECQA) of positive and negative properties, as well as free-flow explanations, for 11K QA pairs taken from the CQA dataset. We propose a latent representation based property retrieval model as well as a GPT-2 based property generation model with a novel two step fine-tuning procedure. We also propose a free-flow explanation generation model. Extensive experiments show that our retrieval model beats BM25 baseline by a relative gain of 100\% in F\_1 score, property generation model achieves a respectable F\_1 score of 36.4, and free-flow generation model achieves a similarity score of 61.9, where last two scores are based on a human correlated semantic similarity metric.}
}

@inproceedings{agrawalLargeLanguageModels2022,
  title = {Large Language Models Are Few-Shot Clinical Information Extractors},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Agrawal, Monica and Hegselmann, Stefan and Lang, Hunter and Kim, Yoon and Sontag, David},
  editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
  year = {2022},
  month = dec,
  pages = {1998--2022},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.emnlp-main.130},
  urldate = {2023-11-29},
  abstract = {A long-running goal of the clinical NLP community is the extraction of important variables trapped in clinical notes. However, roadblocks have included dataset shift from the general domain and a lack of public clinical corpora and annotations. In this work, we show that large language models, such as InstructGPT (Ouyang et al., 2022), perform well at zero- and few-shot information extraction from clinical text despite not being trained specifically for the clinical domain. Whereas text classification and generation performance have already been studied extensively in such models, here we additionally demonstrate how to leverage them to tackle a diverse set of NLP tasks which require more structured outputs, including span identification, token-level sequence classification, and relation extraction. Further, due to the dearth of available data to evaluate these systems, we introduce new datasets for benchmarking few-shot clinical information extraction based on a manual re-annotation of the CASI dataset (Moon et al., 2014) for new tasks. On the clinical extraction tasks we studied, the GPT-3 systems significantly outperform existing zero- and few-shot baselines.}
}

@misc{aguirreSelectingShotsDemographic2023,
  title = {Selecting {{Shots}} for {{Demographic Fairness}} in {{Few-Shot Learning}} with {{Large Language Models}}},
  author = {Aguirre, Carlos and Sasse, Kuleen and Cachola, Isabel and Dredze, Mark},
  year = {2023},
  month = nov,
  number = {arXiv:2311.08472},
  eprint = {2311.08472},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2311.08472},
  urldate = {2023-11-18},
  abstract = {Recently, work in NLP has shifted to few-shot (in-context) learning, with large language models (LLMs) performing well across a range of tasks. However, while fairness evaluations have become a standard for supervised methods, little is known about the fairness of LLMs as prediction systems. Further, common standard methods for fairness involve access to models weights or are applied during finetuning, which are not applicable in few-shot learning. Do LLMs exhibit prediction biases when used for standard NLP tasks? In this work, we explore the effect of shots, which directly affect the performance of models, on the fairness of LLMs as NLP classification systems. We consider how different shot selection strategies, both existing and new demographically sensitive methods, affect model fairness across three standard fairness datasets. We discuss how future work can include LLM fairness evaluations.},
  archiveprefix = {arXiv}
}

@inproceedings{aherUsingLargeLanguage2023,
  title = {Using {{Large Language Models}} to {{Simulate Multiple Humans}} and {{Replicate Human Subject Studies}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Aher, Gati V. and Arriaga, Rosa I. and Kalai, Adam Tauman},
  year = {2023},
  month = jul,
  pages = {337--371},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v202/aher23a.html},
  urldate = {2024-09-05},
  abstract = {We introduce a new type of test, called a Turing Experiment (TE), for evaluating to what extent a given language model, such as GPT models, can simulate different aspects of human behavior. A TE can also reveal consistent distortions in a language model's simulation of a specific human behavior. Unlike the Turing Test, which involves simulating a single arbitrary individual, a TE requires simulating a representative sample of participants in human subject research. We carry out TEs that attempt to replicate well-established findings from prior studies. We design a methodology for simulating TEs and illustrate its use to compare how well different language models are able to reproduce classic economic, psycholinguistic, and social psychology experiments: Ultimatum Game, Garden Path Sentences, Milgram Shock Experiment, and Wisdom of Crowds. In the first three TEs, the existing findings were replicated using recent models, while the last TE reveals a ``hyper-accuracy distortion'' present in some language models (including ChatGPT and GPT-4), which could affect downstream applications in education and the arts.},
  langid = {english}
}

@misc{aherUsingLargeLanguage2023a,
  title = {Using {{Large Language Models}} to {{Simulate Multiple Humans}} and {{Replicate Human Subject Studies}}},
  author = {Aher, Gati and Arriaga, Rosa I. and Kalai, Adam Tauman},
  year = {2023},
  month = jul,
  number = {arXiv:2208.10264},
  eprint = {2208.10264},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2208.10264},
  urldate = {2024-09-05},
  abstract = {We introduce a new type of test, called a Turing Experiment (TE), for evaluating to what extent a given language model, such as GPT models, can simulate different aspects of human behavior. A TE can also reveal consistent distortions in a language model's simulation of a specific human behavior. Unlike the Turing Test, which involves simulating a single arbitrary individual, a TE requires simulating a representative sample of participants in human subject research. We carry out TEs that attempt to replicate well-established findings from prior studies. We design a methodology for simulating TEs and illustrate its use to compare how well different language models are able to reproduce classic economic, psycholinguistic, and social psychology experiments: Ultimatum Game, Garden Path Sentences, Milgram Shock Experiment, and Wisdom of Crowds. In the first three TEs, the existing findings were replicated using recent models, while the last TE reveals a "hyper-accuracy distortion" present in some language models (including ChatGPT and GPT-4), which could affect downstream applications in education and the arts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{ainslieCoLT5FasterLongRange2023,
  title = {{{CoLT5}}: {{Faster Long-Range Transformers}} with {{Conditional Computation}}},
  shorttitle = {{{CoLT5}}},
  author = {Ainslie, Joshua and Lei, Tao and {de Jong}, Michiel and Onta{\~n}{\'o}n, Santiago and Brahma, Siddhartha and Zemlyanskiy, Yury and Uthus, David and Guo, Mandy and {Lee-Thorp}, James and Tay, Yi and Sung, Yun-Hsuan and Sanghai, Sumit},
  year = {2023},
  month = mar,
  number = {arXiv:2303.09752},
  eprint = {2303.09752},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2303.09752},
  urldate = {2023-03-21},
  abstract = {Many natural language processing tasks benefit from long inputs, but processing long documents with Transformers is expensive -- not only due to quadratic attention complexity but also from applying feedforward and projection layers to every token. However, not all tokens are equally important, especially for longer documents. We propose CoLT5, a long-input Transformer model that builds on this intuition by employing conditional computation, devoting more resources to important tokens in both feedforward and attention layers. We show that CoLT5 achieves stronger performance than LongT5 with much faster training and inference, achieving SOTA on the long-input SCROLLS benchmark. Moreover, CoLT5 can effectively and tractably make use of extremely long inputs, showing strong gains up to 64k input length.},
  archiveprefix = {arXiv}
}

@misc{albalakEfficientOnlineData2023,
  title = {Efficient {{Online Data Mixing For Language Model Pre-Training}}},
  author = {Albalak, Alon and Pan, Liangming and Raffel, Colin and Wang, William Yang},
  year = {2023},
  month = dec,
  number = {arXiv:2312.02406},
  eprint = {2312.02406},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2312.02406},
  urldate = {2023-12-07},
  abstract = {The data used to pretrain large language models has a decisive impact on a model's downstream performance, which has led to a large body of work on data selection methods that aim to automatically determine the most suitable data to use for pretraining. Existing data selection methods suffer from slow and computationally expensive processes, a problem amplified by the increasing size of models and of pretraining datasets. Data mixing, on the other hand, reduces the complexity of data selection by grouping data points together and determining sampling probabilities across entire groups. However, data mixing proportions are typically fixed before training and therefore cannot adapt to changing training dynamics. To address these limitations, we develop an efficient algorithm for Online Data Mixing (ODM) that combines elements from both data selection and data mixing. Based on multi-armed bandit algorithms, our online approach optimizes the data mixing proportions during training. Remarkably, our method trains a model that reaches the final perplexity of the next best method with 19{\textbackslash}\% fewer training iterations, and improves performance on the 5-shot MMLU benchmark by 1.9\% relative accuracy, while adding negligible wall-clock time during pretraining.},
  archiveprefix = {arXiv}
}

@article{alemanCardiovascularDiseaseCancer2014,
  title = {Cardiovascular Disease after Cancer Therapy},
  author = {Aleman, Berthe M. P. and Moser, Elizabeth C. and Nuver, Janine and Suter, Thomas M. and Maraldo, Maja V. and Specht, Lena and Vrieling, Conny and Darby, Sarah C.},
  year = {2014},
  month = jun,
  journal = {European Journal of Cancer Supplements},
  series = {1st {{EORTC Cancer Survivorship Summit}}},
  volume = {12},
  number = {1},
  pages = {18--28},
  issn = {1359-6349},
  doi = {10.1016/j.ejcsup.2014.03.002},
  urldate = {2024-03-28},
  abstract = {Improvements in treatment and earlier diagnosis have both contributed to increased survival for many cancer patients. Unfortunately, many treatments carry a risk of late effects including cardiovascular diseases (CVDs), possibly leading to significant morbidity and mortality. In this paper we describe current knowledge of the cardiotoxicity arising from cancer treatments, outline gaps in knowledge, and indicate directions for future research and guideline development, as discussed during the 2014 Cancer Survivorship Summit organised by the European Organisation for Research and Treatment of Cancer (EORTC). Better knowledge is needed of the late effects of modern systemic treatments and of radiotherapy to critical structures of the heart, including the effect of both radiation dose and volume of the heart exposed. Research elucidating the extent to which treatments interact in causing CVD, and the mechanisms involved, as well as the extent to which treatments may increase CVD indirectly by increasing cardiovascular risk factors is also important. Systematic collection of data relating treatment details to late effects is needed, and great care is needed to obtain valid and generalisable results. Better knowledge of these cardiac effects will contribute to both primary and secondary prevention of late complications where exposure to cardiotoxic treatment is unavoidable. Also surrogate markers would help to identify patients at increased risk of cardiotoxicity. Evidence-based screening guidelines for CVD following cancer are also needed. Finally, risk prediction models should be developed to guide primary treatment choice and appropriate follow up after cancer treatment.}
}

@inproceedings{alemumogesMultiPerspectiveReasoningTransformers2021,
  title = {Multi-{{Perspective Reasoning Transformers}}},
  booktitle = {2021 13th {{International Conference}} on {{Machine Learning}} and {{Computing}}},
  author = {Alemu Moges, Dagmawi and Andre Niyongabo, Rubungo and Qu, Hong},
  year = {2021},
  pages = {503--508},
  doi = {10.1145/3457682.3457759}
}

@misc{AmazoncomBrevilleBES870XL,
  title = {Amazon.Com: {{Breville BES870XL Espresso Machine}}, {{One Size}}, {{Brushed Stainless Steel}}: {{Semi Automatic Pump Espresso Machines}}: {{Home}} \& {{Kitchen}}},
  url = {https://www.amazon.com/Breville-BES870XL-Barista-Express-Espresso/dp/B00CH9QWOU/ref=sr_1_1?crid=2YI186PH7YTSE&dib=eyJ2IjoiMSJ9.W2aF5LjcJbzPDIfm_STgtkOI8uOc5qzbtsYPx0gXhPA-HmKD8IVyqS34RwhW60PBpQUCTENYt5JFI3BYcOid-SGpUwzXUdM6Y_c-76L1pt_uaT-DqSPoo-rSKfKkDmRITkGqBLs2ZK9AC_yfAtFtV7SfqFguDbvvsS_aXr4VlNL8dh-lAb9MJCHV6LPcYd9UInndrbpNlXz7iawawZDVYlpVCi1k2xEDYzQrT-dGMm9jfOpdWxniFtiB7bgIxs4BnHQV2sSa8Qh7oKfZJ6YyuEtvTnxTtaPqIuVLdZw2p4I.yyiH7TsxJxiktxQ-zOtPn6BQ1b_06By5CikvQ_ADW44&dib_tag=se&keywords=breville%2Bexpress&qid=1732481621&sprefix=%2Caps%2C373&sr=8-1&th=1},
  urldate = {2024-11-24}
}

@misc{AmazonWebServices,
  title = {Amazon {{Web Services Sign-In}}},
  url = {https://signin.aws.amazon.com/signin?redirect_uri=https%3A%2F%2Fus-east-1.console.aws.amazon.com%2Fsagemaker%2Fhome%3FhashArgs%3D%2523%252Fnotebook-instances%252Fyuxuanlu%26isauthcode%3Dtrue%26oauthStart%3D1733346513602%26region%3Dus-east-1%26state%3DhashArgsFromTB_us-east-1_c7a9fc20e96f3c2e&client_id=arn%3Aaws%3Asignin%3A%3A%3Aconsole%2Fsagemaker&forceMobileApp=0&code_challenge=dzYjr1KLiJA4iOvfhWKDXMuLLrTVt7sr2hZQuyP1prM&code_challenge_method=SHA-256},
  urldate = {2024-12-04}
}

@inproceedings{amershiGuidelinesHumanAIInteraction2019,
  title = {Guidelines for {{Human-AI Interaction}}},
  booktitle = {Proceedings of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Amershi, Saleema and Weld, Dan and Vorvoreanu, Mihaela and Fourney, Adam and Nushi, Besmira and Collisson, Penny and Suh, Jina and Iqbal, Shamsi and Bennett, Paul N. and Inkpen, Kori and Teevan, Jaime and {Kikin-Gil}, Ruth and Horvitz, Eric},
  year = {2019},
  month = may,
  pages = {1--13},
  publisher = {ACM},
  address = {Glasgow Scotland Uk},
  doi = {10.1145/3290605.3300233},
  urldate = {2023-01-13},
  abstract = {Advances in artifcial intelligence (AI) frame opportunities and challenges for user interface design. Principles for humanAI interaction have been discussed in the human-computer interaction community for over two decades, but more study and innovation are needed in light of advances in AI and the growing uses of AI technologies in human-facing applications. We propose 18 generally applicable design guidelines for human-AI interaction. These guidelines are validated through multiple rounds of evaluation including a user study with 49 design practitioners who tested the guidelines against 20 popular AI-infused products. The results verify the relevance of the guidelines over a spectrum of interaction scenarios and reveal gaps in our knowledge, highlighting opportunities for further research. Based on the evaluations, we believe the set of design guidelines can serve as a resource to practitioners working on the design of applications and features that harness AI technologies, and to researchers interested in the further development of guidelines for human-AI interaction design.},
  isbn = {978-1-4503-5970-2},
  langid = {english}
}

@misc{AnatomySearchEngine,
  title = {The {{Anatomy}} of a {{Search Engine}}},
  url = {http://infolab.stanford.edu/~backrub/google.html},
  urldate = {2024-08-27}
}

@inproceedings{andreasenWhatHappenedRemote2007,
  title = {What Happened to Remote Usability Testing? An Empirical Study of Three Methods},
  shorttitle = {What Happened to Remote Usability Testing?},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Andreasen, Morten Sieker and Nielsen, Henrik Villemann and Schr{\o}der, Simon Ormholt and Stage, Jan},
  year = {2007},
  month = apr,
  series = {{{CHI}} '07},
  pages = {1405--1414},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1240624.1240838},
  urldate = {2024-09-13},
  abstract = {The idea of conducting usability tests remotely emerged ten years ago. Since then, it has been studied empirically, and some software organizations employ remote methods. Yet there are still few comparisons involving more than one remote method. This paper presents results from a systematic empirical comparison of three methods for remote usability testing and a conventional laboratory-based think-aloud method. The three remote methods are a remote synchronous condition, where testing is conducted in real time but the test monitor is separated spatially from the test subjects, and two remote asynchronous conditions, where the test monitor and the test subjects are separated both spatially and temporally. The results show that the remote synchronous method is virtually equivalent to the conventional method. Thereby, it has the potential to conveniently involve broader user groups in usability testing and support new development approaches. The asynchronous methods are considerably more time-consuming for the test subjects and identify fewer usability problems, yet they may still be worthwhile.},
  isbn = {978-1-59593-593-9}
}

@article{anwyl-irvineGorillaOurMidst2020,
  title = {Gorilla in Our Midst: {{An}} Online Behavioral Experiment Builder},
  shorttitle = {Gorilla in Our Midst},
  author = {{Anwyl-Irvine}, Alexander L. and Massonni{\'e}, Jessica and Flitton, Adam and Kirkham, Natasha and Evershed, Jo K.},
  year = {2020},
  month = feb,
  journal = {Behavior Research Methods},
  volume = {52},
  number = {1},
  pages = {388--407},
  issn = {1554-3528},
  doi = {10.3758/s13428-019-01237-x},
  urldate = {2024-09-13},
  abstract = {Behavioral researchers are increasingly conducting their studies online, to gain access to large and diverse samples that would be difficult to get in a laboratory environment. However, there are technical access barriers to building experiments online, and web browsers can present problems for consistent timing---an important issue with reaction-time-sensitive measures. For example, to ensure accuracy and test--retest reliability in presentation and response recording, experimenters need a working knowledge of programming languages such as JavaScript. We review some of the previous and current tools for online behavioral research, as well as how well they address the issues of usability and timing. We then present the Gorilla Experiment Builder (gorilla.sc), a fully tooled experiment authoring and deployment platform, designed to resolve many timing issues and make reliable online experimentation open and accessible to a wider range of technical abilities. To demonstrate the platform's aptitude for accessible, reliable, and scalable research, we administered a task with a range of participant groups (primary school children and adults), settings (without supervision, at home, and under supervision, in both schools and public engagement events), equipment (participant's own computer, computer supplied by the researcher), and connection types (personal internet connection, mobile phone 3G/4G). We used a simplified flanker task taken from the attentional network task (Rueda, Posner, \& Rothbart, 2004). We replicated the ``conflict network'' effect in all these populations, demonstrating the platform's capability to run reaction-time-sensitive experiments. Unresolved limitations of running experiments online are then discussed, along with potential solutions and some future features of the platform.},
  langid = {english},
  keywords = {Artificial Intelligence,Attentional control,Browser timing,Online methods,Online research,Remote testing,Timing accuracy}
}

@inproceedings{arndtMetaReinforcementLearning2020,
  title = {Meta {{Reinforcement Learning}} for {{Sim-to-real Domain Adaptation}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Arndt, Karol and Hazara, Murtaza and Ghadirzadeh, Ali and Kyrki, Ville},
  year = {2020},
  month = may,
  pages = {2725--2731},
  issn = {2577-087X},
  doi = {10.1109/ICRA40945.2020.9196540},
  urldate = {2025-02-14},
  abstract = {Modern reinforcement learning methods suffer from low sample efficiency and unsafe exploration, making it infeasible to train robotic policies entirely on real hardware. In this work, we propose to address the problem of sim-to-real domain transfer by using meta learning to train a policy that can adapt to a variety of dynamic conditions, and using a task-specific trajectory generation model to provide an action space that facilitates quick exploration. We evaluate the method by performing domain adaptation in simulation and analyzing the structure of the latent space during adaptation. We then deploy this policy on a KUKA LBR 4+ robot and evaluate its performance on a task of hitting a hockey puck to a target. Our method shows more consistent and stable domain adaptation than the baseline, resulting in better overall performance.},
  keywords = {Adaptation models,Heuristic algorithms,Learning (artificial intelligence),Robots,Task analysis,Training,Trajectory}
}

@misc{ArtificialIntelligenceHuman,
  title = {Artificial {{Intelligence}} for {{Human Computer Interaction}}: {{A Modern Approach}} {\textbar} {{SpringerLink}}},
  url = {https://link.springer.com/book/10.1007/978-3-030-82681-9},
  urldate = {2024-09-13}
}

@article{ASHABotLLMPoweredChatbot,
  title = {{{ASHABot}}: {{An LLM-Powered Chatbot}} to {{Support}} the {{Informational Needs}} of {{Community Health Workers}}},
  langid = {english}
}

@inproceedings{aubinlequereLLMsResearchTools2024,
  title = {{{LLMs}} as {{Research Tools}}: {{Applications}} and {{Evaluations}} in {{HCI Data Work}}},
  shorttitle = {{{LLMs}} as {{Research Tools}}},
  booktitle = {Extended {{Abstracts}} of the 2024 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Aubin Le Qu{\'e}r{\'e}, Marianne and Schroeder, Hope and Randazzo, Casey and Gao, Jie and Epstein, Ziv and Perrault, Simon Tangi and Mimno, David and Barkhuus, Louise and Li, Hanlin},
  year = {2024},
  month = may,
  series = {{{CHI EA}} '24},
  pages = {1--7},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3613905.3636301},
  urldate = {2024-09-12},
  abstract = {Large language models (LLMs) stand to reshape traditional methods of working with data. While LLMs unlock new and potentially useful ways of interfacing with data, their use in research processes requires methodological and critical evaluation. In this workshop, we seek to gather a community of HCI researchers interested in navigating the responsible integration of LLMs into data work: data collection, processing, and analysis. We aim to create an understanding of how LLMs are being used to work with data in HCI research, and document the early challenges and concerns that have arisen. Together, we will outline a research agenda on using LLMs as research tools to work with data by defining the open empirical and ethical evaluation questions and thus contribute to setting norms in the community. We believe CHI to be the ideal place to address these questions due to the methodologically diverse researcher attendees, the prevalence of HCI research on human interaction with new computing and data paradigms, and the community's sense of ethics and care. Insights from this forum can contribute to other research communities grappling with related questions.},
  isbn = {979-8-4007-0331-7}
}

@misc{AutomatedUsabilityTesting,
  title = {Automated {{Usability Testing Using HUI Analyzer}} {\textbar} {{IEEE Conference Publication}} {\textbar} {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/4483248?casa_token=E6mZYi7RsMIAAAAA:WGDWuQbMI4DBBIdAK4M_bFEQkTjnVJBIIiO5VAcCZNxMPCeIlVLfyy2qm4hoy9k3wU3nwCCF},
  urldate = {2024-09-11}
}

@misc{AutomatingGUITesting,
  title = {Automating {{GUI Testing}} with {{Image-Based Deep Reinforcement Learning}} {\textbar} {{IEEE Conference Publication}} {\textbar} {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9196452?casa_token=kkyAZ1hom8kAAAAA:rUBOQAyRa4Pqh06wUjZ1GsS54JKKPmq44Rhgg-bNOQ-ou-8aRoC-ODUzbCurlFSv2N_8_zbM},
  urldate = {2024-09-10}
}

@inproceedings{bachSnorkelDryBellCase2019,
  title = {Snorkel {{DryBell}}: {{A Case Study}} in {{Deploying Weak Supervision}} at {{Industrial Scale}}},
  shorttitle = {Snorkel {{DryBell}}},
  booktitle = {Proceedings of the 2019 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Bach, Stephen H. and Rodriguez, Daniel and Liu, Yintao and Luo, Chong and Shao, Haidong and Xia, Cassandra and Sen, Souvik and Ratner, Alex and Hancock, Braden and Alborzi, Houman and Kuchhal, Rahul and R{\'e}, Chris and Malkin, Rob},
  year = {2019},
  month = jun,
  pages = {362--375},
  publisher = {ACM},
  address = {Amsterdam Netherlands},
  doi = {10.1145/3299869.3314036},
  urldate = {2023-08-02},
  isbn = {978-1-4503-5643-5},
  langid = {english}
}

@article{bailCanGenerativeAI2024,
  title = {Can {{Generative AI}} Improve Social Science?},
  author = {Bail, Christopher A.},
  year = {2024},
  month = may,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {121},
  number = {21},
  pages = {e2314021121},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2314021121},
  urldate = {2024-07-05},
  abstract = {Generative AI that can produce realistic text, images, and other human-like outputs is currently transforming many different industries. Yet it is not yet known how such tools might influence social science research. I argue Generative AI has the potential to improve survey research, online experiments, automated content analyses, agent-based models, and other techniques commonly used to study human behavior. In the second section of this article, I discuss the many limitations of Generative AI. I examine how bias in the data used to train these tools can negatively impact social science research---as well as a range of other challenges related to ethics, replication, environmental impact, and the proliferation of low-quality research. I conclude by arguing that social scientists can address many of these limitations by creating open-source infrastructure for research on human behavior. Such infrastructure is not only necessary to ensure broad access to high-quality research tools, I argue, but also because the progress of AI will require deeper understanding of the social forces that guide human behavior.}
}

@inproceedings{bakerAutomatedUsabilityTesting2008,
  title = {Automated {{Usability Testing Using HUI Analyzer}}},
  booktitle = {19th {{Australian Conference}} on {{Software Engineering}} (Aswec 2008)},
  author = {Baker, Simon and Au, Fiora and Dobbie, Gillian and Warren, Ian},
  year = {2008},
  month = mar,
  pages = {579--588},
  issn = {2377-5408},
  doi = {10.1109/ASWEC.2008.4483248},
  urldate = {2024-09-11},
  abstract = {In this paper, we present an overview of HUI Analyzer, a tool intended for automating usability testing. The tool allows a user interface's expected and actual use to be captured unobtrusively, with any mismatches indicating potential usability problems being highlighted. HUI Analyzer also supports specification and checking of assertions governing a user interface's layout and actual user interaction. Assertions offer a low cost means of detecting usability defects and are intended to be checked iteratively during a user interface's development. Hotspot analysis is a feature that highlights the relative use of GUI components in a form. This is useful in informing form layout, for example to collocate heavily used components thereby reducing unnecessary scrolling or movement. Based on evaluation, we have found HUI Analyzer's performance in detecting usability defects to be comparable to conventional formal user testing. However the time taken by HUI Analyzer to automatically process and analyze user interactions is much less than that for formal user testing.},
  keywords = {Automatic testing,Computer science,Costs,Gold,Graphical user interfaces,Performance analysis,Software engineering,Software quality,Software testing,Usability}
}

@article{baksiRecentAdvancesAutomated2021,
  title = {Recent {{Advances}} in {{Automated Question Answering In Biomedical Domain}}},
  author = {Baksi, Krishanu Das},
  year = {2021},
  journal = {arXiv preprint arXiv:2111.05937},
  eprint = {2111.05937},
  archiveprefix = {arXiv}
}

@article{balabanoviAdaptiveAgentAutomated,
  title = {An {{Adaptive Agent}} for {{Automated Web Browsing}}},
  author = {Balabanovi, Marko},
  abstract = {The current exponential growth of the Internet precipitates a need for new tools to help people cope with the volume of information. To complement recent work on creating searchable indexes of the WorldWide Web and systems for ltering incoming e-mail and Usenet news articles, we describe a system which learns to browse the Internet on behalf of a user. Every day it presents a selection of interesting Web pages. The user evaluates each page, and given this feedback the system adapts and attempts to produce better pages the following day. After demonstrating that our system is able to learn a model of a user with a single well-de ned interest, we present an initial experiment where over the course of 24 days the output of our system was compared to both randomly-selected and human-selected pages. It consistently performed better than the random pages, and was better than the human-selected pages half of the time.},
  langid = {english},
  keywords = {No DOI found}
}

@techreport{balabanovicAdaptiveAgentAutomated1997,
  type = {Technical {{Report}}},
  title = {An {{Adaptive Agent}} for {{Automated Web Browsing}}},
  author = {Balabanovic, Marko and Shoham, Yoav and Yun, Yeogirl},
  year = {1997},
  month = jan,
  address = {Stanford, CA, USA},
  institution = {Stanford University},
  abstract = {The current exponential growth of the Internet precipitates a need for new tools to help people cope with the volume of information. To complement recent work on creating searchable indexes of the World-Wide Web and systems for filtering incoming e-mail and Usenet news articles, we describe a system which learns to browse the Internet on behalf of a user. Every day it presents a selection of interesting Web pages. The user evaluates each page, and given this feedback the system adapts and attempts to produce better pages the following day. After demonstrating that our system is able to learn a model of a user with a single well-defined interest, we present an initial experiment where over the course of 24 days the output of our system was compared to both randomly-selected and human-selected pages. It consistently performed better than the random pages, and was better than the human-selected pages half of the time.}
}

@misc{balogUserSimulationEvaluating2024,
  title = {User {{Simulation}} for {{Evaluating Information Access Systems}}},
  author = {Balog, Krisztian and Zhai, ChengXiang},
  year = {2024},
  month = may,
  number = {arXiv:2306.08550},
  eprint = {2306.08550},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.08550},
  urldate = {2025-04-26},
  abstract = {Information access systems, such as search engines, recommender systems, and conversational assistants, have become integral to our daily lives as they help us satisfy our information needs. However, evaluating the effectiveness of these systems presents a long-standing and complex scientific challenge. This challenge is rooted in the difficulty of assessing a system's overall effectiveness in assisting users to complete tasks through interactive support, and further exacerbated by the substantial variation in user behaviour and preferences. To address this challenge, user simulation emerges as a promising solution. This book focuses on providing a thorough understanding of user simulation techniques designed specifically for evaluation purposes. We begin with a background of information access system evaluation and explore the diverse applications of user simulation. Subsequently, we systematically review the major research progress in user simulation, covering both general frameworks for designing user simulators, utilizing user simulation for evaluation, and specific models and algorithms for simulating user interactions with search engines, recommender systems, and conversational assistants. Realizing that user simulation is an interdisciplinary research topic, whenever possible, we attempt to establish connections with related fields, including machine learning, dialogue systems, user modeling, and economics. We end the book with a detailed discussion of important future research directions, many of which extend beyond the evaluation of information access systems and are expected to have broader impact on how to evaluate interactive intelligent systems in general.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Information Retrieval}
}

@misc{baoBiStepGroundingParadigm2023,
  title = {A {{Bi-Step Grounding Paradigm}} for {{Large Language Models}} in {{Recommendation Systems}}},
  author = {Bao, Keqin and Zhang, Jizhi and Wang, Wenjie and Zhang, Yang and Yang, Zhengyi and Luo, Yancheng and Chen, Chong and Feng, Fuli and Tian, Qi},
  year = {2023},
  month = dec,
  number = {arXiv:2308.08434},
  eprint = {2308.08434},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.08434},
  urldate = {2024-09-24},
  abstract = {As the focus on Large Language Models (LLMs) in the field of recommendation intensifies, the optimization of LLMs for recommendation purposes (referred to as LLM4Rec) assumes a crucial role in augmenting their effectiveness in providing recommendations. However, existing approaches for LLM4Rec often assess performance using restricted sets of candidates, which may not accurately reflect the models' overall ranking capabilities. In this paper, our objective is to investigate the comprehensive ranking capacity of LLMs and propose a two-step grounding framework known as BIGRec (Bi-step Grounding Paradigm for Recommendation). It initially grounds LLMs to the recommendation space by fine-tuning them to generate meaningful tokens for items and subsequently identifies appropriate actual items that correspond to the generated tokens. By conducting extensive experiments on two datasets, we substantiate the superior performance, capacity for handling few-shot scenarios, and versatility across multiple domains exhibited by BIGRec. Furthermore, we observe that the marginal benefits derived from increasing the quantity of training samples are modest for BIGRec, implying that LLMs possess the limited capability to assimilate statistical information, such as popularity and collaborative filtering, due to their robust semantic priors. These findings also underline the efficacy of integrating diverse statistical information into the LLM4Rec framework, thereby pointing towards a potential avenue for future research. Our code and data are available at https://github.com/SAI990323/Grounding4Rec.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval}
}

@inproceedings{baoContextualizedRewritingText2021,
  title = {Contextualized {{Rewriting}} for {{Text Summarization}}},
  booktitle = {{{AAAI}}},
  author = {Bao, Guangsheng and Zhang, Yue},
  year = {2021},
  abstract = {Extractive summarization suffers from irrelevance, redundancy and incoherence. Existing work shows that abstractive rewriting for extractive summaries can improve the conciseness and readability. These rewriting systems consider extracted summaries as the only input, which is relatively focused but can lose important background knowledge. In this paper, we investigate contextualized rewriting, which ingests the entire original document. We formalize contextualized rewriting as a seq2seq problem with group alignments, introducing group tag as a solution to model the alignments, identifying extracted summaries through content-based addressing. Results show that our approach significantly outperforms non-contextualized rewriting systems without requiring reinforcement learning, achieving strong improvements on ROUGE scores upon multiple extractive summarizers.}
}

@inproceedings{baoTALLRecEffectiveEfficient2023,
  title = {{{TALLRec}}: {{An Effective}} and {{Efficient Tuning Framework}} to {{Align Large Language Model}} with {{Recommendation}}},
  shorttitle = {{{TALLRec}}},
  booktitle = {Proceedings of the 17th {{ACM Conference}} on {{Recommender Systems}}},
  author = {Bao, Keqin and Zhang, Jizhi and Zhang, Yang and Wang, Wenjie and Feng, Fuli and He, Xiangnan},
  year = {2023},
  month = sep,
  eprint = {2305.00447},
  primaryclass = {cs},
  pages = {1007--1014},
  doi = {10.1145/3604915.3608857},
  urldate = {2024-09-24},
  abstract = {Large Language Models (LLMs) have demonstrated remarkable performance across diverse domains, thereby prompting researchers to explore their potential for use in recommendation systems. Initial attempts have leveraged the exceptional capabilities of LLMs, such as rich knowledge and strong generalization through In-context Learning, which involves phrasing the recommendation task as prompts. Nevertheless, the performance of LLMs in recommendation tasks remains suboptimal due to a substantial disparity between the training tasks for LLMs and recommendation tasks, as well as inadequate recommendation data during pre-training. To bridge the gap, we consider building a Large Recommendation Language Model by tunning LLMs with recommendation data. To this end, we propose an efficient and effective Tuning framework for Aligning LLMs with Recommendation, namely TALLRec. We have demonstrated that the proposed TALLRec framework can significantly enhance the recommendation capabilities of LLMs in the movie and book domains, even with a limited dataset of fewer than 100 samples. Additionally, the proposed framework is highly efficient and can be executed on a single RTX 3090 with LLaMA-7B. Furthermore, the fine-tuned LLM exhibits robust cross-domain generalization. Our code and data are available at https://github.com/SAI990323/TALLRec.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval}
}

@article{baradaranSurveyMachineReading2020,
  title = {A Survey on Machine Reading Comprehension Systems},
  author = {Baradaran, Razieh and Ghiasi, Razieh and Amirkhani, Hossein},
  year = {2020},
  journal = {arXiv preprint arXiv:2001.01582},
  eprint = {2001.01582},
  archiveprefix = {arXiv}
}

@inproceedings{barataActiveLearningImbalanced2021,
  title = {Active Learning for Imbalanced Data under Cold Start},
  booktitle = {Proceedings of the {{Second ACM International Conference}} on {{AI}} in {{Finance}}},
  author = {Barata, Ricardo and Leite, Miguel and Pacheco, Ricardo and Sampaio, Marco O. P. and Ascens{\~a}o, Jo{\~a}o Tiago and Bizarro, Pedro},
  year = {2021},
  month = nov,
  series = {{{ICAIF}} '21},
  pages = {1--9},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3490354.3494423},
  urldate = {2024-04-25},
  abstract = {Modern systems that rely on Machine Learning (ML) for predictive modelling, may suffer from the cold-start problem: supervised models work well but, initially, there are no labels, which are costly or slow to obtain. This problem is even worse in imbalanced data scenarios, where labels of the positive class take longer to accumulate. We propose an Active Learning (AL) system for datasets with orders of magnitude of class imbalance, in a cold start streaming scenario. We present a computationally efficient Outlier-based Discriminative AL approach (ODAL) and design a novel 3-stage sequence of AL labeling policies where ODAL is used as warm-up. Then, we perform empirical studies in four real world datasets, with various magnitudes of class imbalance. The results show that our method can more quickly reach a high performance model than standard AL policies without ODAL warm-up. Its observed gains over random sampling can reach 80\% and be competitive with policies with an unlimited annotation budget or additional historical data (using just 2\% to 10\% of the labels).},
  isbn = {978-1-4503-9148-1},
  langid = {american},
  keywords = {Cold Start Active Learning}
}

@article{barnhoornQRTEngineEasySolution2015,
  title = {{{QRTEngine}}: {{An}} Easy Solution for Running Online Reaction Time Experiments Using {{Qualtrics}}},
  shorttitle = {{{QRTEngine}}},
  author = {Barnhoorn, Jonathan S. and Haasnoot, Erwin and Bocanegra, Bruno R. and {van Steenbergen}, Henk},
  year = {2015},
  month = dec,
  journal = {Behavior Research Methods},
  volume = {47},
  number = {4},
  pages = {918--929},
  issn = {1554-3528},
  doi = {10.3758/s13428-014-0530-7},
  urldate = {2024-09-13},
  abstract = {Performing online behavioral research is gaining increased popularity among researchers in psychological and cognitive science. However, the currently available methods for conducting online reaction time experiments are often complicated and typically require advanced technical skills. In this article, we introduce the Qualtrics Reaction Time Engine (QRTEngine), an open-source JavaScript engine that can be embedded in the online survey development environment Qualtrics. The QRTEngine can be used to easily develop browser-based online reaction time experiments with accurate timing within current browser capabilities, and it requires only minimal programming skills. After introducing the QRTEngine, we briefly discuss how to create and distribute a Stroop task. Next, we describe a study in which we investigated the timing accuracy of the engine under different processor loads using external chronometry. Finally, we show that the QRTEngine can be used to reproduce classic behavioral effects in three reaction time paradigms: a Stroop task, an attentional blink task, and a masked-priming task. These findings demonstrate that QRTEngine can be used as a tool for conducting online behavioral research even when this requires accurate stimulus presentation times.},
  langid = {english},
  keywords = {Amazon Mechanical Turk,JavaScript,Online experiments,Open-source,Qualtrics}
}

@book{barnum2020usability,
  title = {Usability Testing Essentials: {{Ready}}, Set... Test!},
  author = {Barnum, Carol M},
  year = {2020},
  publisher = {Morgan Kaufmann}
}

@inproceedings{bastanBioNLIGeneratingBiomedical2022,
  title = {{{BioNLI}}: {{Generating}} a {{Biomedical NLI Dataset Using Lexico-semantic Constraints}} for {{Adversarial Examples}}},
  shorttitle = {{{BioNLI}}},
  author = {Bastan, Mohaddeseh and Surdeanu, Mihai and Balasubramanian, Niranjan},
  year = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2210.14814},
  urldate = {2023-10-23},
  abstract = {Natural language inference (NLI) is critical for complex decision-making in biomedical domain. One key question, for example, is whether a given biomedical mechanism is supported by experimental evidence. This can be seen as an NLI problem but there are no directly usable datasets to address this. The main challenge is that manually creating informative negative examples for this task is difficult and expensive. We introduce a novel semi-supervised procedure that bootstraps an NLI dataset from existing biomedical dataset that pairs mechanisms with experimental evidence in abstracts. We generate a range of negative examples using nine strategies that manipulate the structure of the underlying mechanisms both with rules, e.g., flip the roles of the entities in the interaction, and, more importantly, as perturbations via logical constraints in a neuro-logical decoding system. We use this procedure to create a novel dataset for NLI in the biomedical domain, called BioNLI and benchmark two state-of-the-art biomedical classifiers. The best result we obtain is around mid 70s in F1, suggesting the difficulty of the task. Critically, the performance on the different classes of negative examples varies widely, from 97\% F1 on the simple role change negative examples, to barely better than chance on the negative examples generated using neuro-logic decoding.},
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{bastienUsabilityTestingReview2010,
  title = {Usability Testing: A Review of Some Methodological and Technical Aspects of the Method},
  shorttitle = {Usability Testing},
  author = {Bastien, J.M. Christian},
  year = {2010},
  month = apr,
  journal = {International Journal of Medical Informatics},
  volume = {79},
  number = {4},
  pages = {e18-e23},
  issn = {13865056},
  doi = {10.1016/j.ijmedinf.2008.12.004},
  urldate = {2024-09-10},
  abstract = {Semantic Scholar extracted view of "Usability testing: a review of some methodological and technical aspects of the method" by J. Bastien},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english}
}

@inproceedings{basuDesignImplementationAcademic2022,
  title = {Design and {{Implementation}} of an {{Academic Integrity Module}} for {{Undergraduate CS Students}}},
  booktitle = {Proceedings of the 53rd {{ACM Technical Symposium}} on {{Computer Science Education V}}. 2},
  author = {Basu, Debarati and Ramaprasad, Harini},
  year = {2022},
  month = mar,
  series = {{{SIGCSE}} 2022},
  pages = {1082},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3478432.3499057},
  urldate = {2024-09-12},
  abstract = {In recent years, Academic Integrity (AI) cases in the Computer Science (CS) discipline are on the rise, with students often being unaware of what constitutes a violation and its potential consequences. This prompted us to develop an AI module that aims to educate CS undergraduate students to recognize misconduct in academic scenarios, their potential consequences, and recall strategies/resources to avoid misconduct in the CS context. We have deployed this module in eight CS courses in UNC Charlotte. In this poster, we present our motivation, design guidelines, deployment details, and the results of a study based on data collected in Spring and Fall 2021.},
  isbn = {978-1-4503-9071-2}
}

@misc{BatchInferenceAmazon,
  title = {Batch {{Inference}} {\textbar} {{Amazon Bedrock}} {\textbar} Us-East-1},
  url = {https://825193025562-iuaewhlh.us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/batch-inference},
  urldate = {2025-04-14}
}

@article{beamishSystematicReviewMetaanalysis2015,
  title = {Systematic Review and Meta-Analysis of Enhanced Recovery Programmes in Gastric Cancer Surgery},
  author = {Beamish, Andrew James and Chan, David Sheng Yi and Blake, Paul A. and Karran, Alexandra and Lewis, Wyn Griffith},
  year = {2015},
  month = jul,
  journal = {International Journal of Surgery},
  volume = {19},
  pages = {46--54},
  issn = {1743-9191},
  doi = {10.1016/j.ijsu.2015.05.021},
  urldate = {2024-04-26},
  abstract = {This systematic review and meta-analysis was performed to determine the influence of enhanced recovery programmes (ERPs) on outcomes after gastric cancer surgery. Medline, Embase, the Cochrane library and ClinicalTrials.gov were searched for studies on outcomes of gastrectomy in enhanced recovery or fast-track programmes. The primary outcome measure was post-operative duration of hospital stay (LOHS), and secondary outcome measures were selected based inclusion in two or more studies. Statistical analysis was performed using standardized mean difference (SMD) and odds ratio (OR) as the summary statistics. Fourteen studies, totalling 1676 patients with gastric cancer were analysed, including nine randomized trials. LOHS was significantly shorter after ERP when compared with control patients (CON, SMD -1.10, 95\% confidence interval~-1.56 to~-0.65, p~{$<~$}0.001), but with significant heterogeneity between studies (I2~=~93\%, p~{$<~$}0.001). ERP was also associated with reduced serum inflammatory response (CRP: SMD -0.68 (-1.16 to~-0.19), p~=~0.007; IL-6: SMD -0.62 (-0.94 to~-0.29), p~{$<~$}0.001), less weight loss (SMD -0.79 (-1.11 to~-0.46), p~{$<~$}0.001), and lower cost (SMD -1.02 (-1.59 to~-0.45), p~{$<~$}0.001), as well as a trend toward shorter duration of intravenous infusion (SMD -2.70 (-5.35 to~-0.05), p~=~0.05). Inclusion in an ERP was not associated with increased post-operative morbidity (OR 0.83 (0.65 to 1.06), p~=~0.13) or hospital readmission (OR 1.67 (0.88 to 3.19), p~=~0.12). From this review the authors concluded that multimodal, standardized perioperative gastrectomy care appears feasible, safe and cost effective.},
  keywords = {Enhanced recovery,Fast track surgery,Gastric cancer,Meta-analysis,Perioperative care}
}

@article{beatonCardiacDeathBreast2019,
  title = {Cardiac Death after Breast Radiotherapy and the {{QUANTEC}} Cardiac Guidelines},
  author = {Beaton, Laura and Bergman, Alanah and Nichol, Alan and Aparicio, Maria and Wong, Graham and Gondara, Lovedeep and Speers, Caroline and Weir, Lorna and Davis, Margot and Tyldesley, Scott},
  year = {2019},
  month = nov,
  journal = {Clinical and Translational Radiation Oncology},
  volume = {19},
  pages = {39--45},
  issn = {24056308},
  doi = {10.1016/j.ctro.2019.08.001},
  abstract = {BACKGROUND: Breast/chest wall irradiation (RT) increases risk of cardiovascular death. International Quantitative Analysis of Normal Tissue Effects in the Clinic (QUANTEC) guidelines state for partial heart irradiation a "V25Gy {$<$}10\% will be associated with a {$<$}1\% probability of cardiac mortality" in long-term follow-up after RT. We assessed whether women treated with breast/chest wall RT 10-years ago who died of cardiovascular disease (CVD) violated QUANTEC guidelines. MATERIALS/METHODS: A population-based database identified all cardiovascular deaths in women with early-stage breast cancer {$<$}80\,years, treated with adjuvant breast/chest wall RT from 2002 to 2006. Ten-year rate of cardiovascular death was calculated using a Kaplan-Meier method. Patients were matched on a 2:1 basis with controls that did not die of CVD. For left-sided cases, the heart and left anterior descending (LAD) artery were retrospectively delineated. Dose-volume histograms were calculated, and heart V25Gy compared to QUANTEC guidelines. RESULTS: 5249 eligible patients received breast/chest wall RT from 2002 to 2006: 76 (1.4\% at 10-years) died of CVD by June 2015. Forty-two patients received left-sided RT (1.7\% CVD death at 10-years), 34 right-sided RT (1.3\% at 10-years). Heart V25Gy did not exceed 10\% in any left-sided cases. No cardiac dosimetry parameter distinguished left-sided cases from controls. CONCLUSIONS: QUANTEC guidelines were not violated in any patient that died of CVD after left-sided RT. The risk of radiation induced cardiac death at 10-years appears to be very low if MHD is {$<$}3.3\,Gy and maximum LAD dose (EQD23 Gy) is {$<$}45.4\,Gy. Further studies are needed to evaluate heart and LAD constraints in the CT-planning era.},
  langid = {american},
  pmcid = {PMC6715791},
  pmid = {31485490}
}

@misc{beckSTENCILSubmodularMutual2024,
  title = {{{STENCIL}}: {{Submodular Mutual Information Based Weak Supervision}} for {{Cold-Start Active Learning}}},
  shorttitle = {{{STENCIL}}},
  author = {Beck, Nathan and Iyer, Adithya and Iyer, Rishabh},
  year = {2024},
  month = feb,
  number = {arXiv:2402.13468},
  eprint = {2402.13468},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2402.13468},
  urldate = {2024-04-25},
  abstract = {As supervised fine-tuning of pre-trained models within NLP applications increases in popularity, larger corpora of annotated data are required, especially with increasing parameter counts in large language models. Active learning, which attempts to mine and annotate unlabeled instances to improve model performance maximally fast, is a common choice for reducing the annotation cost; however, most methods typically ignore class imbalance and either assume access to initial annotated data or require multiple rounds of active learning selection before improving rare classes. We present STENCIL, which utilizes a set of text exemplars and the recently proposed submodular mutual information to select a set of weakly labeled rare-class instances that are then strongly labeled by an annotator. We show that STENCIL improves overall accuracy by \$10{\textbackslash}\%-24{\textbackslash}\%\$ and rare-class F-1 score by \$17{\textbackslash}\%-40{\textbackslash}\%\$ on multiple text classification datasets over common active learning methods within the class-imbalanced cold-start setting.},
  archiveprefix = {arXiv},
  langid = {american},
  keywords = {Cold Start Active Learning}
}

@article{BeFriendlyNot,
  title = {Be {{Friendly}}, {{Not Friends}}: {{How LLM Agents}}' {{Friendliness}} and {{Sycophancy Affect User Trust}}},
  langid = {english}
}

@article{begerCitegeistAutomatedGeneration,
  title = {Citegeist: {{Automated Generation}} of {{Related Work Analysis}} on the {{arXiv Corpus}}},
  author = {Beger, Claas and Henneking, Carl-Leander},
  abstract = {Large Language Models provide significant new opportunities for the generation of highquality written works. However, their employment in the research community is inhibited by their tendency to hallucinate invalid sources and lack of direct access to a knowledge base of relevant scientific articles. In this work, we present Citegeist: An application pipeline using dynamic Retrieval Augmented Generation (RAG) on the arXiv Corpus to generate a related work section and other citation-backed outputs. For this purpose, we employ a mixture of embedding-based similarity matching, summarization, and multi-stage filtering. To adapt to the continuous growth of the document base, we also present an optimized way of incorporating new and modified papers. To enable easy utilization in the scientific community, we release both, a website, as well as an implementation harness that works with several different LLM implementations.},
  langid = {english}
}

@article{bellamScalableInterpretableTool,
  title = {A {{Scalable}} and {{Interpretable Tool}} for {{Visual Demonstrations}} of {{NLP Models}}},
  author = {Bellam, Pavan Kumar and Titung, Rajesh and Alm, Cecilia O},
  abstract = {There is a need for platforms that allow researchers and professionals to present NLP models visually, beyond standard evaluation metrics. Building on a prior visualization system, we report on a model demonstration platform that has been redesigned for this purpose. It supports PyTorch, TensorFlow, and Hugging Face models and integrates interpretability with SHAP and LIME methods to allow observers to inspect and visually analyze predictions. The new platform's system architecture ensures scalability, and evaluation demonstrates its ability to handle multiple requests efficiently. As a proof of concept, the platform focuses on sentiment analysis, with plans to expand support across NLP tasks and enhance interpretability functionalities. This platform provides a structured approach for researchers and practitioners to, formatively or summatively, present and demonstrate NLP models.},
  langid = {english}
}

@article{beltagySciBERTPretrainedLanguage2019,
  title = {{{SciBERT}}: {{A Pretrained Language Model}} for {{Scientific Text}}},
  shorttitle = {{{SciBERT}}},
  author = {Beltagy, Iz and Lo, Kyle and Cohan, Arman},
  year = {2019},
  month = sep,
  journal = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  eprint = {1903.10676},
  pages = {3615--3620},
  doi = {10.18653/v1/D19-1371},
  urldate = {2022-01-17},
  abstract = {Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et. al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.},
  archiveprefix = {arXiv}
}

@misc{beurer-kellnerGuidingLLMsRight2024,
  title = {Guiding {{LLMs The Right Way}}: {{Fast}}, {{Non-Invasive Constrained Generation}}},
  shorttitle = {Guiding {{LLMs The Right Way}}},
  author = {{Beurer-Kellner}, Luca and Fischer, Marc and Vechev, Martin},
  year = {2024},
  month = feb,
  number = {arXiv:2403.06988},
  eprint = {2403.06988},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.06988},
  urldate = {2024-09-17},
  abstract = {To ensure that text generated by large language models (LLMs) is in an expected format, constrained decoding proposes to enforce strict formal language constraints during generation. However, as we show in this work, not only do such methods incur performance overhead during generation, but many of them also significantly impair task accuracy, if they do not correctly align the underlying LLM sub-word vocabularies with external constraints. To address this, we present a novel decoding algorithm, DOMINO, that can enforce constraints in a fully subword-aligned fashion, while leveraging pre-computation and speculative decoding to achieve virtually no overhead and in some cases even almost 2\${\textbackslash}times\$ speedup over unconstrained decoding -- thereby outperforming existing approaches by a wide margin.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@inproceedings{bholaRetrievingSkillsJob2020,
  title = {Retrieving {{Skills}} from {{Job Descriptions}}: {{A Language Model Based Extreme Multi-label Classification Framework}}},
  shorttitle = {Retrieving {{Skills}} from {{Job Descriptions}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Computational Linguistics}}},
  author = {Bhola, Akshay and Halder, Kishaloy and Prasad, Animesh and Kan, Min-Yen},
  year = {2020},
  month = dec,
  pages = {5832--5842},
  publisher = {International Committee on Computational Linguistics},
  address = {Barcelona, Spain (Online)},
  doi = {10.18653/v1/2020.coling-main.513},
  urldate = {2022-07-12},
  abstract = {We introduce a deep learning model to learn the set of enumerated job skills associated with a job description. In our analysis of a large-scale government job portal mycareersfuture.sg, we observe that as much as 65\% of job descriptions miss describing a significant number of relevant skills. Our model addresses this task from the perspective of an extreme multi-label classification (XMLC) problem, where descriptions are the evidence for the binary relevance of thousands of individual skills. Building upon the current state-of-the-art language modeling approaches such as BERT, we show our XMLC method improves on an existing baseline solution by over 9\% and 7\% absolute improvements in terms of recall and normalized discounted cumulative gain. We further show that our approach effectively addresses the missing skills problem, and helps in recovering relevant skills that were missed out in the job postings by taking into account the structured semantic representation of skills and their co-occurrences through a Correlation Aware Bootstrapping process. We further show that our approach, to ensure the BERT-XMLC model accounts for structured semantic representation of skills and their co-occurrences through a Correlation Aware Bootstrapping process, effectively addresses the missing skills problem, and helps in recovering relevant skills that were missed out in the job postings. To facilitate future research and replication of our work, we have made the dataset and the implementation of our model publicly available.}
}

@misc{biancoImprovingImageCaptioning2023,
  title = {Improving {{Image Captioning Descriptiveness}} by {{Ranking}} and {{LLM-based Fusion}}},
  author = {Bianco, Simone and Celona, Luigi and Donzella, Marco and Napoletano, Paolo},
  year = {2023},
  month = jun,
  number = {arXiv:2306.11593},
  eprint = {2306.11593},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.11593},
  urldate = {2024-03-25},
  abstract = {State-of-The-Art (SoTA) image captioning models often rely on the Microsoft COCO (MS-COCO) dataset for training. This dataset contains annotations provided by human annotators, who typically produce captions averaging around ten tokens. However, this constraint presents a challenge in effectively capturing complex scenes and conveying detailed information. Furthermore, captioning models tend to exhibit bias towards the ``average'' caption, which captures only the more general aspects. What would happen if we were able to automatically generate longer captions, thereby making them more detailed? Would these captions, evaluated by humans, be more or less representative of the image content compared to the original MS-COCO captions? In this paper, we present a novel approach to address previous challenges by showcasing how captions generated from different SoTA models can be effectively fused, resulting in richer captions. Our proposed method leverages existing models from the literature, eliminating the need for additional training. Instead, it utilizes an image-text based metric to rank the captions generated by SoTA models for a given image. Subsequently, the top two captions are fused using a Large Language Model (LLM). Experimental results demonstrate the effectiveness of our approach, as the captions generated by our model exhibit higher consistency with human judgment when evaluated on the MS-COCO test set. By combining the strengths of various SoTA models, our method enhances the quality and appeal of image captions, bridging the gap between automated systems and the rich, informative nature of human-generated descriptions. This advance opens up new possibilities for generating captions that are more suitable for the training of both vision-language and captioning models.},
  archiveprefix = {arXiv}
}

@inproceedings{bikaunQuickGraphRapidAnnotation2022,
  title = {{{QuickGraph}}: {{A Rapid Annotation Tool}} for {{Knowledge Graph Extraction}} from {{Technical Text}}},
  shorttitle = {{{QuickGraph}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{System Demonstrations}}},
  author = {Bikaun, Tyler and Stewart, Michael and Liu, Wei},
  year = {2022},
  month = may,
  pages = {270--278},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-demo.27},
  urldate = {2023-04-03},
  abstract = {Acquiring high-quality annotated corpora for complex multi-task information extraction (MT-IE) is an arduous and costly process for human-annotators. Adoption of unsupervised techniques for automated annotation have thus become popular. However, these techniques rely heavily on dictionaries, gazetteers, and knowledge bases. While such resources are abundant for general domains, they are scarce for specialised technical domains. To tackle this challenge, we present QuickGraph, the first collaborative MT-IE annotation tool built with indirect weak supervision and clustering to maximise annotator productivity.QuickGraph's main contribution is a set of novel features that enable knowledge graph extraction through rapid and consistent complex multi-task entity and relation annotation. In this paper, we discuss these key features and qualitatively compare QuickGraph to existing annotation tools.}
}

@inproceedings{birdNLTKNaturalLanguage2004,
  title = {{{NLTK}}: {{The Natural Language Toolkit}}},
  shorttitle = {{{NLTK}}},
  booktitle = {Proceedings of the {{ACL Interactive Poster}} and {{Demonstration Sessions}}},
  author = {Bird, Steven and Loper, Edward},
  year = {2004},
  pages = {214--217},
  publisher = {Association for Computational Linguistics},
  address = {Barcelona, Spain},
  doi = {10.3115/1219044.1219075},
  urldate = {2022-01-10},
  annotation = {02186}
}

@inproceedings{bordesTranslatingEmbeddingsModeling2013,
  title = {Translating {{Embeddings}} for {{Modeling Multi-relational Data}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bordes, Antoine and Usunier, Nicolas and {Garcia-Duran}, Alberto and Weston, Jason and Yakhnenko, Oksana},
  year = {2013},
  volume = {26},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2013/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html},
  urldate = {2021-12-07}
}

@misc{borgeaudImprovingLanguageModels2022,
  title = {Improving Language Models by Retrieving from Trillions of Tokens},
  author = {Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and van den Driessche, George and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and Casas, Diego de Las and Guy, Aurelia and Menick, Jacob and Ring, Roman and Hennigan, Tom and Huang, Saffron and Maggiore, Loren and Jones, Chris and Cassirer, Albin and Brock, Andy and Paganini, Michela and Irving, Geoffrey and Vinyals, Oriol and Osindero, Simon and Simonyan, Karen and Rae, Jack W. and Elsen, Erich and Sifre, Laurent},
  year = {2022},
  month = feb,
  number = {arXiv:2112.04426},
  eprint = {2112.04426},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.04426},
  urldate = {2022-09-06},
  abstract = {We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a \$2\$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25\${\textbackslash}times\$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.},
  archiveprefix = {arXiv}
}

@inproceedings{bowmanLargeAnnotatedCorpus2015,
  title = {A Large Annotated Corpus for Learning Natural Language Inference},
  booktitle = {Proceedings of the 2015 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Bowman, Samuel R. and Angeli, Gabor and Potts, Christopher and Manning, Christopher D.},
  editor = {M{\`a}rquez, Llu{\'i}s and {Callison-Burch}, Chris and Su, Jian},
  year = {2015},
  month = sep,
  pages = {632--642},
  publisher = {Association for Computational Linguistics},
  address = {Lisbon, Portugal},
  doi = {10.18653/v1/D15-1075},
  urldate = {2023-12-15}
}

@article{bridgesTimingMegastudyComparing2020,
  title = {The Timing Mega-Study: Comparing a Range of Experiment Generators, Both Lab-Based and Online},
  shorttitle = {The Timing Mega-Study},
  author = {Bridges, David and Pitiot, Alain and MacAskill, Michael R. and Peirce, Jonathan W.},
  year = {2020},
  month = jul,
  journal = {PeerJ},
  volume = {8},
  pages = {e9414},
  publisher = {PeerJ Inc.},
  issn = {2167-8359},
  doi = {10.7717/peerj.9414},
  urldate = {2024-09-13},
  abstract = {Many researchers in the behavioral sciences depend on research software that presents stimuli, and records response times, with sub-millisecond precision. There are a large number of software packages with which to conduct these behavioral experiments and measure response times and performance of participants. Very little information is available, however, on what timing performance they achieve in practice. Here we report a wide-ranging study looking at the precision and accuracy of visual and auditory stimulus timing and response times, measured with a Black Box Toolkit. We compared a range of popular packages: PsychoPy, E-Prime{\textregistered}, NBS Presentation{\textregistered}, Psychophysics Toolbox, OpenSesame, Expyriment, Gorilla, jsPsych, Lab.js and Testable. Where possible, the packages were tested on Windows, macOS, and Ubuntu, and in a range of browsers for the online studies, to try to identify common patterns in performance. Among the lab-based experiments, Psychtoolbox, PsychoPy, Presentation and E-Prime provided the best timing, all with mean precision under 1 millisecond across the visual, audio and response measures. OpenSesame had slightly less precision across the board, but most notably in audio stimuli and Expyriment had rather poor precision. Across operating systems, the pattern was that precision was generally very slightly better under Ubuntu than Windows, and that macOS was the worst, at least for visual stimuli, for all packages. Online studies did not deliver the same level of precision as lab-based systems, with slightly more variability in all measurements. That said, PsychoPy and Gorilla, broadly the best performers, were achieving very close to millisecond precision on several browser/operating system combinations. For response times (measured using a high-performance button box), most of the packages achieved precision at least under 10 ms in all browsers, with PsychoPy achieving a precision under 3.5 ms in all. There was considerable variability between OS/browser combinations, especially in audio-visual synchrony which is the least precise aspect of the browser-based experiments. Nonetheless, the data indicate that online methods can be suitable for a wide range of studies, with due thought about the sources of variability that result. The results, from over 110,000 trials, highlight the wide range of timing qualities that can occur even in these dedicated software packages for the task. We stress the importance of scientists making their own timing validation measurements for their own stimuli and computer configuration.},
  langid = {english}
}

@article{brinAnatomyLargescaleHypertextual1998,
  title = {The Anatomy of a Large-Scale Hypertextual {{Web}} Search Engine},
  author = {Brin, Sergey and Page, Lawrence},
  year = {1998},
  month = apr,
  journal = {Computer Networks and ISDN Systems},
  series = {Proceedings of the {{Seventh International World Wide Web Conference}}},
  volume = {30},
  number = {1},
  pages = {107--117},
  issn = {0169-7552},
  doi = {10.1016/S0169-7552(98)00110-X},
  urldate = {2024-08-27},
  abstract = {In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/ To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of Web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the Web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and Web proliferation, creating a Web search engine today is very different from three years ago. This paper provides an in-depth description of our large-scale Web search engine --- the first such detailed public description we know of to date. Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections where anyone can publish anything they want.},
  keywords = {Google,Information retrieval,PageRank,Search engines,World Wide Web}
}

@incollection{brookeSUSQuickDirty1996,
  title = {{{SUS}}: {{A}} '{{Quick}} and {{Dirty}}' {{Usability Scale}}},
  shorttitle = {{{SUS}}},
  booktitle = {Usability {{Evaluation In Industry}}},
  author = {Brooke, john},
  year = {1996},
  publisher = {CRC Press},
  abstract = {Usability is not a quality that exists in any real or absolute sense. Perhaps it can be  best summed up as being a general quality of the appropriateness to a purpose of  any particular artefact. This notion is neatly summed up by Terry Pratchett in his  novel Moving Pictures:In just the same way, the usability of any tool or system has to be viewed in terms  of the context in which it is used, and its appropriateness to that context. With  particular reference to information systems, this view of usability is reflected in the  current draft international standard ISO 9241-11 and in the European Community  ESPRIT project MUSiC (Measuring Usability of Systems in Context) (e.g. Bevan  et al., 1991). In general, it is impossible to specify the usability of a system (i.e. its  fitness for purpose) without first defining who are the intended users of the system,  the tasks those users will perform with it, and the characteristics of the physical,  organizational and social environment in which it will be used.},
  isbn = {978-0-429-15701-1},
  langid = {american}
}

@inproceedings{brownLanguageModelsAre2020,
  title = {Language Models Are Few-Shot Learners},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = dec,
  series = {{{NIPS}}'20},
  pages = {1877--1901},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2023-06-23},
  abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
  isbn = {978-1-7138-2954-6}
}

@misc{bubeckSparksArtificialGeneral2023,
  title = {Sparks of {{Artificial General Intelligence}}: {{Early}} Experiments with {{GPT-4}}},
  shorttitle = {Sparks of {{Artificial General Intelligence}}},
  author = {Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
  year = {2023},
  month = mar,
  number = {arXiv:2303.12712},
  eprint = {2303.12712},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.12712},
  urldate = {2023-03-24},
  abstract = {Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.},
  archiveprefix = {arXiv}
}

@misc{budzianowskiMultiWOZLargeScaleMultiDomain2020,
  title = {{{MultiWOZ}} -- {{A Large-Scale Multi-Domain Wizard-of-Oz Dataset}} for {{Task-Oriented Dialogue Modelling}}},
  author = {Budzianowski, Pawe{\l} and Wen, Tsung-Hsien and Tseng, Bo-Hsiang and Casanueva, I{\~n}igo and Ultes, Stefan and Ramadan, Osman and Ga{\v s}i{\'c}, Milica},
  year = {2020},
  month = apr,
  number = {arXiv:1810.00278},
  eprint = {1810.00278},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/1810.00278},
  urldate = {2024-10-01},
  abstract = {Even though machine learning has become the major scene in dialogue research community, the real breakthrough has been blocked by the scale of data available. To address this fundamental obstacle, we introduce the MultiDomain Wizard-of-Oz dataset (MultiWOZ), a fully-labeled collection of human-human written conversations spanning over multiple domains and topics. At a size of 10k dialogues, it is at least one order of magnitude larger than all previous annotated task-oriented corpora. The contribution of this work apart from the open-sourced dataset labelled with dialogue belief states and dialogue actions is two-fold: firstly, a detailed description of the data collection procedure along with a summary of data structure and analysis is provided. The proposed data-collection pipeline is entirely based on crowd-sourcing without the need of hiring professional annotators; secondly, a set of benchmark results of belief tracking, dialogue act and response generation is reported, which shows the usability of the data and sets a baseline for future studies.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language}
}

@inproceedings{caiGenerationPatientAfterVisit2022,
  title = {Generation of {{Patient After-Visit Summaries}} to {{Support Physicians}}},
  booktitle = {Proceedings of the 29th {{International Conference}} on {{Computational Linguistics}}},
  author = {Cai, Pengshan and Liu, Fei and Bajracharya, Adarsha and Sills, Joe and Kapoor, Alok and Liu, Weisong and Berlowitz, Dan and Levy, David and Pradhan, Richeek and Yu, Hong},
  editor = {Calzolari, Nicoletta and Huang, Chu-Ren and Kim, Hansaem and Pustejovsky, James and Wanner, Leo and Choi, Key-Sun and Ryu, Pum-Mo and Chen, Hsin-Hsi and Donatelli, Lucia and Ji, Heng and Kurohashi, Sadao and Paggio, Patrizia and Xue, Nianwen and Kim, Seokhwan and Hahm, Younggyun and He, Zhong and Lee, Tony Kyungil and Santus, Enrico and Bond, Francis and Na, Seung-Hoon},
  year = {2022},
  month = oct,
  pages = {6234--6247},
  publisher = {International Committee on Computational Linguistics},
  address = {Gyeongju, Republic of Korea},
  url = {https://aclanthology.org/2022.coling-1.544},
  urldate = {2024-03-18},
  abstract = {An after-visit summary (AVS) is a summary note given to patients after their clinical visit. It recaps what happened during their clinical visit and guides patients' disease self-management. Studies have shown that a majority of patients found after-visit summaries useful. However, many physicians face excessive workloads and do not have time to write clear and informative summaries. In this paper, we study the problem of automatic generation of after-visit summaries and examine whether those summaries can convey the gist of clinical visits. We report our findings on a new clinical dataset that contains a large number of electronic health record (EHR) notes and their associated summaries. Our results suggest that generation of lay language after-visit summaries remains a challenging task. Crucially, we introduce a feedback mechanism that alerts physicians when an automatic summary fails to capture the important details of the clinical notes or when it contains hallucinated facts that are potentially detrimental to the summary quality. Automatic and human evaluation demonstrates the effectiveness of our approach in providing writing feedback and supporting physicians.}
}

@misc{CalibratingImbalancedClassifiers,
  title = {Calibrating {{Imbalanced Classifiers}} with {{Focal Loss}}: {{An Empirical Study}} - {{ACL Anthology}}},
  url = {https://aclanthology.org/2022.emnlp-industry.14/},
  urldate = {2023-06-20}
}

@inproceedings{calvanoLeveragingLargeLanguage2025,
  title = {Leveraging {{Large Language Models}} for {{Usability Testing}}: A {{Preliminary Study}}},
  shorttitle = {Leveraging {{Large Language Models}} for {{Usability Testing}}},
  booktitle = {Companion {{Proceedings}} of the 30th {{International Conference}} on {{Intelligent User Interfaces}}},
  author = {Calvano, Miriana and Curci, Antonio and Lanzilotti, Rosa and Piccinno, Antonio and Ragone, Azzurra},
  year = {2025},
  month = mar,
  series = {{{IUI}} '25 {{Companion}}},
  pages = {78--81},
  publisher = {ACM},
  address = {Cagliari Italy},
  doi = {10.1145/3708557.3716341},
  urldate = {2025-03-31},
  abstract = {Despite growing efforts to prioritize user experience in product development, software organizations often perform little or no usability engineering activities. Therefore, it is crucial to develop strategies to integrate them effectively into software development processes. The rapid advances in Artificial Intelligence have significantly influenced various aspects of daily life, particularly with the emergence of Large Language Models (LLMs), which can serve as promising tools to support activities to enhance the usability of software products. This paper presents a study investigating the potential of LLMs to assist practitioners in conducting usability tests. Specifically, we conducted an experiment where LLMs generate usability test tasks. Our goal is to assess whether AI can effectively support evaluators by comparing tasks generated by LLMs to those defined by usability experts. The findings indicate that while LLMs can provide valuable support, effective usability testing still requires human oversight and expert intervention.},
  isbn = {979-8-4007-1409-2},
  langid = {english}
}

@article{camposYAKEKeywordExtraction2020,
  title = {{{YAKE}}! {{Keyword}} Extraction from Single Documents Using Multiple Local Features},
  author = {Campos, Ricardo and Mangaravite, V{\'i}tor and Pasquali, Arian and Jorge, Al{\'i}pio and Nunes, C{\'e}lia and Jatowt, Adam},
  year = {2020},
  month = jan,
  journal = {Information Sciences},
  volume = {509},
  pages = {257--289},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2019.09.013},
  urldate = {2022-10-17},
  abstract = {As the amount of generated information grows, reading and summarizing texts of large collections turns into a challenging task. Many documents do not come with descriptive terms, thus requiring humans to generate keywords on-the-fly. The need to automate this kind of task demands the development of keyword extraction systems with the ability to automatically identify keywords within the text. One approach is to resort to machine-learning algorithms. These, however, depend on large annotated text corpora, which are not always available. An alternative solution is to consider an unsupervised approach. In this article, we describe YAKE!, a light-weight unsupervised automatic keyword extraction method which rests on statistical text features extracted from single documents to select the most relevant keywords of a text. Our system does not need to be trained on a particular set of documents, nor does it depend on dictionaries, external corpora, text size, language, or domain. To demonstrate the merits and significance of YAKE!, we compare it against ten state-of-the-art unsupervised approaches and one supervised method. Experimental results carried out on top of twenty datasets show that YAKE! significantly outperforms other unsupervised methods on texts of different sizes, languages, and domains.},
  langid = {english}
}

@article{caoThinkingFastSlow2023,
  title = {Thinking Fast and Slow: A Revised {{SOR}} Model for an Empirical Examination of Impulse Buying at a Luxury Fashion Outlet},
  shorttitle = {Thinking Fast and Slow},
  author = {Cao, Dongmei and Meadows, Maureen and Ma, Xiao},
  year = {2023},
  month = jan,
  journal = {European Journal of Marketing},
  volume = {58},
  number = {1},
  pages = {342--368},
  publisher = {Emerald Publishing Limited},
  issn = {0309-0566},
  doi = {10.1108/EJM-01-2022-0046},
  urldate = {2024-08-13},
  abstract = {Purpose Despite the extensive stimulus--organism--response (SOR) literature, little attention has been paid to the role of marketing activity as a key environmental stimulus, and there is a dearth of research examining the interplay between emotions and cognition on consumer behaviour, as well as the sequential effects of emotions on cognition. To address these gaps, this study aims to develop a revised SOR model by incorporating Kahneman's fast and slow thinking theory to investigate the impulse buying of affordable luxury fashion (ALF). Design/methodology/approach The authors use outlet stores at Bicester village (BV) in England as the research context for ALF shopping. Partial least squares structural equation modelling was used to analyse a survey sample of 633 consumers with a BV shopping experience. Findings The authors find that impulse buying of ALF arises from the interplay of emotional and cognitive factors, as well as a sequential and dual process involving in-store stimuli affecting on-site emotion and in-store browsing. Research limitations/implications This study reveals that brand connection has a significant and negative influence on the relationship between on-site emotion and in-store browsing, advancing the SOR paradigm and reflecting the interactive effect of human emotion and reasoning on the impulse buying of ALF items. Practical implications Insights into consumers' impulse buying offer practical implications for luxury brand management, specifically for ALF outlet retailers and store managers. Originality/value The results suggest a robust sequential effect of on-site emotion towards in-store browsing on impulse buying, providing updated empirical support for Kahneman's theory of System 1 and System 2 thinking.},
  keywords = {Affordable luxury fashion (ALF),Brand connection,Impulse buying,In-store browsing,On-site emotion,Retail outlet,Stimulus-organism-response (SOR)}
}

@misc{cassanoMultiPLEScalableExtensible2022,
  title = {{{MultiPL-E}}: {{A Scalable}} and {{Extensible Approach}} to {{Benchmarking Neural Code Generation}}},
  shorttitle = {{{MultiPL-E}}},
  author = {Cassano, Federico and Gouwar, John and Nguyen, Daniel and Nguyen, Sydney and {Phipps-Costin}, Luna and Pinckney, Donald and Yee, Ming-Ho and Zi, Yangtian and Anderson, Carolyn Jane and Feldman, Molly Q. and Guha, Arjun and Greenberg, Michael and Jangda, Abhinav},
  year = {2022},
  month = dec,
  number = {arXiv:2208.08227},
  eprint = {2208.08227},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2208.08227},
  urldate = {2024-01-11},
  abstract = {Large language models have demonstrated the ability to generate both natural language and programming language text. Such models open up the possibility of multi-language code generation: could code generation models generalize knowledge from one language to another? Although contemporary code generation models can generate semantically correct Python code, little is known about their abilities with other languages. We propose MultiPL-E, a system for translating unit test-driven code generation benchmarks to new languages. We create the first massively multilingual code generation benchmark by using MultiPL-E to translate two popular Python code generation benchmarks to 18 additional programming languages. We use MultiPL-E to extend the HumanEval benchmark and MBPP benchmark to 18 languages that encompass a range of programming paradigms and popularity. Using these new parallel benchmarks, we evaluate the multi-language performance of three state-of-the-art code generation models: Codex, CodeGen, and InCoder. We find that Codex matches or even exceeds its performance on Python for several other languages. The range of programming languages represented in MultiPL-E allow us to explore the impact of language frequency and language features on model performance. Finally, the MultiPL-E approach of compiling code generation benchmarks to new programming languages is both scalable and extensible, making it straightforward to evaluate new models, benchmarks, and languages.},
  archiveprefix = {arXiv}
}

@inproceedings{chalkidisLargeScaleMultiLabelText2019,
  title = {Large-{{Scale Multi-Label Text Classification}} on {{EU Legislation}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Chalkidis, Ilias and Fergadiotis, Emmanouil and Malakasiotis, Prodromos and Androutsopoulos, Ion},
  year = {2019},
  month = jul,
  pages = {6314--6322},
  publisher = {Association for Computational Linguistics},
  address = {Florence, Italy},
  doi = {10.18653/v1/P19-1636},
  urldate = {2023-10-23},
  abstract = {We consider Large-Scale Multi-Label Text Classification (LMTC) in the legal domain. We release a new dataset of 57k legislative documents from EUR-LEX, annotated with {$\sim$}4.3k EUROVOC labels, which is suitable for LMTC, few- and zero-shot learning. Experimenting with several neural classifiers, we show that BIGRUs with label-wise attention perform better than other current state of the art methods. Domain-specific WORD2VEC and context-sensitive ELMO embeddings further improve performance. We also find that considering only particular zones of the documents is sufficient. This allows us to bypass BERT's maximum text length limit and fine-tune BERT, obtaining the best results in all but zero-shot learning cases.},
  langid = {english}
}

@inproceedings{changAlloyClusteringCrowds2016,
  title = {Alloy: {{Clustering}} with {{Crowds}} and {{Computation}}},
  shorttitle = {Alloy},
  booktitle = {Proceedings of the 2016 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Chang, Joseph Chee and Kittur, Aniket and Hahn, Nathan},
  year = {2016},
  month = may,
  series = {{{CHI}} '16},
  pages = {3180--3191},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2858036.2858411},
  urldate = {2023-08-31},
  abstract = {Crowdsourced clustering approaches present a promising way to harness deep semantic knowledge for clustering complex information. However, existing approaches have difficulties supporting the global context needed for workers to generate meaningful categories, and are costly because all items require human judgments. We introduce Alloy, a hybrid approach that combines the richness of human judgments with the power of machine algorithms. Alloy supports greater global context through a new "sample and search" crowd pattern which changes the crowd's task from classifying a fixed subset of items to actively sampling and querying the entire dataset. It also improves efficiency through a two phase process in which crowds provide examples to help a machine cluster the head of the distribution, then classify low-confidence examples in the tail. To accomplish this, Alloy introduces a modular "cast and gather" approach which leverages a machine learning backbone to stitch together different types of judgment tasks.},
  isbn = {978-1-4503-3362-7}
}

@inproceedings{changUsingExploringHierarchical2016,
  title = {Using and {{Exploring Hierarchical Data}} in {{Spreadsheets}}},
  booktitle = {Proceedings of the 2016 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Chang, Kerry Shih-Ping and Myers, Brad A.},
  year = {2016},
  month = may,
  series = {{{CHI}} '16},
  pages = {2497--2507},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2858036.2858430},
  urldate = {2023-08-31},
  abstract = {More and more data nowadays exist in hierarchical formats such as JSON due to the increasing popularity of web applications and web services. While many end-user systems support getting hierarchical data from databases without programming, they provide very little support for using hierarchical data beyond turning the data into a flat string or table. In this paper, we present a spreadsheet tool for using and exploring hierarchical datasets. We introduce novel interaction techniques and algorithms to manipulate and visualize hierarchical data in a spreadsheet using the data's relative hierarchical relationships with the data in its adjacent columns. Our tool leverages the data's structural information to support selecting, grouping, joining, sorting and filtering hierarchical data in spreadsheets. Our lab study showed that our tool helped spreadsheet users complete data exploration tasks nearly two times faster than using Excel and even outperform programmers in most tasks.},
  isbn = {978-1-4503-3362-7}
}

@article{chanMangoMangoHow2018,
  title = {``{{Mango Mango}}, {{How}} to {{Let The Lettuce Dry Without A Spinner}}?'': {{Explore The Advantages And Challenges When Employing An LLM-Based Voice Assistant}} in {{Cooking Scenarios}}},
  author = {Chan, Szeyi and Li, Jiachen and Yao, Bingsheng and Mahmood, Amama and Huang, Chien-Ming and Jimison, Holly and Mynatt, Elizabeth D and Wang, Dakuo},
  year = {2018},
  abstract = {The rapid advancement of the Large Language Model (LLM) has created numerous potentials for integration with conversational assistants (CAs) assisting people in their daily tasks, particularly due to their extensive flexibility. However, users' real-world experiences interacting with these assistants remain unexplored. In this research, we chose cooking, a complex daily task, as a scenario to investigate people's successful and unsatisfactory experiences while receiving assistance from an LLM-based CA, Mango Mango. We discovered that participants value the system's ability to provide extensive information beyond the recipe, offer customized instructions based on context, and assist them in dynamically planning the task. However, they expect the system to be more adaptive to oral conversation and provide more suggestive responses to keep users actively involved. Recognizing that users began treating our LLM-CA as a personal assistant or even a partner rather than just a recipe-reading tool, we propose several design considerations for future development. CCS Concepts: {$\bullet$} Human-centered computing {$\rightarrow$} User studies; Sound-based input / output; Auditory feedback; Empirical studies in HCI.},
  langid = {english}
}

@misc{chanMangoMangoHow2023,
  title = {"{{Mango Mango}}, {{How}} to {{Let The Lettuce Dry Without A Spinner}}?'': {{Exploring User Perceptions}} of {{Using An LLM-Based Conversational Assistant Toward Cooking Partner}}},
  shorttitle = {"{{Mango Mango}}, {{How}} to {{Let The Lettuce Dry Without A Spinner}}?},
  author = {Chan, Szeyi and Li, Jiachen and Yao, Bingsheng and Mahmood, Amama and Huang, Chien-Ming and Jimison, Holly and Mynatt, Elizabeth D. and Wang, Dakuo},
  year = {2023},
  month = oct,
  number = {arXiv:2310.05853},
  eprint = {2310.05853},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.05853},
  urldate = {2024-05-02},
  abstract = {The rapid advancement of the Large Language Model (LLM) has created numerous potentials for integration with conversational assistants (CAs) assisting people in their daily tasks, particularly due to their extensive flexibility. However, users' real-world experiences interacting with these assistants remain unexplored. In this research, we chose cooking, a complex daily task, as a scenario to investigate people's successful and unsatisfactory experiences while receiving assistance from an LLM-based CA, Mango Mango. We discovered that participants value the system's ability to provide extensive information beyond the recipe, offer customized instructions based on context, and assist them in dynamically planning the task. However, they expect the system to be more adaptive to oral conversation and provide more suggestive responses to keep users actively involved. Recognizing that users began treating our LLM-CA as a personal assistant or even a partner rather than just a recipe-reading tool, we propose several design considerations for future development.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction}
}

@misc{chanScalingSyntheticData2024,
  title = {Scaling {{Synthetic Data Creation}} with 1,000,000,000 {{Personas}}},
  author = {Chan, Xin and Wang, Xiaoyang and Yu, Dian and Mi, Haitao and Yu, Dong},
  year = {2024},
  month = jun,
  number = {arXiv:2406.20094},
  eprint = {2406.20094},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2406.20094},
  urldate = {2024-07-05},
  abstract = {We propose a novel persona-driven data synthesis methodology that leverages various perspectives within a large language model (LLM) to create diverse synthetic data. To fully exploit this methodology at scale, we introduce Persona Hub -- a collection of 1 billion diverse personas automatically curated from web data. These 1 billion personas ({$\sim$}13\% of the world's total population), acting as distributed carriers of world knowledge, can tap into almost every perspective encapsulated within the LLM, thereby facilitating the creation of diverse synthetic data at scale for various scenarios. By showcasing Persona Hub's use cases in synthesizing high-quality mathematical and logical reasoning problems, instructions (i.e., user prompts), knowledge-rich texts, game NPCs and tools (functions) at scale, we demonstrate persona-driven data synthesis is versatile, scalable, flexible, and easy to use, potentially driving a paradigm shift in synthetic data creation and applications in practice, which may have a profound impact on LLM research and development.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{chaoEchoGPTLargeLanguage2024,
  title = {{{EchoGPT}}: {{A Large Language Model}} for {{Echocardiography Report Summarization}}},
  shorttitle = {{{EchoGPT}}},
  author = {Chao, Chieh-Ju and Banerjee, Imon and Arsanjani, Reza and Ayoub, Chadi and Tseng, Andrew and Kane, Garvan C. and Oh, Jae K. and Li, Fei-Fei and Adeli, Ehsan and Langlotz, Curtis P.},
  year = {2024},
  month = jan,
  pages = {2024.01.18.24301503},
  publisher = {medRxiv},
  doi = {10.1101/2024.01.18.24301503},
  urldate = {2024-01-20},
  abstract = {Background The increasing need for diagnostic echocardiography (echo) tests presents challenges in preserving the quality and promptness of reports. While Large Language Models (LLMs) have proven effective in summarizing clinical texts, their application in echo remains underexplored. To address this, we proposed EchoGPT, a dedicated, domain-specific LLM focused on echo report summarization. Methods Adult echo studies conducted at the Mayo Clinic from January 1, 2017, to December 31, 2017, were collected and categorized into two groups: development (all Mayo locations except Arizona) and external validation (Mayo Arizona). We adapted open-source LLMs (Llama-2, MedAlpaca, Zephyr, and Flan-T5) using In-Context Learning (ICL) and Quantized Low-Rank Adaptation (QLoRA) fine-tuning for echo text summarization. The models' performance was assessed both quantitatively with automatic metrics and qualitatively by cardiologists. Results The development dataset included 97,506 reports from 71,717 unique patients, predominantly male (54.3\%), with an average age of 64.1+/-16.1 years. The final split contains 95,506 for training, and 1,000 each for validation and testing. EchoGPT, a QLoRA fine-tuned Llama-2 model, outperformed other LLMs with about 90\% win rates in various metrics (BLEU, METEOR, ROUGE-L, BERT Score, and RadGraph F1 Score), and produced reports comparable to cardiologists in 30 randomly selected cases for qualitative human review (significantly preferred in conciseness (p{$<$} 0.001), with no significant preference in completeness, correctness, and clinical utility). In the external validation set (n=1,000), EchoGPT consistently outperformed fine-tuned Zephyr model across the same automatic metrics (all p {$<$} 0.0001). Conclusions Capable of generating echocardiography reports on par with human experts, EchoGPT could be used to generate draft reports for human review and approval, with significant workflow advantages.},
  archiveprefix = {medRxiv},
  copyright = {{\copyright} 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english}
}

@article{chaoPilotRandomizedControlled2017,
  title = {A {{Pilot Randomized Controlled Trial}} of a {{Technology-Based Approach}} for {{Preventing Excess Weight Gain}} during {{Pregnancy}} among {{Women}} with {{Overweight}}},
  author = {Chao, Ariana M. and Srinivas, Sindhu K. and Studt, Stacia K. and Diewald, Lisa K. and Sarwer, David B. and Allison, Kelly C.},
  year = {2017},
  journal = {Frontiers in Nutrition},
  volume = {4},
  issn = {2296-861X},
  doi = {10.3389/fnut.2017.00057},
  urldate = {2024-02-12},
  abstract = {ObjectiveOverweight/obesity and excess weight gain during pregnancy are associated with adverse maternal and neonatal outcomes. Few interventions have been effective in limiting gestational weight gain among women with overweight or obesity. This pilot, randomized clinical trial compared treatment as usual (TAU) to a lifestyle modification program delivered via phone for the prevention of excess gestational weight gain in women who had overweight or obesity.MethodsParticipants included 41 pregnant women with a body mass index (BMI)\,{$\geq$}\,25\,kg/m2 (mean age\,=\,28.7\,{\textpm}\,5.8\,years; mean pre-gravid BMI\,=\,31.2\,{\textpm}\,6.2\,kg/m2; 54\% black, 39\% white). The intervention group (n\,=\,20) received weekly telephone counseling sessions and used WiFi scales to monitor their weight from weeks 16 to 36 of pregnancy. We compared differences in weight and birth outcomes for the intervention vs. the TAU group (n\,=\,21).ResultsThe intervention and TAU groups did not differ with respect to: gestational weight gain (15.5\,{\textpm}\,5.3 vs. 13.3\,{\textpm}\,6.8\,kg, respectively); proportion gaining above the 2009 Institute of Medicine recommended weight range (83 vs. 70\%); and weight gain from pre-pregnancy weight to 6\,weeks postpartum (4.8\,{\textpm}\,4.6 vs. 3.0\,{\textpm}\,5.5\,kg). Other birth and health outcomes also did not differ.ConclusionA telemedicine intervention designed to decrease logistical burden on participants was not more successful in reducing excessive weight gain during pregnancy as compared to TAU. Future studies should examine more intensive forms of remote treatment beginning earlier in pregnancy as well as interventions promoting a healthy weight prior to pregnancy.}
}

@misc{chenAgentFLANDesigningData2024,
  title = {Agent-{{FLAN}}: {{Designing Data}} and {{Methods}} of {{Effective Agent Tuning}} for {{Large Language Models}}},
  shorttitle = {Agent-{{FLAN}}},
  author = {Chen, Zehui and Liu, Kuikun and Wang, Qiuchen and Zhang, Wenwei and Liu, Jiangning and Lin, Dahua and Chen, Kai and Zhao, Feng},
  year = {2024},
  month = mar,
  number = {arXiv:2403.12881},
  eprint = {2403.12881},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.12881},
  urldate = {2025-03-20},
  abstract = {Open-sourced Large Language Models (LLMs) have achieved great success in various NLP tasks, however, they are still far inferior to API-based models when acting as agents. How to integrate agent ability into general LLMs becomes a crucial and urgent problem. This paper first delivers three key observations: (1) the current agent training corpus is entangled with both formats following and agent reasoning, which significantly shifts from the distribution of its pre-training data; (2) LLMs exhibit different learning speeds on the capabilities required by agent tasks; and (3) current approaches have side-effects when improving agent abilities by introducing hallucinations. Based on the above findings, we propose Agent-FLAN to effectively Fine-tune LANguage models for Agents. Through careful decomposition and redesign of the training corpus, Agent-FLAN enables Llama2-7B to outperform prior best works by 3.5{\textbackslash}\% across various agent evaluation datasets. With comprehensively constructed negative samples, Agent-FLAN greatly alleviates the hallucination issues based on our established evaluation benchmark. Besides, it consistently improves the agent capability of LLMs when scaling model sizes while slightly enhancing the general capability of LLMs. The code will be available at https://github.com/InternLM/Agent-FLAN.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@inproceedings{chenAutoGSRNeuralArchitecture2022,
  title = {{{AutoGSR}}: {{Neural Architecture Search}} for {{Graph-based Session Recommendation}}},
  shorttitle = {{{AutoGSR}}},
  booktitle = {Proceedings of the 45th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Chen, Jingfan and Zhu, Guanghui and Hou, Haojun and Yuan, Chunfeng and Huang, Yihua},
  year = {2022},
  month = jul,
  series = {{{SIGIR}} '22},
  pages = {1694--1704},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3477495.3531940},
  urldate = {2024-09-19},
  abstract = {Session-based recommendation aims to predict next click action (e.g., item) of anonymous users based on a fixed number of previous actions. Recently, Graph Neural Networks (GNNs) have shown superior performance in various applications. Inspired by the success of GNNs, tremendous endeavors have been devoted to introduce GNNs into session-based recommendation and have achieved significant results. Nevertheless, due to the highly diverse types of potential information in sessions, existing GNNs-based methods perform differently on different session datasets, leading to the need for efficient design of neural networks adapted to various session recommendation scenarios. To address this problem, we propose Automated neural architecture search for Graph-based Session Recommendation, namely AutoGSR, a framework that provides a practical and general solution to automatically find the optimal GNNs-based session recommendation model. In AutoGSR, we propose two novel GNN operations to build an expressive and compact search space. Building upon the search space, we employ a differentiable search algorithm to search for the optimal graph neural architecture. Furthermore, to consider all types of session information together, we propose to learn the item meta knowledge, which acts as a priori knowledge for guiding the optimization of final session representations. Comprehensive experiments on three real-world datasets demonstrate that AutoGSR is able to find effective neural architectures and achieve state-of-the-art results. To the best of our knowledge, we are the first to study the neural architecture search for the session-based recommendation.},
  isbn = {978-1-4503-8732-3}
}

@inproceedings{chenCharacterizingLLMEmpoweredPersonalized2025,
  title = {Characterizing {{LLM-Empowered Personalized Story Reading}} and {{Interaction}} for {{Children}}: {{Insights From Multi-Stakeholder Perspectives}}},
  shorttitle = {Characterizing {{LLM-Empowered Personalized Story Reading}} and {{Interaction}} for {{Children}}},
  booktitle = {Proceedings of the 2025 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Chen, Jiaju and Tang, Minglong and Lu, Yuxuan and Yao, Bingsheng and Fan, Elissa and Ma, Xiaojuan and Xu, Ying and Wang, Dakuo and Sun, Yuling and He, Liang},
  year = {2025},
  month = apr,
  series = {{{CHI}} '25},
  pages = {1--24},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3706598.3713275},
  urldate = {2025-04-30},
  abstract = {Personalized interaction is highly valued by parents in their story-reading activities with children. While AI-empowered story-reading tools have been increasingly used, their abilities to support personalized interaction with children are still limited. Recent advances in large language models (LLMs) show promise in facilitating personalized interactions, but little is known about how to effectively and appropriately use LLMs to enhance children's personalized story-reading experiences. This work explores this question through a design-based study. Drawing on a formative study, we designed and developed StoryMate, an LLM-empowered personalized interactive story-reading tool for children, following an empirical study with children, parents, and education experts. Our participants valued the personalized features in StoryMate, and also highlighted the need to support personalized content, guiding mechanisms, reading context variations, and interactive interfaces. Based on these findings, we propose a series of design recommendations for better using LLMs to empower children's personalized story reading and interaction.},
  isbn = {979-8-4007-1394-1}
}

@misc{chenControllableTextGeneration2022,
  title = {Controllable {{Text Generation}} with {{Language Constraints}}},
  author = {Chen, Howard and Li, Huihan and Chen, Danqi and Narasimhan, Karthik},
  year = {2022},
  month = dec,
  number = {arXiv:2212.10466},
  eprint = {2212.10466},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.10466},
  urldate = {2023-01-02},
  abstract = {We consider the task of text generation in language models with constraints specified in natural language. To this end, we first create a challenging benchmark Cognac that provides as input to the model a topic with example text, along with a constraint on text to be avoided. Unlike prior work, our benchmark contains knowledge-intensive constraints sourced from databases like Wordnet and Wikidata, which allows for straightforward evaluation while striking a balance between broad attribute-level and narrow lexical-level controls. We find that even state-of-the-art language models like GPT-3 fail often on this task, and propose a solution to leverage a language model's own internal knowledge to guide generation. Our method, called CognacGen, first queries the language model to generate guidance terms for a specified topic or constraint, and uses the guidance to modify the model's token generation probabilities. We propose three forms of guidance (binary verifier, top-k tokens, textual example), and employ prefix-tuning approaches to distill the guidance to tackle diverse natural language constraints. Through extensive empirical evaluations, we demonstrate that CognacGen can successfully generalize to unseen instructions and outperform competitive baselines in generating constraint conforming text.},
  archiveprefix = {arXiv}
}

@misc{chenDesignGuidelineRPA2025,
  title = {Towards a {{Design Guideline}} for {{RPA Evaluation}}: {{A Survey}} of {{Large Language Model-Based Role-Playing Agents}}},
  shorttitle = {Towards a {{Design Guideline}} for {{RPA Evaluation}}},
  author = {Chen, Chaoran and Yao, Bingsheng and Zou, Ruishi and Hua, Wenyue and Lyu, Weimin and Li, Toby Jia-Jun and Wang, Dakuo},
  year = {2025},
  month = feb,
  number = {arXiv:2502.13012},
  eprint = {2502.13012},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.13012},
  urldate = {2025-02-27},
  abstract = {Role-Playing Agent (RPA) is an increasingly popular type of LLM Agent that simulates human-like behaviors in a variety of tasks. However, evaluating RPAs is challenging due to diverse task requirements and agent designs. This paper proposes an evidence-based, actionable, and generalizable evaluation design guideline for LLM-based RPA by systematically reviewing 1,676 papers published between Jan. 2021 and Dec. 2024. Our analysis identifies six agent attributes, seven task attributes, and seven evaluation metrics from existing literature. Based on these findings, we present an RPA evaluation design guideline to help researchers develop more systematic and consistent evaluation methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Human-Computer Interaction}
}

@inproceedings{chenEmpathyBasedSandboxApproach2024,
  title = {An {{Empathy-Based Sandbox Approach}} to {{Bridge}} the {{Privacy Gap}} among {{Attitudes}}, {{Goals}}, {{Knowledge}}, and {{Behaviors}}},
  booktitle = {Proceedings of the {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Chen, Chaoran and Li, Weijun and Song, Wenxin and Ye, Yanfang and Yao, Yaxing and Li, Toby Jia-Jun},
  year = {2024},
  month = may,
  series = {{{CHI}} '24},
  pages = {1--28},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3613904.3642363},
  urldate = {2024-07-04},
  abstract = {Managing privacy to reach privacy goals is challenging, as evidenced by the privacy attitude-behavior gap. Mitigating this discrepancy requires solutions that account for both system opaqueness and users' hesitations in testing different privacy settings due to fears of unintended data exposure. We introduce an empathy-based approach that allows users to experience how privacy attributes may alter system outcomes in a risk-free sandbox environment from the perspective of artificially generated personas. To generate realistic personas, we introduce a novel pipeline that augments the outputs of large language models (e.g., GPT-4) using few-shot learning, contextualization, and chain of thoughts. Our empirical studies demonstrated the adequate quality of generated personas and highlighted the changes in privacy-related applications (e.g., online advertising) caused by different personas. Furthermore, users demonstrated cognitive and emotional empathy towards the personas when interacting with our sandbox. We offered design implications for downstream applications in improving user privacy literacy.},
  isbn = {979-8-4007-0330-0}
}

@misc{chenEvaluatingLargeLanguage2021,
  title = {Evaluating {{Large Language Models Trained}} on {{Code}}},
  author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and {Herbert-Voss}, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
  year = {2021},
  month = jul,
  number = {arXiv:2107.03374},
  eprint = {2107.03374},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2107.03374},
  urldate = {2024-01-11},
  abstract = {We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8\% of the problems, while GPT-3 solves 0\% and GPT-J solves 11.4\%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2\% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.},
  archiveprefix = {arXiv}
}

@article{chenEvaluatingLLMAgents,
  title = {Evaluating the {{LLM Agents}} for {{Simulating Humanoid Behavior}}},
  author = {Chen, Chaoran and Yao, Bingsheng and Ye, Yanfang and Wang, Dakuo and Li, Toby Jia-Jun},
  abstract = {Large Language Model (LLM)-based agents have showcased remarkable abilities in simulating human-like behavior, prompting widespread application across multiple fields. The growing use of these agents brings to light the critical need for robust evaluation metrics to assess their performance and for clear guidelines to direct their use across various downstream tasks. In this literature review, we identify three primary challenges in the evaluation of LLM agents: the overlook of evaluation metrics, the absence of a detailed taxonomy for aligning simulated humanoid behavior data with specific downstream tasks, and the disconnect between agent-oriented and task-oriented metrics. To tackle these issues, we summarized existing evaluation metrics and developed a comprehensive taxonomy for the research goals and the evaluation of LLM agents in simulating humanoid behavior. Through a systematic literature review, we aim to provide guidance for researchers in evaluating such LLM agents, ultimately mitigating the gap between the evaluation and the downstream tasks.},
  langid = {english},
  keywords = {No DOI found}
}

@inproceedings{chengSeeClickHarnessingGUI2024,
  title = {{{SeeClick}}: {{Harnessing GUI Grounding}} for {{Advanced Visual GUI Agents}}},
  shorttitle = {{{SeeClick}}},
  booktitle = {Proceedings of the 62nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Cheng, Kanzhi and Sun, Qiushi and Chu, Yougang and Xu, Fangzhi and YanTao, Li and Zhang, Jianbing and Wu, Zhiyong},
  editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
  year = {2024},
  month = aug,
  pages = {9313--9332},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.505},
  urldate = {2024-12-11},
  abstract = {Graphical User Interface (GUI) agents are designed to automate complex tasks on digital devices, such as smartphones and desktops. Most existing GUI agents interact with the environment through extracted structured data, which can be notably lengthy (e.g., HTML) and occasionally inaccessible (e.g., on desktops). To alleviate this issue, we propose a novel visual GUI agent -- SeeClick, which only relies on screenshots for task automation. In our preliminary study, we have discovered a key challenge in developing visual GUI agents: GUI grounding -- the capacity to accurately locate screen elements based on instructions. To tackle this challenge, we propose to enhance SeeClick with GUI grounding pre-training and devise a method to automate the curation of GUI grounding data. Along with the efforts above, we have also created ScreenSpot, the first realistic GUI grounding benchmark that encompasses mobile, desktop, and web environments. After pre-training, SeeClick demonstrates significant improvement in ScreenSpot over various baselines. Moreover, comprehensive evaluations on three widely used benchmarks consistently support our finding that advancements in GUI grounding directly correlate with enhanced performance in downstream GUI agent tasks. The model, data and code will be open-sourced.}
}

@inproceedings{chenInterpretingTrajectoriesMultiple2022,
  title = {Interpreting {{Trajectories}} from {{Multiple Views}}: {{A Hierarchical Self-Attention Network}} for {{Estimating}} the {{Time}} of {{Arrival}}},
  shorttitle = {Interpreting {{Trajectories}} from {{Multiple Views}}},
  booktitle = {Proceedings of the 28th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Chen, Zebin and Xiao, Xiaolin and Gong, Yue-Jiao and Fang, Jun and Ma, Nan and Chai, Hua and Cao, Zhiguang},
  year = {2022},
  month = aug,
  series = {{{KDD}} '22},
  pages = {2771--2779},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3534678.3539051},
  urldate = {2023-02-01},
  abstract = {Estimating the time of arrival is a crucial task in intelligent transportation systems. Although considerable efforts have been made to solve this problem, most of them decompose a trajectory into several segments and then compute the travel time by integrating the attributes from all segments. The segment view, though being able to depict the local traffic conditions straightforwardly, is insufficient to embody the intrinsic structure of trajectories on the road network. To overcome the limitation, this study proposes multi-view trajectory representation that comprehensively interprets a trajectory from the segment-, link-, and intersection-views. To fulfill the purpose, we design a hierarchical self-attention network (HierETA) that accurately models the local traffic conditions and the underlying trajectory structure. Specifically, a segment encoder is developed to capture the spatio-temporal dependencies at a fine granularity, within which an adaptive self-attention module is designed to boost performance. Further, a joint link-intersection encoder is developed to characterize the natural trajectory structure consisting of alternatively arranged links and intersections. Afterward, a hierarchy-aware attention decoder is designed to realize a tradeoff between the multi-view spatio-temporal features. The hierarchical encoders and the attentive decoder are simultaneously learned to achieve an overall optimality. Experiments on two large-scale practical datasets show the superiority of HierETA over the state-of-the-arts.},
  isbn = {978-1-4503-9385-0}
}

@article{chenLearningQLargeScaleDataset2018,
  title = {{{LearningQ}}: {{A Large-Scale Dataset}} for {{Educational Question Generation}}},
  shorttitle = {{{LearningQ}}},
  author = {Chen, Guanliang and Yang, Jie and Hauff, Claudia and Houben, Geert-Jan},
  year = {2018},
  month = jun,
  journal = {Proceedings of the International AAAI Conference on Web and Social Media},
  volume = {12},
  number = {1},
  issn = {2334-0770},
  doi = {10.1609/icwsm.v12i1.14987},
  urldate = {2023-11-07},
  abstract = {We present LearningQ, a challenging educational question generation dataset containing over 230K document-question pairs. It includes 7K instructor-designed questions assessing knowledge concepts being taught and 223K learner-generated questions seeking in-depth understanding of the taught concepts. We show that, compared to existing datasets that can be used to generate educational questions, LearningQ (i) covers a wide range of educational topics and (ii) contains long and cognitively demanding documents for which question generation requires reasoning over the relationships between sentences and paragraphs. As a result, a significant percentage of LearningQ questions ({\textasciitilde}30\%) require higher-order cognitive skills to solve (such as applying, analyzing), in contrast to existing question-generation datasets that are designed mostly for the lowest cognitive skill level (i.e. remembering). To understand the effectiveness of existing question generation methods in producing educational questions, we evaluate both rule-based and deep neural network based methods on LearningQ. Extensive experiments show that state-of-the-art methods which perform well on existing datasets cannot generate useful educational questions. This implies that LearningQ is a challenging test bed for the generation of high-quality educational questions and worth further investigation. We open-source the dataset and our codes at https://dataverse.mpi-sws.org/dataverse/icwsm18.},
  copyright = {Copyright (c) 2022 Proceedings of the International AAAI Conference on Web and Social Media},
  langid = {english}
}

@misc{chenMakingYourFirst2022,
  title = {Making {{Your First Choice}}: {{To Address Cold Start Problem}} in {{Vision Active Learning}}},
  shorttitle = {Making {{Your First Choice}}},
  author = {Chen, Liangyu and Bai, Yutong and Huang, Siyu and Lu, Yongyi and Wen, Bihan and Yuille, Alan L. and Zhou, Zongwei},
  year = {2022},
  month = oct,
  number = {arXiv:2210.02442},
  eprint = {2210.02442},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2210.02442},
  urldate = {2023-12-15},
  abstract = {Active learning promises to improve annotation efficiency by iteratively selecting the most important data to be annotated first. However, we uncover a striking contradiction to this promise: active learning fails to select data as efficiently as random selection at the first few choices. We identify this as the cold start problem in vision active learning, caused by a biased and outlier initial query. This paper seeks to address the cold start problem by exploiting the three advantages of contrastive learning: (1) no annotation is required; (2) label diversity is ensured by pseudo-labels to mitigate bias; (3) typical data is determined by contrastive features to reduce outliers. Experiments are conducted on CIFAR-10-LT and three medical imaging datasets (i.e. Colon Pathology, Abdominal CT, and Blood Cell Microscope). Our initial query not only significantly outperforms existing active querying strategies but also surpasses random selection by a large margin. We foresee our solution to the cold start problem as a simple yet strong baseline to choose the initial query for vision active learning. Code is available: https://github.com/c-liangyu/CSVAL},
  archiveprefix = {arXiv}
}

@inproceedings{chenMakingYourFirst2023,
  title = {Making {{Your First Choice}}: {{To Address Cold Start Problem}} in {{Medical Active Learning}}},
  shorttitle = {Making {{Your First Choice}}},
  booktitle = {International {{Conference}} on {{Medical Imaging}} with {{Deep Learning}}},
  author = {Chen, Liangyu and Yuille, A. and Zhou, Zongwei},
  year = {2023},
  url = {https://www.semanticscholar.org/paper/Making-Your-First-Choice%3A-To-Address-Cold-Start-in-Chen-Yuille/251516c1549fc4566b801788c932ef1f18f343b3},
  urldate = {2024-04-25},
  abstract = {Active learning promises to improve annotation efficiency by iteratively selecting the most important data to be annotated first. However, we uncover a striking contradiction to this promise: at the first few choices, active learning fails to select data as efficiently as random selection. We identify this as the cold start problem in active learning, caused by a biased and outlier initial query. This paper seeks to address the cold start problem and develops a novel active querying strategy, named HaCon , that can exploit the three advantages of contrastive learning: (1) no annotation is required; (2) label diversity is ensured by pseudo-labels to mitigate bias; (3) typical data is determined by contrastive features to reduce outliers. Experiments on three public medical datasets show that HaCon not only significantly outperforms existing active querying strategies but also surpasses random selection by a large margin. Code is available at https://github.com/cliangyu/CSVAL .},
  langid = {american},
  keywords = {Cold Start Active Learning,No DOI found}
}

@inproceedings{chenMultichoiceRelationalReasoning2020,
  title = {Multi-Choice {{Relational Reasoning}} for {{Machine Reading Comprehension}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Computational Linguistics}}},
  author = {Chen, Wuya and Quan, Xiaojun and Kit, Chunyu and Min, Zhengcheng and Wang, Jiahai},
  year = {2020},
  pages = {6448--6458},
  publisher = {International Committee on Computational Linguistics},
  address = {Barcelona, Spain (Online)},
  doi = {10.18653/v1/2020.coling-main.567},
  urldate = {2022-04-12},
  abstract = {This paper presents our study of cloze-style reading comprehension by imitating human reading comprehension, which normally involves tactical comparing and reasoning over candidates while choosing the best answer. We propose a multi-choice relational reasoning (McR\${\textasciicircum}2\$) model with an aim to enable relational reasoning on candidates based on fusion representations of document, query and candidates. For the fusion representations, we develop an efficient encoding architecture by integrating the schemes of bidirectional attention flow, self-attention and document-gated query reading. Then, comparing and inferring over candidates are executed by a novel relational reasoning network. We conduct extensive experiments on four datasets derived from two public corpora, Children's Book Test and Who DiD What, to verify the validity and advantages of our model. The results show that it outperforms all baseline models significantly on the four benchmark datasets. The effectiveness of its key components is also validated by an ablation study.}
}

@article{chenOTreeOpensourcePlatform2016,
  title = {{{oTree}}---{{An}} Open-Source Platform for Laboratory, Online, and Field Experiments},
  author = {Chen, Daniel L. and Schonger, Martin and Wickens, Chris},
  year = {2016},
  month = mar,
  journal = {Journal of Behavioral and Experimental Finance},
  volume = {9},
  pages = {88--97},
  issn = {2214-6350},
  doi = {10.1016/j.jbef.2015.12.001},
  urldate = {2024-09-13},
  abstract = {oTree is an open-source and online software for implementing interactive experiments in the laboratory, online, the field or combinations thereof. oTree does not require installation of software on subjects' devices; it can run on any device that has a web browser, be that a desktop computer, a tablet or a smartphone. Deployment can be internet-based without a shared local network, or local-network-based even without internet access. For coding, Python is used, a popular, open-source programming language. www.oTree.org provides the source code, a library of standard game templates and demo games which can be played by anyone.},
  keywords = {Classroom experiments,Experimental economics,Field experiments,Laboratory experiments,Online experiments,Software}
}

@inproceedings{chenPersonalizedChitChatGeneration2022,
  title = {Personalized {{Chit-Chat Generation}} for {{Recommendation Using External Chat Corpora}}},
  booktitle = {Proceedings of the 28th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Chen, Changyu and Wang, Xiting and Yi, Xiaoyuan and Wu, Fangzhao and Xie, Xing and Yan, Rui},
  year = {2022},
  month = aug,
  series = {{{KDD}} '22},
  pages = {2721--2731},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3534678.3539215},
  urldate = {2023-02-01},
  abstract = {Chit-chat has been shown effective in engaging users in human-computer interaction. We find with a user study that generating appropriate chit-chat for news articles can help expand user interest and increase the probability that a user reads a recommended news article. Based on this observation, we propose a method to generate personalized chit-chat for news recommendation. Different from existing methods for personalized text generation, our method only requires an external chat corpus obtained from an online forum, which can be disconnected from the recommendation dataset from both the user and item (news) perspectives. This is achieved by designing a weak supervision method for estimating users' personalized interest in a chit-chat post by transferring knowledge learned by a news recommendation model. Based on the method for estimating user interest, a reinforcement learning framework is proposed to generate personalized chit-chat. Extensive experiments, including the automatic offline evaluation and user studies, demonstrate the effectiveness of our method.},
  isbn = {978-1-4503-9385-0}
}

@misc{chenPersonaPersonalizationSurvey2024,
  title = {From {{Persona}} to {{Personalization}}: {{A Survey}} on {{Role-Playing Language Agents}}},
  shorttitle = {From {{Persona}} to {{Personalization}}},
  author = {Chen, Jiangjie and Wang, Xintao and Xu, Rui and Yuan, Siyu and Zhang, Yikai and Shi, Wei and Xie, Jian and Li, Shuang and Yang, Ruihan and Zhu, Tinghui and Chen, Aili and Li, Nianqi and Chen, Lida and Hu, Caiyu and Wu, Siye and Ren, Scott and Fu, Ziquan and Xiao, Yanghua},
  year = {2024},
  month = oct,
  number = {arXiv:2404.18231},
  eprint = {2404.18231},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.18231},
  urldate = {2025-03-20},
  abstract = {Recent advancements in large language models (LLMs) have significantly boosted the rise of Role-Playing Language Agents (RPLAs), i.e., specialized AI systems designed to simulate assigned personas. By harnessing multiple advanced abilities of LLMs, including in-context learning, instruction following, and social intelligence, RPLAs achieve a remarkable sense of human likeness and vivid role-playing performance. RPLAs can mimic a wide range of personas, ranging from historical figures and fictional characters to real-life individuals. Consequently, they have catalyzed numerous AI applications, such as emotional companions, interactive video games, personalized assistants and copilots, and digital clones. In this paper, we conduct a comprehensive survey of this field, illustrating the evolution and recent progress in RPLAs integrating with cutting-edge LLM technologies. We categorize personas into three types: 1) Demographic Persona, which leverages statistical stereotypes; 2) Character Persona, focused on well-established figures; and 3) Individualized Persona, customized through ongoing user interactions for personalized services. We begin by presenting a comprehensive overview of current methodologies for RPLAs, followed by the details for each persona type, covering corresponding data sourcing, agent construction, and evaluation. Afterward, we discuss the fundamental risks, existing limitations, and future prospects of RPLAs. Additionally, we provide a brief review of RPLAs in AI applications, which reflects practical user demands that shape and drive RPLA research. Through this work, we aim to establish a clear taxonomy of RPLA research and applications, and facilitate future research in this critical and ever-evolving field, and pave the way for a future where humans and RPLAs coexist in harmony.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@misc{chenRareBenchCanLLMs2024,
  title = {{{RareBench}}: {{Can LLMs Serve}} as {{Rare Diseases Specialists}}?},
  shorttitle = {{{RareBench}}},
  author = {Chen, Xuanzhong and Mao, Xiaohao and Guo, Qihan and Wang, Lun and Zhang, Shuyang and Chen, Ting},
  year = {2024},
  month = feb,
  number = {arXiv:2402.06341},
  eprint = {2402.06341},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2402.06341},
  urldate = {2024-02-12},
  abstract = {Generalist Large Language Models (LLMs), such as GPT-4, have shown considerable promise in various domains, including medical diagnosis. Rare diseases, affecting approximately 300 million people worldwide, often have unsatisfactory clinical diagnosis rates primarily due to a lack of experienced physicians and the complexity of differentiating among many rare diseases. In this context, recent news such as "ChatGPT correctly diagnosed a 4-year-old's rare disease after 17 doctors failed" underscore LLMs' potential, yet underexplored, role in clinically diagnosing rare diseases. To bridge this research gap, we introduce RareBench, a pioneering benchmark designed to systematically evaluate the capabilities of LLMs on 4 critical dimensions within the realm of rare diseases. Meanwhile, we have compiled the largest open-source dataset on rare disease patients, establishing a benchmark for future studies in this domain. To facilitate differential diagnosis of rare diseases, we develop a dynamic few-shot prompt methodology, leveraging a comprehensive rare disease knowledge graph synthesized from multiple knowledge bases, significantly enhancing LLMs' diagnostic performance. Moreover, we present an exhaustive comparative study of GPT-4's diagnostic capabilities against those of specialist physicians. Our experimental findings underscore the promising potential of integrating LLMs into the clinical diagnostic process for rare diseases. This paves the way for exciting possibilities in future advancements in this field.},
  archiveprefix = {arXiv}
}

@misc{chenRMR1RewardModeling2025,
  title = {{{RM-R1}}: {{Reward Modeling}} as {{Reasoning}}},
  shorttitle = {{{RM-R1}}},
  author = {Chen, Xiusi and Li, Gaotang and Wang, Ziqi and Jin, Bowen and Qian, Cheng and Wang, Yu and Wang, Hongru and Zhang, Yu and Zhang, Denghui and Zhang, Tong and Tong, Hanghang and Ji, Heng},
  year = {2025},
  month = may,
  number = {arXiv:2505.02387},
  eprint = {2505.02387},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.02387},
  urldate = {2025-06-04},
  abstract = {Reward modeling is essential for aligning large language models with human preferences through reinforcement learning from human feedback. To provide accurate reward signals, a reward model (RM) should stimulate deep thinking and conduct interpretable reasoning before assigning a score or a judgment. Inspired by recent advances of long chain-of-thought on reasoning-intensive tasks, we hypothesize and validate that integrating reasoning capabilities into reward modeling significantly enhances RMs interpretability and performance. To this end, we introduce a new class of generative reward models - Reasoning Reward Models (ReasRMs) - which formulate reward modeling as a reasoning task. We propose a reasoning-oriented training pipeline and train a family of ReasRMs, RM-R1. RM-R1 features a chain-of-rubrics (CoR) mechanism - self-generating sample-level chat rubrics or math/code solutions, and evaluating candidate responses against them. The training of RM-R1 consists of two key stages: (1) distillation of high-quality reasoning chains and (2) reinforcement learning with verifiable rewards. Empirically, our models achieve state-of-the-art performance across three reward model benchmarks on average, outperforming much larger open-weight models (e.g., INF-ORM-Llama3.1-70B) and proprietary ones (e.g., GPT-4o) by up to 4.9\%. Beyond final performance, we perform thorough empirical analyses to understand the key ingredients of successful ReasRM training. To facilitate future research, we release six REASRM models along with code and data at https://github.com/RM-R1-UIUC/RM-R1.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@article{chenSelfSupervisedReinforcementLearning,
  title = {Self-{{Supervised Reinforcement Learning}} That {{Transfers}} Using {{Random Features}}},
  author = {Chen, Boyuan and Zhu, Chuning and Agrawal, Pulkit and Zhang, Kaiqing and Gupta, Abhishek},
  abstract = {Model-free reinforcement learning algorithms have exhibited great potential in solving single-task sequential decision-making problems with high-dimensional observations and long horizons, but are known to be hard to generalize across tasks. Model-based RL, on the other hand, learns task-agnostic models of the world that naturally enables transfer across different reward functions, but struggles to scale to complex environments due to the compounding error. To get the best of both worlds, we propose a self-supervised reinforcement learning method that enables the transfer of behaviors across tasks with different rewards, while circumventing the challenges of model-based RL. In particular, we show self-supervised pretraining of model-free reinforcement learning with a number of random features as rewards allows implicit modeling of long-horizon environment dynamics. Then, planning techniques like model-predictive control using these implicit models enable fast adaptation to problems with new reward functions. Our method is self-supervised in that it can be trained on offline datasets without reward labels, but can then be quickly deployed on new tasks. We validate that our proposed method enables transfer across tasks on a variety of manipulation and locomotion domains in simulation, opening the door to generalist decision-making agents.},
  langid = {english}
}

@misc{chenSelfSupervisedReinforcementLearning2023,
  title = {Self-{{Supervised Reinforcement Learning}} That {{Transfers}} Using {{Random Features}}},
  author = {Chen, Boyuan and Zhu, Chuning and Agrawal, Pulkit and Zhang, Kaiqing and Gupta, Abhishek},
  year = {2023},
  month = may,
  number = {arXiv:2305.17250},
  eprint = {2305.17250},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.17250},
  urldate = {2025-06-04},
  abstract = {Model-free reinforcement learning algorithms have exhibited great potential in solving single-task sequential decision-making problems with high-dimensional observations and long horizons, but are known to be hard to generalize across tasks. Model-based RL, on the other hand, learns task-agnostic models of the world that naturally enables transfer across different reward functions, but struggles to scale to complex environments due to the compounding error. To get the best of both worlds, we propose a self-supervised reinforcement learning method that enables the transfer of behaviors across tasks with different rewards, while circumventing the challenges of model-based RL. In particular, we show self-supervised pre-training of model-free reinforcement learning with a number of random features as rewards allows implicit modeling of long-horizon environment dynamics. Then, planning techniques like model-predictive control using these implicit models enable fast adaptation to problems with new reward functions. Our method is self-supervised in that it can be trained on offline datasets without reward labels, but can then be quickly deployed on new tasks. We validate that our proposed method enables transfer across tasks on a variety of manipulation and locomotion domains in simulation, opening the door to generalist decision-making agents.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@misc{chenSymbolicDiscoveryOptimization2023,
  title = {Symbolic {{Discovery}} of {{Optimization Algorithms}}},
  author = {Chen, Xiangning and Liang, Chen and Huang, Da and Real, Esteban and Wang, Kaiyuan and Liu, Yao and Pham, Hieu and Dong, Xuanyi and Luong, Thang and Hsieh, Cho-Jui and Lu, Yifeng and Le, Quoc V.},
  year = {2023},
  month = feb,
  number = {arXiv:2302.06675},
  eprint = {2302.06675},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.06675},
  urldate = {2023-02-20},
  abstract = {We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target tasks, we also introduce program selection and simplification strategies. Our method discovers a simple and effective optimization algorithm, \${\textbackslash}textbf\{Lion\}\$ (\${\textbackslash}textit\{Evo\${\textbackslash}textbf\{L\}\$ved S\${\textbackslash}textbf\{i\}\$gn M\${\textbackslash}textbf\{o\}\$me\${\textbackslash}textbf\{n\}\$tum\}\$). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks. On image classification, Lion boosts the accuracy of ViT by up to 2\% on ImageNet and saves up to 5x the pre-training compute on JFT. On vision-language contrastive learning, we achieve 88.3\% \${\textbackslash}textit\{zero-shot\}\$ and 91.1\% \${\textbackslash}textit\{fine-tuning\}\$ accuracy on ImageNet, surpassing the previous best results by 2\% and 0.1\%, respectively. On diffusion models, Lion outperforms Adam by achieving a better FID score and reducing the training compute by up to 2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion exhibits a similar or better performance compared to Adam. Our analysis of Lion reveals that its performance gain grows with the training batch size. It also requires a smaller learning rate than Adam due to the larger norm of the update produced by the sign function. Additionally, we examine the limitations of Lion and identify scenarios where its improvements are small or not statistically significant. The implementation of Lion is publicly available.},
  archiveprefix = {arXiv}
}

@misc{chenUnlockingWhyBuying2024,
  title = {Unlocking the `{{Why}}' of {{Buying}}: {{Introducing}} a {{New Dataset}} and {{Benchmark}} for {{Purchase Reason}} and {{Post-Purchase Experience}}},
  shorttitle = {Unlocking the `{{Why}}' of {{Buying}}},
  author = {Chen, Tao and Zuo, Siqi and Li, Cheng and Zhang, Mingyang and Mei, Qiaozhu and Bendersky, Michael},
  year = {2024},
  month = jul,
  number = {arXiv:2402.13417},
  eprint = {2402.13417},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.13417},
  urldate = {2024-10-01},
  abstract = {Explanations are crucial for enhancing user trust and understanding within modern recommendation systems. To build truly explainable systems, we need high-quality datasets that elucidate why users make choices. While previous efforts have focused on extracting users' post-purchase sentiment in reviews, they ignore the reasons behind the decision to buy. In our work, we propose a novel purchase reason explanation task. To this end, we introduce an LLM-based approach to generate a dataset that consists of textual explanations of why real users make certain purchase decisions. We induce LLMs to explicitly distinguish between the reasons behind purchasing a product and the experience after the purchase in a user review. An automated, LLM-driven evaluation, as well as a small scale human evaluation, confirms the effectiveness of our approach to obtaining high-quality, personalized explanations. We benchmark this dataset on two personalized explanation generation tasks. We release the code and prompts to spur further research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval}
}

@misc{chiangCanLargeLanguage2023,
  title = {Can {{Large Language Models Be}} an {{Alternative}} to {{Human Evaluations}}?},
  author = {Chiang, Cheng-Han and Lee, Hung-yi},
  year = {2023},
  month = may,
  number = {arXiv:2305.01937},
  eprint = {2305.01937},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.01937},
  urldate = {2023-10-26},
  abstract = {Human evaluation is indispensable and inevitable for assessing the quality of texts generated by machine learning models or written by humans. However, human evaluation is very difficult to reproduce and its quality is notoriously unstable, hindering fair comparisons among different natural language processing (NLP) models and algorithms. Recently, large language models (LLMs) have demonstrated exceptional performance on unseen tasks when only the task instructions are provided. In this paper, we explore if such an ability of the LLMs can be used as an alternative to human evaluation. We present the LLMs with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask the LLMs to generate responses to those questions; we dub this LLM evaluation. We use human evaluation and LLM evaluation to evaluate the texts in two NLP tasks: open-ended story generation and adversarial attacks. We show that the result of LLM evaluation is consistent with the results obtained by expert human evaluation: the texts rated higher by human experts are also rated higher by the LLMs. We also find that the results of LLM evaluation are stable over different formatting of the task instructions and the sampling algorithm used to generate the answer. We are the first to show the potential of using LLMs to assess the quality of texts and discuss the limitations and ethical considerations of LLM evaluation.},
  archiveprefix = {arXiv}
}

@inproceedings{chiEnhancingCrossDeviceInteraction2016,
  title = {Enhancing {{Cross-Device Interaction Scripting}} with {{Interactive Illustrations}}},
  booktitle = {Proceedings of the 2016 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Chi, Pei-Yu (Peggy) and Li, Yang and Hartmann, Bj{\"o}rn},
  year = {2016},
  month = may,
  series = {{{CHI}} '16},
  pages = {5482--5493},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2858036.2858382},
  urldate = {2023-08-31},
  abstract = {Cross-device interactions involve input and output on multiple computing devices. Implementing and reasoning about interactions that cover multiple devices with a diversity of form factors and capabilities can be complex. To assist developers in programming cross-device interactions, we created DemoScript, a technique that automatically analyzes a cross-device interaction program while it is being written. DemoScript visually illustrates the step-by-step execution of a selected portion or the entire program with a novel, automatically generated cross-device storyboard visualization. In addition to helping developers understand the behavior of the program, DemoScript also allows developers to revise their program by interactively manipulating the cross-device storyboard. We evaluated DemoScript with 8 professional programmers and found that DemoScript significantly improved development efficiency by helping developers interpret and manage cross-device interaction; it also encourages testing to think through the script in a development process.},
  isbn = {978-1-4503-3362-7}
}

@inproceedings{chilanaUnderstandingUsabilityPractices2010,
  title = {Understanding Usability Practices in Complex Domains},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Chilana, Parmit K. and Wobbrock, Jacob O. and Ko, Amy J.},
  year = {2010},
  month = apr,
  series = {{{CHI}} '10},
  pages = {2337--2346},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1753326.1753678},
  urldate = {2024-09-10},
  abstract = {Although usability methods are widely used for evaluating conventional graphical user interfaces and websites, there is a growing concern that current approaches are inadequate for evaluating complex, domain-specific tools. We interviewed 21 experienced usability professionals, including in-house experts, external consultants, and managers working in a variety of complex domains, and uncovered the challenges commonly posed by domain complexity and how practitioners work around them. We found that despite the best efforts by usability professionals to get familiar with complex domains on their own, the lack of formal domain expertise can be a significant hurdle for carrying out effective usability evaluations. Partnerships with domain experts lead to effective results as long as domain experts are willing to be an integral part of the usability team. These findings suggest that for achieving usability in complex domains, some fundamental educational changes may be needed in the training of usability professionals.},
  isbn = {978-1-60558-929-9}
}

@article{choiTinkerTalesInteractive,
  title = {Tinker {{Tales}}: {{Interactive Storytelling Framework}} for {{Early Childhood Narrative Development}} and {{AI Literacy}}},
  author = {Choi, Nayoung and Cyebukayire, Peace and Choi, Jinho D},
  abstract = {This paper presents Tinker Tales, an interactive storytelling framework in the format of a board game, designed to support both narrative development and AI literacy in early childhood. The framework integrates tangible and speech-based interactions with AI through NFC chip-attached pawns and tokens, along with a speaker and microphone. Children select and define key story elements---such as characters, places, items, and emotions---using the pawns and tokens, providing further details to the AI and receiving proper assistance, similar to how adults prompt AI for specific tasks (e.g., writing). For evaluation, several game sessions were simulated with a child AI agent, and the quality and safety of the generated stories were assessed from various perspectives. This work highlights the potential of combining physical and digital elements in AI literacy, offering a safe and engaging way for children to learn how to effectively collaborate with AI.},
  langid = {english}
}

@misc{christakopoulouAgentsThinkingFast2024,
  title = {Agents {{Thinking Fast}} and {{Slow}}: {{A Talker-Reasoner Architecture}}},
  shorttitle = {Agents {{Thinking Fast}} and {{Slow}}},
  author = {Christakopoulou, Konstantina and Mourad, Shibl and Matari{\'c}, Maja},
  year = {2024},
  month = oct,
  number = {arXiv:2410.08328},
  eprint = {2410.08328},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.08328},
  urldate = {2025-04-05},
  abstract = {Large language models have enabled agents of all kinds to interact with users through natural conversation. Consequently, agents now have two jobs: conversing and planning/reasoning. Their conversational responses must be informed by all available information, and their actions must help to achieve goals. This dichotomy between conversing with the user and doing multi-step reasoning and planning can be seen as analogous to the human systems of "thinking fast and slow" as introduced by Kahneman. Our approach is comprised of a "Talker" agent (System 1) that is fast and intuitive, and tasked with synthesizing the conversational response; and a "Reasoner" agent (System 2) that is slower, more deliberative, and more logical, and is tasked with multi-step reasoning and planning, calling tools, performing actions in the world, and thereby producing the new agent state. We describe the new Talker-Reasoner architecture and discuss its advantages, including modularity and decreased latency. We ground the discussion in the context of a sleep coaching agent, in order to demonstrate real-world relevance.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{christianosLightweightNeuralApp2024,
  title = {Lightweight {{Neural App Control}}},
  author = {Christianos, Filippos and Papoudakis, Georgios and Coste, Thomas and Hao, Jianye and Wang, Jun and Shao, Kun},
  year = {2024},
  month = oct,
  number = {arXiv:2410.17883},
  eprint = {2410.17883},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2410.17883},
  urldate = {2024-10-25},
  abstract = {This paper introduces a novel mobile phone control architecture, termed ``app agents'', for efficient interactions and controls across various Android apps. The proposed Lightweight Multi-modal App Control (LiMAC) takes as input a textual goal and a sequence of past mobile observations, such as screenshots and corresponding UI trees, to generate precise actions. To address the computational constraints inherent to smartphones, within LiMAC, we introduce a small Action Transformer (AcT) integrated with a fine-tuned vision-language model (VLM) for real-time decision-making and task execution. We evaluate LiMAC on two open-source mobile control datasets, demonstrating the superior performance of our small-form-factor approach against fine-tuned versions of open-source VLMs, such as Florence2 and Qwen2-VL. It also significantly outperforms prompt engineering baselines utilising closed-source foundation models like GPT-4o. More specifically, LiMAC increases the overall action accuracy by up to 19\% compared to fine-tuned VLMs, and up to 42\% compared to prompt-engineering baselines.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence}
}

@inproceedings{chronopoulouEfficientHierarchicalDomain2022b,
  title = {Efficient {{Hierarchical Domain Adaptation}} for {{Pretrained Language Models}}},
  booktitle = {Proceedings of the 2022 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Chronopoulou, Alexandra and Peters, Matthew and Dodge, Jesse},
  editor = {Carpuat, Marine and {de Marneffe}, Marie-Catherine and Meza Ruiz, Ivan Vladimir},
  year = {2022},
  month = jul,
  pages = {1336--1351},
  publisher = {Association for Computational Linguistics},
  address = {Seattle, United States},
  doi = {10.18653/v1/2022.naacl-main.96},
  urldate = {2024-06-11},
  abstract = {The remarkable success of large language models has been driven by dense models trained on massive unlabeled, unstructured corpora. These corpora typically contain text from diverse, heterogeneous sources, but information about the source of the text is rarely used during training. Transferring their knowledge to a target domain is typically done by continuing training in-domain. In this paper, we introduce a method to permit domain adaptation to many diverse domains using a computationally efficient adapter approach. Our method is based on the observation that textual domains are partially overlapping, and we represent domains as a hierarchical tree structure where each node in the tree is associated with a set of adapter weights. When combined with a frozen pretrained language model, this approach enables parameter sharing among related domains, while avoiding negative interference between unrelated ones. Experimental results with GPT-2 and a large fraction of the 100 most represented websites in C4 show across-the-board improvements in-domain. We additionally provide an inference time algorithm for a held-out domain and show that averaging over multiple paths through the tree enables further gains in generalization, while adding only a marginal cost to inference.}
}

@misc{chungScalingInstructionFinetunedLanguage2022,
  title = {Scaling {{Instruction-Finetuned Language Models}}},
  author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and {Castro-Ros}, Alex and Pellat, Marie and Robinson, Kevin and Valter, Dasha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},
  year = {2022},
  month = dec,
  number = {arXiv:2210.11416},
  eprint = {2210.11416},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2210.11416},
  urldate = {2023-08-15},
  abstract = {Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4\% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2\% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.},
  archiveprefix = {arXiv}
}

@misc{CoDesignCoparentingCloseness,
  title = {Co-{{Design}} for Co-Parenting Closeness {{CHI}}'25 (for Writing Review)},
  url = {https://www.overleaf.com/project/6657b6d6eac4f0a98cc0c381},
  urldate = {2024-08-22},
  abstract = {An online LaTeX editor that's easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.},
  langid = {english}
}

@book{cohenStatisticalPowerAnalysis1988,
  title = {Statistical {{Power Analysis}} for the {{Behavioral Sciences}}},
  author = {Cohen, Jacob},
  year = {1988},
  month = jul,
  edition = {2},
  publisher = {Routledge},
  address = {New York},
  doi = {10.4324/9780203771587},
  abstract = {Statistical Power Analysis is a nontechnical guide to power analysis in research planning that provides users of applied statistics with the tools they need for more effective analysis. The Second Edition includes:  * a chapter covering power analysis in set correlation and multivariate methods; * a chapter considering effect size, psychometric reliability, and the efficacy of "qualifying" dependent variables and; * expanded power and sample size tables for multiple regression/correlation.},
  isbn = {978-0-203-77158-7}
}

@article{ContextualizingRecommendationExplanations,
  title = {Contextualizing {{Recommendation Explanations}} with {{LLMs}}: {{A User Study}}},
  langid = {english}
}

@article{corchadoFacultadInformaticaUniversidad,
  title = {Facultad de {{Informatica}}, {{Universidad Complutense Madrid}}, {{Ciudad Universitaria}} s/n, 28040 {{Madrid}}, {{Spain E-mail}}: J.Pavon@sip.Ucm.Es},
  author = {Corchado, Juan M},
  langid = {english},
  keywords = {No DOI found}
}

@article{cosmaApproachSourcecodePlagiarism2011,
  title = {An Approach to Source-Code Plagiarism Detection and Investigation Using Latent Semantic Analysis},
  author = {Cosma, Georgina and Joy, Mike},
  year = {2011},
  journal = {IEEE transactions on computers},
  volume = {61},
  number = {3},
  pages = {379--394},
  publisher = {IEEE},
  doi = {10.1109/TC.2011.223}
}

@article{cosmaDefinitionSourcecodePlagiarism2008,
  title = {Towards a Definition of Source-Code Plagiarism},
  author = {Cosma, Georgina and Joy, Mike},
  year = {2008},
  journal = {IEEE Transactions on Education},
  volume = {51},
  number = {2},
  pages = {195--200},
  publisher = {IEEE},
  doi = {10.1109/TE.2007.906776}
}

@article{CPLLMClinicalPrediction2023,
  title = {{{CPLLM}}: {{Clinical Prediction}} with {{Large Language Models}}},
  shorttitle = {{{CPLLM}}},
  year = {2023},
  month = oct,
  url = {https://openreview.net/forum?id=fnBYPL5Ged},
  urldate = {2024-01-10},
  abstract = {We present Clinical Prediction with Large Language Models (CPLLM), a method that involves fine-tuning a pre-trained Large Language Model (LLM) for clinical disease prediction. We utilized quantization and fine-tuned the LLM using prompts, with the task of predicting whether patients will be diagnosed with a target disease during their next visit or in the subsequent diagnosis, leveraging their historical diagnosis records. We compared our results versus various baselines, including Logistic Regression, RETAIN, and Med-BERT, which is the current state-of-the-art model for disease prediction using structured EHR data. Our experiments have shown that CPLLM surpasses all the tested models in terms of both PR-AUC and ROC-AUC metrics, displaying noteworthy enhancements compared to the baseline models},
  langid = {english}
}

@misc{CSCW24LLM,
  title = {{{CSCW}}'24 {{LLM For Cancer Post-Op Care}} - {{Working}}},
  url = {https://www.overleaf.com/project/65236bd4ae117c85e45ed3f2/detached},
  urldate = {2024-01-14},
  abstract = {An online LaTeX editor that's easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.},
  langid = {english}
}

@inproceedings{cuiAttentionoverAttentionNeuralNetworks2017,
  title = {Attention-over-{{Attention Neural Networks}} for {{Reading Comprehension}}},
  booktitle = {Proceedings of the 55th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Cui, Yiming and Chen, Zhipeng and Wei, Si and Wang, Shijin and Liu, Ting and Hu, Guoping},
  year = {2017},
  pages = {593--602},
  publisher = {Association for Computational Linguistics},
  address = {Vancouver, Canada},
  doi = {10.18653/v1/P17-1055},
  urldate = {2022-01-17},
  abstract = {Cloze-style reading comprehension is a representative problem in mining relationship between document and query. In this paper, we present a simple but novel model called attention-over-attention reader for better solving cloze-style reading comprehension task. The proposed model aims to place another attention mechanism over the document-level attention and induces ``attended attention'' for final answer predictions. One advantage of our model is that it is simpler than related works while giving excellent performance. In addition to the primary model, we also propose an N-best re-ranking strategy to double check the validity of the candidates and further improve the performance. Experimental results show that the proposed methods significantly outperform various state-of-the-art systems by a large margin in public datasets, such as CNN and Children's Book Test.},
  annotation = {00396}
}

@inproceedings{cuiConsensusAttentionbasedNeural2016,
  title = {Consensus {{Attention-based Neural Networks}} for {{Chinese Reading Comprehension}}},
  booktitle = {Proceedings of {{COLING}} 2016, the 26th {{International Conference}} on {{Computational Linguistics}}: {{Technical Papers}}},
  author = {Cui, Yiming and Liu, Ting and Chen, Zhipeng and Wang, Shijin and Hu, Guoping},
  year = {2016},
  pages = {1777--1786},
  publisher = {The COLING 2016 Organizing Committee},
  address = {Osaka, Japan},
  url = {https://aclanthology.org/C16-1167},
  urldate = {2022-01-15},
  abstract = {Reading comprehension has embraced a booming in recent NLP research. Several institutes have released the Cloze-style reading comprehension data, and these have greatly accelerated the research of machine comprehension. In this work, we firstly present Chinese reading comprehension datasets, which consist of People Daily news dataset and Children's Fairy Tale (CFT) dataset. Also, we propose a consensus attention-based neural network architecture to tackle the Cloze-style reading comprehension problem, which aims to induce a consensus attention over every words in the query. Experimental results show that the proposed neural network significantly outperforms the state-of-the-art baselines in several public datasets. Furthermore, we setup a baseline for Chinese reading comprehension task, and hopefully this would speed up the process for future research.},
  annotation = {00085}
}

@article{curiglianoCardiotoxicityAnticancerTreatments2016,
  title = {Cardiotoxicity of Anticancer Treatments: {{Epidemiology}}, Detection, and Management},
  shorttitle = {Cardiotoxicity of Anticancer Treatments},
  author = {Curigliano, Giuseppe and Cardinale, Daniela and Dent, Susan and Criscitiello, Carmen and Aseyev, Olexiy and Lenihan, Daniel and Cipolla, Carlo Maria},
  year = {2016},
  month = jul,
  journal = {CA: A Cancer Journal for Clinicians},
  volume = {66},
  number = {4},
  pages = {309--325},
  issn = {0007-9235, 1542-4863},
  doi = {10.3322/caac.21341},
  urldate = {2024-04-23},
  abstract = {Answer questions and earn CME/CNE                                         Cancer and heart disease are the leading causes of morbidity and mortality in the industrialized world. Modern treatment strategies have led to an improvement in the chances of surviving a diagnosis of cancer; however, these gains can come at a cost. Patients may experience adverse cardiovascular events related to their cancer treatment or as a result of an exacerbation of underlying cardiovascular disease. With longer periods of survival, late effects of cancer treatment may become clinically evident years or decades after completion of therapy. Current cancer therapy incorporates multiple agents whose deleterious cardiac effects may be additive or synergistic. Cardiac dysfunction may result from agents that can result in myocyte destruction, such as with anthracycline use, or from agents that appear to transiently affect left ventricular contractility. In addition, cancer treatment may be associated with other cardiac events, such as severe treatment-induced hypertension and vasospastic and thromboembolic ischemia, as well as rhythm disturbances, including QTc prolongation, that may be rarely life-threatening. Early and late effects of chest radiation can lead to radiation-induced heart disease, including pericardial disease, myocardial fibrosis, cardiomyopathy, coronary artery disease, valvular disease, and arrhythmias, in the setting of myocardial fibrosis. The discipline of cardio-oncology has developed in response to the combined decision making necessary to optimize the care of cancer patients, whether they are receiving active treatment or are long-term survivors. Strategies to prevent or mitigate cardiovascular damage from cancer treatment are needed to provide the best cancer care. This review will focus on the common cardiovascular issues that may arise during or after cancer therapy, the detection and monitoring of cardiovascular injury, and the best management principles to protect against or minimize cardiotoxicity during the spectrum of cancer treatment strategies.               CA Cancer J Clin 2016;66:309-325. {\copyright} 2016 American Cancer Society               .},
  copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
  langid = {american}
}

@inproceedings{daiUncoveringChatGPTsCapabilities2023,
  title = {Uncovering {{ChatGPT}}'s {{Capabilities}} in {{Recommender Systems}}},
  booktitle = {Proceedings of the 17th {{ACM Conference}} on {{Recommender Systems}}},
  author = {Dai, Sunhao and Shao, Ninglu and Zhao, Haiyuan and Yu, Weijie and Si, Zihua and Xu, Chen and Sun, Zhongxiang and Zhang, Xiao and Xu, Jun},
  year = {2023},
  month = sep,
  series = {{{RecSys}} '23},
  pages = {1126--1132},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3604915.3610646},
  urldate = {2024-09-24},
  abstract = {The debut of ChatGPT has recently attracted significant attention from the natural language processing (NLP) community and beyond. Existing studies have demonstrated that ChatGPT shows significant improvement in a range of downstream NLP tasks, but the capabilities and limitations of ChatGPT in terms of recommendations remain unclear. In this study, we aim to enhance ChatGPT's recommendation capabilities by aligning it with traditional information retrieval (IR) ranking capabilities, including point-wise, pair-wise, and list-wise ranking. To achieve this goal, we re-formulate the aforementioned three recommendation policies into prompt formats tailored specifically to the domain at hand. Through extensive experiments on four datasets from different domains, we analyze the distinctions among the three recommendation policies. Our findings indicate that ChatGPT achieves an optimal balance between cost and performance when equipped with list-wise ranking. This research sheds light on a promising direction for aligning ChatGPT with recommendation tasks. To facilitate further explorations in this area, the full code and detailed original results are open-sourced at https://github.com/rainym00d/LLM4RS.},
  isbn = {979-8-4007-0241-9}
}

@misc{damaniLearningHowHard2024,
  title = {Learning {{How Hard}} to {{Think}}: {{Input-Adaptive Allocation}} of {{LM Computation}}},
  shorttitle = {Learning {{How Hard}} to {{Think}}},
  author = {Damani, Mehul and Shenfeld, Idan and Peng, Andi and Bobu, Andreea and Andreas, Jacob},
  year = {2024},
  month = oct,
  number = {arXiv:2410.04707},
  eprint = {2410.04707},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2410.04707},
  urldate = {2024-10-21},
  abstract = {Computationally intensive decoding procedures---including search, reranking, and self-critique---can improve the quality of language model (LM) outputs in problems spanning code generation, numerical reasoning, and dialog. Existing work typically applies the same decoding procedure for every input to an LM. But not all inputs require the same amount of computation to process. Can we allocate decoding computation adaptively, using more resources to answer questions whose answers will be harder to compute? We present an approach that predicts the distribution of rewards given an input and computation budget, then allocates additional computation to inputs for which it is predicted to be most useful. We apply this approach in two decoding procedures: first, an adaptive best-of-k procedure that dynamically selects the number of samples to generate as input to a reranker; second, a routing procedure that dynamically responds to a query using a decoding procedure that is expensive but accurate, or one that is cheaper but less capable. Across a suite of programming, mathematics, and dialog tasks, we show that accurate computation-allocation procedures can be learned, and reduce computation by up to 50\% at no cost to response quality, or improve quality by up to 10\% at a fixed computational budget.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{daoFlashAttentionFastMemoryEfficient2022,
  title = {{{FlashAttention}}: {{Fast}} and {{Memory-Efficient Exact Attention}} with {{IO-Awareness}}},
  shorttitle = {{{FlashAttention}}},
  author = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  year = {2022},
  month = jun,
  number = {arXiv:2205.14135},
  eprint = {2205.14135},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.14135},
  urldate = {2023-03-13},
  abstract = {Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15\% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3\${\textbackslash}times\$ speedup on GPT-2 (seq. length 1K), and 2.4\${\textbackslash}times\$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4\% accuracy) and Path-256 (seq. length 64K, 63.1\% accuracy).},
  archiveprefix = {arXiv}
}

@misc{dasDecoderonlyFoundationModel2024,
  title = {A Decoder-Only Foundation Model for Time-Series Forecasting},
  author = {Das, Abhimanyu and Kong, Weihao and Sen, Rajat and Zhou, Yichen},
  year = {2024},
  month = feb,
  number = {arXiv:2310.10688},
  eprint = {2310.10688},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2310.10688},
  urldate = {2024-02-14},
  abstract = {Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. Our model is based on pretraining a patched-decoder style attention model on a large time-series corpus, and can work well across different forecasting history lengths, prediction lengths and temporal granularities.},
  archiveprefix = {arXiv}
}

@article{dasigiDatasetInformationSeekingQuestions2021,
  title = {A {{Dataset}} of {{Information-Seeking Questions}} and {{Answers Anchored}} in {{Research Papers}}},
  author = {Dasigi, Pradeep and Lo, Kyle and Beltagy, Iz and Cohan, Arman and Smith, Noah A. and Gardner, Matt},
  year = {2021},
  journal = {arXiv preprint arXiv:2105.03011},
  eprint = {2105.03011},
  archiveprefix = {arXiv}
}

@article{decker-maurerMethodMadnessUsability2012,
  title = {Method Madness: A Usability Testing Pilot Research Case Study},
  shorttitle = {Method Madness},
  author = {{Decker-Maurer}, Heidi},
  year = {2012},
  month = mar,
  journal = {Commun. Des. Q. Rev},
  volume = {13},
  number = {1},
  pages = {14--18},
  issn = {2166-1200},
  doi = {10.1145/2424837.2424839},
  urldate = {2024-09-11},
  abstract = {This case study was created to analyze the methodology and procedures used during a pilot study on mobile usability and preferences conducted at a small Midwestern state college. The pilot study set forth to test features of the pre-redesign University of Wisconsin-Stout website as seen through the screen of a mobile device and then ascertain what students wanted to see in a redesigned version of the mobile interface.The findings of the pilot study were less surprising to the researcher than the problems encountered during the research itself. Future researchers would be well advised to attend to passing trends in mobile technology, as well as avoiding limitations on sample size caused by choice of delivery method and choice of user pool.}
}

@misc{deepseek-aiDeepSeekR1IncentivizingReasoning2025,
  title = {{{DeepSeek-R1}}: {{Incentivizing Reasoning Capability}} in {{LLMs}} via {{Reinforcement Learning}}},
  shorttitle = {{{DeepSeek-R1}}},
  author = {{DeepSeek-AI} and Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and Zhang, Xiaokang and Yu, Xingkai and Wu, Yu and Wu, Z. F. and Gou, Zhibin and Shao, Zhihong and Li, Zhuoshu and Gao, Ziyi and Liu, Aixin and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Feng, Bei and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and Dai, Damai and Chen, Deli and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Dai, Fucong and Luo, Fuli and Hao, Guangbo and Chen, Guanting and Li, Guowei and Zhang, H. and Bao, Han and Xu, Hanwei and Wang, Haocheng and Ding, Honghui and Xin, Huajian and Gao, Huazuo and Qu, Hui and Li, Hui and Guo, Jianzhong and Li, Jiashi and Wang, Jiawei and Chen, Jingchang and Yuan, Jingyang and Qiu, Junjie and Li, Junlong and Cai, J. L. and Ni, Jiaqi and Liang, Jian and Chen, Jin and Dong, Kai and Hu, Kai and Gao, Kaige and Guan, Kang and Huang, Kexin and Yu, Kuai and Wang, Lean and Zhang, Lecong and Zhao, Liang and Wang, Litong and Zhang, Liyue and Xu, Lei and Xia, Leyi and Zhang, Mingchuan and Zhang, Minghua and Tang, Minghui and Li, Meng and Wang, Miaojun and Li, Mingming and Tian, Ning and Huang, Panpan and Zhang, Peng and Wang, Qiancheng and Chen, Qinyu and Du, Qiushi and Ge, Ruiqi and Zhang, Ruisong and Pan, Ruizhe and Wang, Runji and Chen, R. J. and Jin, R. L. and Chen, Ruyi and Lu, Shanghao and Zhou, Shangyan and Chen, Shanhuang and Ye, Shengfeng and Wang, Shiyu and Yu, Shuiping and Zhou, Shunfeng and Pan, Shuting and Li, S. S. and Zhou, Shuang and Wu, Shaoqing and Ye, Shengfeng and Yun, Tao and Pei, Tian and Sun, Tianyu and Wang, T. and Zeng, Wangding and Zhao, Wanjia and Liu, Wen and Liang, Wenfeng and Gao, Wenjun and Yu, Wenqin and Zhang, Wentao and Xiao, W. L. and An, Wei and Liu, Xiaodong and Wang, Xiaohan and Chen, Xiaokang and Nie, Xiaotao and Cheng, Xin and Liu, Xin and Xie, Xin and Liu, Xingchao and Yang, Xinyu and Li, Xinyuan and Su, Xuecheng and Lin, Xuheng and Li, X. Q. and Jin, Xiangyue and Shen, Xiaojin and Chen, Xiaosha and Sun, Xiaowen and Wang, Xiaoxiang and Song, Xinnan and Zhou, Xinyi and Wang, Xianzu and Shan, Xinxia and Li, Y. K. and Wang, Y. Q. and Wei, Y. X. and Zhang, Yang and Xu, Yanhong and Li, Yao and Zhao, Yao and Sun, Yaofeng and Wang, Yaohui and Yu, Yi and Zhang, Yichao and Shi, Yifan and Xiong, Yiliang and He, Ying and Piao, Yishi and Wang, Yisong and Tan, Yixuan and Ma, Yiyang and Liu, Yiyuan and Guo, Yongqiang and Ou, Yuan and Wang, Yuduan and Gong, Yue and Zou, Yuheng and He, Yujia and Xiong, Yunfan and Luo, Yuxiang and You, Yuxiang and Liu, Yuxuan and Zhou, Yuyang and Zhu, Y. X. and Xu, Yanhong and Huang, Yanping and Li, Yaohui and Zheng, Yi and Zhu, Yuchen and Ma, Yunxian and Tang, Ying and Zha, Yukun and Yan, Yuting and Ren, Z. Z. and Ren, Zehui and Sha, Zhangli and Fu, Zhe and Xu, Zhean and Xie, Zhenda and Zhang, Zhengyan and Hao, Zhewen and Ma, Zhicheng and Yan, Zhigang and Wu, Zhiyu and Gu, Zihui and Zhu, Zijia and Liu, Zijun and Li, Zilin and Xie, Ziwei and Song, Ziyang and Pan, Zizheng and Huang, Zhen and Xu, Zhipeng and Zhang, Zhongyu and Zhang, Zhen},
  year = {2025},
  month = jan,
  number = {arXiv:2501.12948},
  eprint = {2501.12948},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.12948},
  urldate = {2025-03-14},
  abstract = {We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@article{deguchiMBRSLibraryMinimum,
  title = {{{MBRS}}: {{A Library}} for {{Minimum Bayes Risk Decoding}}},
  author = {Deguchi, Hiroyuki and Sakai, Yusuke and Kamigaito, Hidetaka and Watanabe, Taro},
  langid = {english},
  keywords = {No DOI found}
}

@inproceedings{delacruzIntegratingRealTimeCrowd2015,
  title = {Towards {{Integrating Real-Time Crowd Advice}} with {{Reinforcement Learning}}},
  booktitle = {Companion {{Proceedings}} of the 20th {{International Conference}} on {{Intelligent User Interfaces}}},
  author = {De La Cruz, Gabriel V. and Peng, Bei and Lasecki, Walter S. and Taylor, Matthew E.},
  year = {2015},
  month = mar,
  pages = {17--20},
  publisher = {ACM},
  address = {Atlanta Georgia USA},
  doi = {10.1145/2732158.2732180},
  urldate = {2025-06-04},
  isbn = {978-1-4503-3308-5},
  langid = {english}
}

@misc{deletangLanguageModelingCompression2024,
  title = {Language {{Modeling Is Compression}}},
  author = {Del{\'e}tang, Gr{\'e}goire and Ruoss, Anian and Duquenne, Paul-Ambroise and Catt, Elliot and Genewein, Tim and Mattern, Christopher and {Grau-Moya}, Jordi and Wenliang, Li Kevin and Aitchison, Matthew and Orseau, Laurent and Hutter, Marcus and Veness, Joel},
  year = {2024},
  month = mar,
  number = {arXiv:2309.10668},
  eprint = {2309.10668},
  primaryclass = {cs, math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.10668},
  urldate = {2024-07-05},
  abstract = {It has long been established that predictive models can be transformed into lossless compressors and vice versa. Incidentally, in recent years, the machine learning community has focused on training increasingly large and powerful self-supervised (language) models. Since these large language models exhibit impressive predictive capabilities, they are well-positioned to be strong compressors. In this work, we advocate for viewing the prediction problem through the lens of compression and evaluate the compression capabilities of large (foundation) models. We show that large language models are powerful general-purpose predictors and that the compression viewpoint provides novel insights into scaling laws, tokenization, and in-context learning. For example, Chinchilla 70B, while trained primarily on text, compresses ImageNet patches to 43.4\% and LibriSpeech samples to 16.4\% of their raw size, beating domain-specific compressors like PNG (58.5\%) or FLAC (30.3\%), respectively. Finally, we show that the prediction-compression equivalence allows us to use any compressor (like gzip) to build a conditional generative model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Theory,Computer Science - Machine Learning}
}

@misc{deluciaUsingNaturalLanguage2024,
  title = {Using {{Natural Language Inference}} to {{Improve Persona Extraction}} from {{Dialogue}} in a {{New Domain}}},
  author = {DeLucia, Alexandra and Zhao, Mengjie and Maeda, Yoshinori and Yoda, Makoto and Yamada, Keiichi and Wakaki, Hiromi},
  year = {2024},
  month = jan,
  number = {arXiv:2401.06742},
  eprint = {2401.06742},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2401.06742},
  urldate = {2024-01-15},
  abstract = {While valuable datasets such as PersonaChat provide a foundation for training persona-grounded dialogue agents, they lack diversity in conversational and narrative settings, primarily existing in the "real" world. To develop dialogue agents with unique personas, models are trained to converse given a specific persona, but hand-crafting these persona can be time-consuming, thus methods exist to automatically extract persona information from existing character-specific dialogue. However, these persona-extraction models are also trained on datasets derived from PersonaChat and struggle to provide high-quality persona information from conversational settings that do not take place in the real world, such as the fantasy-focused dataset, LIGHT. Creating new data to train models on a specific setting is human-intensive, thus prohibitively expensive. To address both these issues, we introduce a natural language inference method for post-hoc adapting a trained persona extraction model to a new setting. We draw inspiration from the literature of dialog natural language inference (NLI), and devise NLI-reranking methods to extract structured persona information from dialogue. Compared to existing persona extraction models, our method returns higher-quality extracted persona and requires less human annotation.},
  archiveprefix = {arXiv}
}

@inproceedings{dengMind2WebGeneralistAgent2023,
  title = {{{Mind2Web}}: {{Towards}} a {{Generalist Agent}} for the {{Web}}},
  shorttitle = {{{Mind2Web}}},
  booktitle = {Thirty-Seventh {{Conference}} on {{Neural Information Processing Systems Datasets}} and {{Benchmarks Track}}},
  author = {Deng, Xiang and Gu, Yu and Zheng, Boyuan and Chen, Shijie and Stevens, Samuel and Wang, Boshi and Sun, Huan and Su, Yu},
  year = {2023},
  month = nov,
  url = {https://openreview.net/forum?id=kiYqbO3wqw},
  urldate = {2024-07-15},
  abstract = {We introduce Mind2Web, the first dataset for developing and evaluating generalist agents for the web that can follow language instructions to complete complex tasks on any website. Existing datasets for web agents either use simulated websites or only cover a limited set of websites and tasks, thus not suitable for generalist web agents. With over 2,000 open-ended tasks collected from 137 websites spanning 31 domains and crowdsourced action sequences for the tasks, Mind2Web provides three necessary ingredients for building generalist web agents: 1) diverse domains, websites, and tasks, 2) use of real-world websites instead of simulated and simplified ones, and 3) a broad spectrum of user interaction patterns. Based on Mind2Web, we conduct an initial exploration of using large language models (LLMs) for building generalist web agents. While the raw HTML of real-world websites are often too large to be fed to LLMs, we show that first filtering it with a small LM significantly improves the effectiveness and efficiency of LLMs. Our solution demonstrates a decent level of performance, even on websites or entire domains the model has never seen before, but there is still a substantial room to improve towards truly generalizable agents. We open-source our dataset, model implementation, and trained models (https://osu-nlp-group.github.io/Mind2Web) to facilitate further research on building a generalist agent for the web.},
  langid = {english}
}

@inproceedings{dengMultiturnInstructionFollowing2024,
  title = {On the {{Multi-turn Instruction Following}} for {{Conversational Web Agents}}},
  booktitle = {Proceedings of the 62nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Deng, Yang and Zhang, Xuan and Zhang, Wenxuan and Yuan, Yifei and Ng, See-Kiong and Chua, Tat-Seng},
  editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
  year = {2024},
  month = aug,
  pages = {8795--8812},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.477},
  urldate = {2024-12-11},
  abstract = {Web agents powered by Large Language Models (LLMs) have demonstrated remarkable abilities in planning and executing multi-step interactions within complex web-based environments, fulfilling a wide range of web navigation tasks. Despite these advancements, the potential for LLM-powered agents to effectively engage with sequential user instructions in real-world scenarios has not been fully explored. In this work, we introduce a new task of Conversational Web Navigation, which necessitates sophisticated interactions that span multiple turns with both the users and the environment, supported by a specially developed dataset named Multi-Turn Mind2Web (MT-Mind2Web). To tackle the limited context length of LLMs and the context-dependency issue of the conversational tasks, we further propose a novel framework, named self-reflective memory-augmented planning (Self-MAP), which employs memory utilization and self-reflection techniques. Extensive experiments are conducted to benchmark the MT-Mind2Web dataset, and validate the effectiveness of the proposed method.}
}

@misc{dettmersLLMInt88bit2022,
  title = {{{LLM}}.Int8(): 8-Bit {{Matrix Multiplication}} for {{Transformers}} at {{Scale}}},
  shorttitle = {{{LLM}}.Int8()},
  author = {Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  year = {2022},
  month = nov,
  number = {arXiv:2208.07339},
  eprint = {2208.07339},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2208.07339},
  urldate = {2023-04-03},
  abstract = {Large language models have been widely adopted but require significant GPU memory for inference. We develop a procedure for Int8 matrix multiplication for feed-forward and attention projection layers in transformers, which cut the memory needed for inference by half while retaining full precision performance. With our method, a 175B parameter 16/32-bit checkpoint can be loaded, converted to Int8, and used immediately without performance degradation. This is made possible by understanding and working around properties of highly systematic emergent features in transformer language models that dominate attention and transformer predictive performance. To cope with these features, we develop a two-part quantization procedure, LLM.int8(). We first use vector-wise quantization with separate normalization constants for each inner product in the matrix multiplication, to quantize most of the features. However, for the emergent outliers, we also include a new mixed-precision decomposition scheme, which isolates the outlier feature dimensions into a 16-bit matrix multiplication while still more than 99.9\% of values are multiplied in 8-bit. Using LLM.int8(), we show empirically it is possible to perform inference in LLMs with up to 175B parameters without any performance degradation. This result makes such models much more accessible, for example making it possible to use OPT-175B/BLOOM on a single server with consumer GPUs. We open-source our software.},
  archiveprefix = {arXiv}
}

@article{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  journal = {Proceedings of the 2019 Conference of the North},
  pages = {4171--4186},
  publisher = {Association for Computational Linguistics},
  address = {Minneapolis, Minnesota},
  doi = {10.18653/v1/N19-1423},
  urldate = {2023-06-23},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  langid = {english}
}

@inproceedings{dingHPCGPTIntegratingLarge2023,
  title = {{{HPC-GPT}}: {{Integrating Large Language Model}} for {{High-Performance Computing}}},
  shorttitle = {{{HPC-GPT}}},
  booktitle = {Proceedings of the {{SC}} '23 {{Workshops}} of {{The International Conference}} on {{High Performance Computing}}, {{Network}}, {{Storage}}, and {{Analysis}}},
  author = {Ding, Xianzhong and Chen, Le and Emani, Murali and Liao, Chunhua and Lin, Pei-Hung and Vanderbruggen, Tristan and Xie, Zhen and Cerpa, Alberto E. and Du, Wan},
  year = {2023},
  month = nov,
  eprint = {2311.12833},
  primaryclass = {cs},
  pages = {951--960},
  doi = {10.1145/3624062.3624172},
  urldate = {2024-01-20},
  abstract = {Large Language Models (LLMs), including the LLaMA model, have exhibited their efficacy across various general-domain natural language processing (NLP) tasks. However, their performance in high-performance computing (HPC) domain tasks has been less than optimal due to the specialized expertise required to interpret the model responses. In response to this challenge, we propose HPC-GPT, a novel LLaMA-based model that has been supervised fine-tuning using generated QA (Question-Answer) instances for the HPC domain. To evaluate its effectiveness, we concentrate on two HPC tasks: managing AI models and datasets for HPC, and data race detection. By employing HPC-GPT, we demonstrate comparable performance with existing methods on both tasks, exemplifying its excellence in HPC-related scenarios. Our experiments on open-source benchmarks yield extensive results, underscoring HPC-GPT's potential to bridge the performance gap between LLMs and HPC-specific tasks. With HPC-GPT, we aim to pave the way for LLMs to excel in HPC domains, simplifying the utilization of language models in complex computing applications.},
  archiveprefix = {arXiv}
}

@article{dingOpenPromptOpensourceFramework2021,
  title = {{{OpenPrompt}}: {{An Open-source Framework}} for {{Prompt-learning}}},
  shorttitle = {{{OpenPrompt}}},
  author = {Ding, Ning and Hu, Shengding and Zhao, Weilin and Chen, Yulin and Liu, Zhiyuan and Zheng, Hai-Tao and Sun, Maosong},
  year = {2021},
  month = nov,
  journal = {arXiv:2111.01998 [cs]},
  eprint = {2111.01998},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2111.01998},
  urldate = {2022-04-15},
  abstract = {Prompt-learning has become a new paradigm in modern natural language processing, which directly adapts pre-trained language models (PLMs) to \$cloze\$-style prediction, autoregressive modeling, or sequence to sequence generation, resulting in promising performances on various tasks. However, no standard implementation framework of prompt-learning is proposed yet, and most existing prompt-learning codebases, often unregulated, only provide limited implementations for specific scenarios. Since there are many details such as templating strategy, initializing strategy, and verbalizing strategy, etc. need to be considered in prompt-learning, practitioners face impediments to quickly adapting the desired prompt learning methods to their applications. In this paper, we present \{OpenPrompt\}, a unified easy-to-use toolkit to conduct prompt-learning over PLMs. OpenPrompt is a research-friendly framework that is equipped with efficiency, modularity, and extendibility, and its combinability allows the freedom to combine different PLMs, task formats, and prompting modules in a unified paradigm. Users could expediently deploy prompt-learning frameworks and evaluate the generalization of them on different NLP tasks without constraints. OpenPrompt is publicly released at \{{\textbackslash}url\{ https://github.com/thunlp/OpenPrompt\}\}.},
  archiveprefix = {arXiv}
}

@article{dinhComprehensiveAnalysisCognitive2024,
  title = {A {{Comprehensive Analysis}} of {{Cognitive CAPTCHAs Through Eye Tracking}}},
  author = {Dinh, Nghia and Ogiela, Lidia Dominika and {Tran-Trung}, Kiet and {Le-Viet}, Tuan and Hoang, Vinh Truong},
  year = {2024},
  journal = {IEEE Access},
  volume = {12},
  pages = {47190--47209},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2024.3373542},
  urldate = {2024-09-11},
  abstract = {CAPTCHA (Completely Automated Public Turing Test to Tell Computers and Humans Apart) has long been employed to combat automated bots. It accomplishes this by utilizing distortion techniques and cognitive characteristics. When it comes to countering security attacks, cognitive CAPTCHA methods have proven to be more effective than other approaches. The advancement of eye-tracking technology has greatly improved human-computer interaction (HCI), enabling users to engage with computers without physical contact. This technology is widely used for studying attention, cognitive processes, and performance. In this specific research, we conducted eye-tracking experiments on participants to investigate how their visual behavior changes as the complexity of cognitive CAPTCHAs varies. By analyzing the distribution of eye gaze on each level of CAPTCHA, we can assess users' visual behavior based on eye movement performance and process metrics. The data collected is then employed in Machine Learning (ML) algorithms to categorize and examine the relative importance of these factors in predicting performance. This study highlights the potential to enhance any cognitive CAPTCHA model by gaining insights into the underlying cognitive processes.},
  keywords = {CAPTCHA,CAPTCHAs,Cognitive,Cognitive processes,eye tracking,Gaze tracking,machine learning,Machine learning,Measurement,Performance evaluation,security,Task analysis,Visualization}
}

@misc{dixitRETAINInteractiveTool2024,
  title = {{{RETAIN}}: {{Interactive Tool}} for {{Regression Testing Guided LLM Migration}}},
  shorttitle = {{{RETAIN}}},
  author = {Dixit, Tanay and Lee, Daniel and Fang, Sally and Harsha, Sai Sree and Sureshan, Anirudh and Maharaj, Akash and Li, Yunyao},
  year = {2024},
  month = sep,
  number = {arXiv:2409.03928},
  eprint = {2409.03928},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.03928},
  urldate = {2024-09-17},
  abstract = {Large Language Models (LLMs) are increasingly integrated into diverse applications. The rapid evolution of LLMs presents opportunities for developers to enhance applications continuously. However, this constant adaptation can also lead to performance regressions during model migrations. While several interactive tools have been proposed to streamline the complexity of prompt engineering, few address the specific requirements of regression testing for LLM Migrations. To bridge this gap, we introduce RETAIN (REgression Testing guided LLM migrAtIoN), a tool designed explicitly for regression testing in LLM Migrations. RETAIN comprises two key components: an interactive interface tailored to regression testing needs during LLM migrations, and an error discovery module that facilitates understanding of differences in model behaviors. The error discovery module generates textual descriptions of various errors or differences between model outputs, providing actionable insights for prompt refinement. Our automatic evaluation and empirical user studies demonstrate that RETAIN, when compared to manual evaluation, enabled participants to identify twice as many errors, facilitated experimentation with 75\% more prompts, and achieves 12\% higher metric scores in a given time frame.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval}
}

@article{dongEfficientlyMitigatingImpact2024,
  title = {Efficiently {{Mitigating}} the {{Impact}} of {{Data Drift}} on {{Machine Learning Pipelines}}},
  author = {Dong, Sijie and Wang, Qitong and Sahri, Soror and Palpanas, Themis and Srivastava, Divesh},
  year = {2024},
  month = jul,
  journal = {Proc. VLDB Endow.},
  volume = {17},
  number = {11},
  pages = {3072--3081},
  issn = {2150-8097},
  doi = {10.14778/3681954.3681984},
  urldate = {2025-02-18},
  abstract = {Despite the increasing success of Machine Learning (ML) techniques in real-world applications, their maintenance over time remains challenging. In particular, the prediction accuracy of deployed ML models can suffer due to significant changes between training and serving data over time, known as data drift. Traditional data drift solutions primarily focus on detecting drift, and then retraining the ML models, but do not discern whether the detected drift is harmful to model performance. In this paper, we observe that not all data drifts lead to degradation in prediction accuracy. We then introduce a novel approach for identifying portions of data distributions in serving data where drift can be potentially harmful to model performance, which we term Data Distributions with Low Accuracy (DDLA). Our approach, using decision trees, precisely pinpoints low-accuracy zones within ML models, especially Blackbox models. By focusing on these DDLAs, we effectively assess the impact of data drift on model performance and make informed decisions in the ML pipeline. In contrast to existing data drift techniques, we advocate for model retraining only in cases of harmful drifts that detrimentally affect model performance. Through extensive experimental evaluations on various datasets and models, our findings demonstrate that our approach significantly improves cost-efficiency over baselines, while achieving comparable accuracy.}
}

@misc{dongreReSpActHarmonizingReasoning2025,
  title = {{{ReSpAct}}: {{Harmonizing Reasoning}}, {{Speaking}}, and {{Acting Towards Building Large Language Model-Based Conversational AI Agents}}},
  shorttitle = {{{ReSpAct}}},
  author = {Dongre, Vardhan and Yang, Xiaocheng and Acikgoz, Emre Can and Dey, Suvodip and Tur, Gokhan and {Hakkani-T{\"u}r}, Dilek},
  year = {2025},
  month = apr,
  number = {arXiv:2411.00927},
  eprint = {2411.00927},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.00927},
  urldate = {2025-05-19},
  abstract = {Large language model (LLM)-based agents are increasingly employed to interact with external environments (e.g., games, APIs, world models) to solve user-provided tasks. However, current frameworks often lack the ability to collaborate effectively with users in fully conversational settings. Conversations are essential for aligning on task details, achieving user-defined goals, and satisfying preferences. While existing agents address ambiguity through clarification questions, they underutilize the broader potential of an LLM's conversational capabilities. In this work, we introduce ReSpAct, an LLM-based agent designed to seamlessly integrate reasoning, decision-making, and dynamic dialogue for task-solving. Expanding on reasoning-first approaches like ReAct, ReSpAct employs active, free-flowing dialogues to interpret instructions, clarify goals, provide status updates, resolve subtask failures, and refine plans based on user inputs without any explicit dialogue schema. By alternating between task-solving actions and interactive conversations, ReSpAct demonstrates improved performance across diverse environments. We evaluate ReSpAct in user-interactive settings, including task-oriented dialogue systems (MultiWOZ) and decision-making tasks (ALFWorld, WebShop). ReSpAct outperforms ReAct with absolute success rate improvements of 6\% and 4\% in ALFWorld and WebShop, respectively, and achieves a 5.5\% gain in Inform and a 3\% gain in Success scores in MultiWOZ. These results highlight the value of integrating dynamic user-agent collaboration for more effective task resolution.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction}
}

@misc{dongSurveyIncontextLearning2023,
  title = {A {{Survey}} on {{In-context Learning}}},
  author = {Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Li, Lei and Sui, Zhifang},
  year = {2023},
  month = jun,
  number = {arXiv:2301.00234},
  eprint = {2301.00234},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.00234},
  urldate = {2023-11-14},
  abstract = {With the increasing ability of large language models (LLMs), in-context learning (ICL) has become a new paradigm for natural language processing (NLP), where LLMs make predictions only based on contexts augmented with a few examples. It has been a new trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, demonstration designing strategies, as well as related analysis. Finally, we discuss the challenges of ICL and provide potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.},
  archiveprefix = {arXiv}
}

@inproceedings{donkersSequentialUserbasedRecurrent2017,
  title = {Sequential {{User-based Recurrent Neural Network Recommendations}}},
  booktitle = {Proceedings of the {{Eleventh ACM Conference}} on {{Recommender Systems}}},
  author = {Donkers, Tim and Loepp, Benedikt and Ziegler, J{\"u}rgen},
  year = {2017},
  month = aug,
  series = {{{RecSys}} '17},
  pages = {152--160},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3109859.3109877},
  urldate = {2024-09-24},
  abstract = {Recurrent Neural Networks are powerful tools for modeling sequences. They are flexibly extensible and can incorporate various kinds of information including temporal order. These properties make them well suited for generating sequential recommendations. In this paper, we extend Recurrent Neural Networks by considering unique characteristics of the Recommender Systems domain. One of these characteristics is the explicit notion of the user recommendations are specifically generated for. We show how individual users can be represented in addition to sequences of consumed items in a new type of Gated Recurrent Unit to effectively produce personalized next item recommendations. Offline experiments on two real-world datasets indicate that our extensions clearly improve objective performance when compared to state-of-the-art recommender algorithms and to a conventional Recurrent Neural Network.},
  isbn = {978-1-4503-4652-8}
}

@inproceedings{doThatImportantHow2023,
  title = {``{{That}}'s Important, but...'': {{How Computer Science Researchers Anticipate Unintended Consequences}} of {{Their Research Innovations}}},
  shorttitle = {``{{That}}'s Important, But...''},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Do, Kimberly and Pang, Rock Yuren and Jiang, Jiachen and Reinecke, Katharina},
  year = {2023},
  month = apr,
  pages = {1--16},
  publisher = {ACM},
  address = {Hamburg Germany},
  doi = {10.1145/3544548.3581347},
  urldate = {2023-12-10},
  isbn = {978-1-4503-9421-5},
  langid = {english}
}

@misc{DRAFTACL2025,
  title = {[{{DRAFT}}] {{ACL}} 2025 {{Finetune LLM Agent}}},
  url = {https://www.overleaf.com/project/66fc911ee1f42188d0a3641d},
  urldate = {2025-03-17},
  abstract = {An online LaTeX editor that's easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.},
  langid = {english}
}

@inproceedings{drouhardAeoniumVisualAnalytics2017,
  title = {Aeonium: {{Visual}} Analytics to Support Collaborative Qualitative Coding},
  shorttitle = {Aeonium},
  booktitle = {2017 {{IEEE Pacific Visualization Symposium}} ({{PacificVis}})},
  author = {Drouhard, Margaret and Chen, Nan-Chen and Suh, Jina and Kocielnik, Rafal and {Pe{\~n}a-Araya}, Vanessa and Cen, Keting and Zheng, Xiangyi and Aragon, Cecilia R.},
  year = {2017},
  month = apr,
  pages = {220--229},
  issn = {2165-8773},
  doi = {10.1109/PACIFICVIS.2017.8031598},
  urldate = {2024-09-13},
  abstract = {Qualitative coding offers the potential to obtain deep insights into social media, but the technique can be inconsistent and hard to scale. Researchers using qualitative coding impose structure on unstructured data through ``codes'' that represent categories for analysis. Our visual analytics interface, Aeonium, supports human insight in collaborative coding through visual overviews of codes assigned by multiple researchers and distributions of important keywords and codes. The underlying machine learning model highlights ambiguity and inconsistency. Our goal was not to reduce qualitative coding to a machine-solvable problem, but rather to bolster human understanding gained from coding and reinterpreting the data collaboratively. We conducted an experimental study with 39 participants who coded tweets using our interface. In addition to increased understanding of the topic, participants reported that Aeonium's collaborative coding functionality helped them reflect on their own interpretations. Feedback from participants demonstrates that visual analytics can help facilitate rich qualitative analysis and suggests design implications for future exploration.},
  keywords = {Collaboration,collaborative coding,Encoding,Interviews,Labeling,machine learning,qualitative coding,social media research,social science,Tools,Visual analytics}
}

@misc{duanAttentionAllYou2020,
  title = {Attention {{Is All You Need}} for {{Chinese Word Segmentation}}},
  author = {Duan, Sufeng and Zhao, Hai},
  year = {2020},
  month = oct,
  number = {arXiv:1910.14537},
  eprint = {1910.14537},
  primaryclass = {cs},
  institution = {arXiv},
  doi = {10.48550/arXiv.1910.14537},
  urldate = {2022-06-09},
  abstract = {Taking greedy decoding algorithm as it should be, this work focuses on further strengthening the model itself for Chinese word segmentation (CWS), which results in an even more fast and more accurate CWS model. Our model consists of an attention only stacked encoder and a light enough decoder for the greedy segmentation plus two highway connections for smoother training, in which the encoder is composed of a newly proposed Transformer variant, Gaussian-masked Directional (GD) Transformer, and a biaffine attention scorer. With the effective encoder design, our model only needs to take unigram features for scoring. Our model is evaluated on SIGHAN Bakeoff benchmark datasets. The experimental results show that with the highest segmentation speed, the proposed model achieves new state-of-the-art or comparable performance against strong baselines in terms of strict closed test setting.},
  archiveprefix = {arXiv}
}

@article{duBiomedicaldomainPretrainedLanguage2020,
  title = {Biomedical-Domain Pre-Trained Language Model for Extractive Summarization},
  author = {Du, Yongping and Li, Qingxiao and Wang, Lulin and He, Yanqing},
  year = {2020},
  month = jul,
  journal = {Knowledge-Based Systems},
  volume = {199},
  pages = {105964},
  issn = {09507051},
  doi = {10/gg286r},
  urldate = {2021-11-30},
  abstract = {In recent years, the performance of deep neural network in extractive summarization task has been improved significantly compared with traditional methods. However, in the field of biomedical extractive summarization, existing methods cannot make good use of the domain-aware external knowledge; furthermore, the document structural feature is omitted by existing deep neural network model. In this paper, we propose a novel model called BioBERTSum to better capture token-level and sentence-level contextual representation, which uses a domain-aware bidirectional language model pre-trained on large-scale biomedical corpora as encoder, and further fine-tunes the language model for extractive text summarization task on single biomedical document. Especially, we adopt a sentence position embedding mechanism, which enables the model to learn the position information of sentences and achieve the structural feature of document. To the best of our knowledge, this is the first work to use the pre-trained language model and fine-tuning strategy for extractive summarization task in the biomedical domain. Experiments on PubMed dataset show that our proposed model outperforms the recent SOTA (state-of-the-art) model by ROUGE-1/2/L.},
  langid = {english}
}

@inproceedings{duDualModelWeighting2021,
  title = {Dual {{Model Weighting Strategy}} and {{Data Augmentation}} in {{Biomedical Question Answering}}},
  booktitle = {Proceedings of the 2021 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  author = {Du, Yongping and Yan, Jingya and Zhao, Yiliang and Lu, Yuxuan and Jin, Xingnan},
  year = {2021},
  abstract = {Biomedical Question Answering aims to extract an answer to the given question from a biomedical context. Due to the strong professionalism of specific domain, it's more difficult to build large-scale datasets for specific domain question answering. Existing methods are limited by the lack of training data, and the performance is not as good as in open-domain settings. We propose a model weighting strategy for the final answer prediction in biomedical domain, which combines the advantage of two models, open-domain model QANet and BioBERT pre- trained in Biomedical domain data. Specially, we adopt effective data augmentation strategies to improve the model performance, including slide window, summarization and round-trip trans- lation. The public biomedical dataset collected from PubMed provided by BioASQ is used to evaluate our approach. The results show that the model performance has been improved significantly on BioASQ 6B, 7B and 8B datasets compared to the single model.},
  annotation = {00000}
}

@inproceedings{duGLMGeneralLanguage2022,
  title = {{{GLM}}: {{General Language Model Pretraining}} with {{Autoregressive Blank Infilling}}},
  shorttitle = {{{GLM}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  year = {2022},
  month = may,
  pages = {320--335},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-long.26},
  urldate = {2023-03-29},
  abstract = {There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5). However, none of the pretraining frameworks performs the best for all tasks of three main categories including natural language understanding (NLU), unconditional generation, and conditional generation. We propose a General Language Model (GLM) based on autoregressive blank infilling to address this challenge. GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT and T5 on NLU tasks. Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of blanks. On a wide range of tasks across NLU, conditional and unconditional generation, GLM outperforms BERT, T5, and GPT given the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25{\textbackslash}mbox\${\textbackslash}times\$ parameters of BERT Large , demonstrating its generalizability to different downstream tasks.}
}

@inproceedings{duHierarchicalQuestionAwareContext2019,
  title = {Hierarchical {{Question-Aware Context Learning}} with {{Augmented Data}} for {{Biomedical Question Answering}}},
  booktitle = {2019 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  author = {Du, Yongping and Guo, Wenyang and Zhao, Yiliang},
  year = {2019},
  month = nov,
  pages = {370--375},
  publisher = {IEEE},
  address = {San Diego, CA, USA},
  doi = {10/gnm95v},
  urldate = {2021-11-30},
  abstract = {This paper is concerned with the task of biomedical Question Answering (QA) which refers to extracting an answer to the given question from a biomedical context. Current works have made progress on this task, but they are still severely restricted by the insufficient training data due to the domain-specific nature, which motivates us to further explore a powerful way to solve this problem. We propose a Hierarchical Question-Aware Context Learning (HQACL) model for the biomedical QA task constituted by multi-level attention. The interaction between the question and the context can be captured layer by layer, with multigrained embeddings to strengthen the ability of the language representation. A special training method called DA, including two parts namely domain adaptation and data augmentation, is also introduced to enhance the model performance. Domain adaptation can be defined as pre-training on a large-scale opendomain dataset and fine-tuning on the small training set of the target domain. As for the data augmentation, the Round-trip translation method is adopted to create new data with various expressions, which almost doubles the training set. The public biomedical dataset collected from PubMed provided by BioASQ is used to evaluate our model. The results show that our approach is superior to the best recent solution and achieves a new state of the art.},
  isbn = {978-1-7281-1867-3},
  langid = {english}
}

@article{duImprovingBiomedicalQuestion2022,
  title = {Improving {{Biomedical Question Answering}} by {{Data Augmentation}} and {{Model Weighting}}},
  author = {Du, Yongping and Yan, Jingya and Lu, Yuxuan and Zhao, Yiliang and Jin, Xingnan},
  year = {2022},
  journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
  pages = {1--1},
  issn = {1557-9964},
  doi = {10.1109/TCBB.2022.3171388},
  abstract = {Biomedical Question Answering aims to extract an answer to the given question from a biomedical context. Due to the strong professionalism of specific domain, its more difficult to build large-scale datasets for specific domain question answering. Existing methods are limited by the lack of training data, and the performance is not as good as in open-domain settings, especially degrading when facing to the adversarial sample. We try to resolve the above issues. Firstly, effective data augmentation strategies are adopted to improve the model training, including slide window, summarization and round-trip translation. Secondly, we propose a model weighting strategy for the final answer prediction in biomedical domain, which combines the advantage of two models, open-domain model QANet and BioBERT pre-trained in biomedical domain data. Finally, we give adversarial training to reinforce the robustness of the model. The public biomedical dataset collected from PubMed provided by BioASQ challenge is used to evaluate our approach. The results show that the model performance has been improved significantly compared to the single model and other models participated in BioASQ challenge. It can learn richer semantic expression from data augmentation and adversarial samples, which is beneficial to solve more complex question answering problems in biomedical domain.}
}

@inproceedings{durrNurseCareDesignInTheWild2020,
  title = {{{NurseCare}}: {{Design}} and '{{In-The-Wild}}' {{Evaluation}} of a {{Mobile System}} to {{Promote}} the {{Ergonomic Transfer}} of {{Patients}}},
  shorttitle = {{{NurseCare}}},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {D{\"u}rr, Maximilian and Gr{\"o}schel, Carla and Pfeil, Ulrike and Reiterer, Harald},
  year = {2020},
  month = apr,
  series = {{{CHI}} '20},
  pages = {1--13},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3313831.3376851},
  urldate = {2023-08-31},
  abstract = {Nurses are frequently required to transfer patients as part of their daily duties. However, the manual transfer of patients is a major risk factor for injuries to the back. Although the Kinaesthetics Care Conception can help to address this issue, existing support for the integration of the concept into nursing-care practice is low. We present NurseCare, a mobile system that aims to promote the practical application of ergonomic patient transfers based on the Kinaesthetics Care Conception. NurseCare consists of a wearable and a smartphone app. Key features of NurseCare include mobile accessible instructions for ergonomic patient transfers, in-situ feedback for the risky bending of the back, and long-term feedback. We evaluated NurseCare in a nine participant 'in-the-wild' evaluation. Results indicate that NurseCare can facilitate ergonomic work while providing a high user experience adequate to the nurses' work domain, and reveal how NurseCare can be incorporated in given practices.},
  isbn = {978-1-4503-6708-0}
}

@inproceedings{ebertQButterflyLightweightSurvey2023,
  title = {{{QButterfly}}: {{Lightweight Survey Extension}} for {{Online User Interaction Studies}} for {{Non-Tech-Savvy Researchers}}},
  shorttitle = {{{QButterfly}}},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Ebert, Nico and Scheppler, Bj{\"o}rn and Ackermann, Kurt Alexander and Geppert, Tim},
  year = {2023},
  month = apr,
  series = {{{CHI}} '23},
  pages = {1--8},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3544548.3580780},
  urldate = {2024-09-13},
  abstract = {We provide a user-friendly, flexible, and lightweight open-source HCI toolkit (github.com/QButterfly) that allows non-tech-savvy researchers to conduct online user interaction studies using the widespread Qualtrics and LimeSurvey platforms. These platforms already provide rich functionality (e.g., for experiments or usability tests) and therefore lend themselves to an extension to display stimulus web pages and record clickstreams. The toolkit consists of a survey template with embedded JavaScript, a JavaScript library embedded in the HTML web pages, and scripts to analyze the collected data. No special programming skills are required to set up a study or match survey data and user interaction data after data collection. We empirically validated the software in a laboratory and a field study. We conclude that this extension, even in its preliminary version, has the potential to make online user interaction studies (e.g., with crowdsourced participants) accessible to a broader range of researchers.},
  isbn = {978-1-4503-9421-5}
}

@misc{eloundouGPTsAreGPTs2023,
  title = {{{GPTs}} Are {{GPTs}}: {{An Early Look}} at the {{Labor Market Impact Potential}} of {{Large Language Models}}},
  shorttitle = {{{GPTs}} Are {{GPTs}}},
  author = {Eloundou, Tyna and Manning, Sam and Mishkin, Pamela and Rock, Daniel},
  year = {2023},
  month = mar,
  number = {arXiv:2303.10130},
  eprint = {2303.10130},
  primaryclass = {cs, econ, q-fin},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.10130},
  urldate = {2023-03-21},
  abstract = {We investigate the potential implications of Generative Pre-trained Transformer (GPT) models and related technologies on the U.S. labor market. Using a new rubric, we assess occupations based on their correspondence with GPT capabilities, incorporating both human expertise and classifications from GPT-4. Our findings indicate that approximately 80\% of the U.S. workforce could have at least 10\% of their work tasks affected by the introduction of GPTs, while around 19\% of workers may see at least 50\% of their tasks impacted. The influence spans all wage levels, with higher-income jobs potentially facing greater exposure. Notably, the impact is not limited to industries with higher recent productivity growth. We conclude that Generative Pre-trained Transformers exhibit characteristics of general-purpose technologies (GPTs), suggesting that as these models could have notable economic, social, and policy implications.},
  archiveprefix = {arXiv}
}

@misc{EnumerateSpacingGoogle,
  title = {Enumerate Spacing - {{Google Search}}},
  url = {https://www.google.com/search?q=enumerate+spacing&rlz=1C5GCEM_enUS1127US1127&oq=enumerate+spacing&gs_lcrp=EgZjaHJvbWUyBggAEEUYOdIBCDQxMDZqMGo0qAIAsAIA&sourceid=chrome&ie=UTF-8},
  urldate = {2024-11-02}
}

@inproceedings{eskonenAutomatingGUITesting2020,
  title = {Automating {{GUI Testing}} with {{Image-Based Deep Reinforcement Learning}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Autonomic Computing}} and {{Self-Organizing Systems}} ({{ACSOS}})},
  author = {Eskonen, Juha and Kahles, Julen and Reijonen, Joel},
  year = {2020},
  month = aug,
  pages = {160--167},
  doi = {10.1109/ACSOS49614.2020.00038},
  urldate = {2024-09-11},
  abstract = {Users interact with modern applications and devices through graphical user interfaces (GUIs). To ensure intuitive and easy usability, the GUIs need to be tested, where developers aim at finding possible bugs and inconsistent functionality. Manual GUI testing requires time and effort, and thus, its efficiency can be improved with automation. Conventional automation tools for GUI testing reduce the burden of manual testing but also introduce challenges in the maintenance of test cases. In order to overcome these issues, we propose a deep-reinforcement-learning-based (DRL) solution for automated and adaptive GUI testing. Specifically, we propose and evaluate the performance of an image-based DRL solution. We adapt the asynchronous advantage actor-critic (A3C) algorithm to GUI testing inspired by how a human uses a GUI. We feed screenshots of the GUI as the input and let the algorithm decide how to interact with GUI components. We observe that our solution can achieve up to six times higher exploration efficiency compared to selected baseline algorithms. Moreover, our solution is more efficient than inexperienced human users and almost as efficient as an experienced human user in our experimental GUI testing scenario. For these reasons, image-based DRL exploration can be considered as a viable GUI testing method.},
  keywords = {automation,deep reinforcement learning,Graphical user interfaces,GUI testing,Humanoid robots,image processing,Kernel,Machine learning,Manuals,Task analysis,Testing}
}

@article{evansDualProcessTheoriesHigher2013,
  title = {Dual-{{Process Theories}} of {{Higher Cognition}}: {{Advancing}} the {{Debate}}},
  shorttitle = {Dual-{{Process Theories}} of {{Higher Cognition}}},
  author = {Evans, Jonathan St B. T. and Stanovich, Keith E.},
  year = {2013},
  month = may,
  journal = {Perspectives on Psychological Science: A Journal of the Association for Psychological Science},
  volume = {8},
  number = {3},
  pages = {223--241},
  issn = {1745-6916},
  doi = {10.1177/1745691612460685},
  abstract = {Dual-process and dual-system theories in both cognitive and social psychology have been subjected to a number of recently published criticisms. However, they have been attacked as a category, incorrectly assuming there is a generic version that applies to all. We identify and respond to 5 main lines of argument made by such critics. We agree that some of these arguments have force against some of the theories in the literature but believe them to be overstated. We argue that the dual-processing distinction is supported by much recent evidence in cognitive science. Our preferred theoretical approach is one in which rapid autonomous processes (Type 1) are assumed to yield default responses unless intervened on by distinctive higher order reasoning processes (Type 2). What defines the difference is that Type 2 processing supports hypothetical thinking and load heavily on working memory.},
  langid = {english},
  pmid = {26172965},
  keywords = {dual processes,dual systems,individual differences,rationality,working memory}
}

@article{fabbriSummEvalReevaluatingSummarization2021,
  title = {{{SummEval}}: {{Re-evaluating Summarization Evaluation}}},
  shorttitle = {{{SummEval}}},
  author = {Fabbri, Alexander R. and Kry{\'s}ci{\'n}ski, Wojciech and McCann, Bryan and Xiong, Caiming and Socher, Richard and Radev, Dragomir},
  year = {2021},
  month = apr,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {9},
  pages = {391--409},
  issn = {2307-387X},
  doi = {10.1162/tacl\_a\_00373},
  urldate = {2022-08-02},
  abstract = {The scarcity of comprehensive up-to-date studies on evaluation metrics for text summarization and the lack of consensus regarding evaluation protocols continue to inhibit progress. We address the existing shortcomings of summarization evaluation methods along five dimensions: 1) we re-evaluate 14 automatic evaluation metrics in a comprehensive and consistent fashion using neural summarization model outputs along with expert and crowd-sourced human annotations; 2) we consistently benchmark 23 recent summarization models using the aforementioned automatic evaluation metrics; 3) we assemble the largest collection of summaries generated by models trained on the CNN/DailyMail news dataset and share it in a unified format; 4) we implement and share a toolkit that provides an extensible and unified API for evaluating summarization models across a broad range of automatic metrics; and 5) we assemble and share the largest and most diverse, in terms of model types, collection of human judgments of model-generated summaries on the CNN/Daily Mail dataset annotated by both expert judges and crowd-source workers. We hope that this work will help promote a more complete evaluation protocol for text summarization as well as advance research in developing evaluation metrics that better correlate with human judgments.}
}

@misc{fangWebEvolverEnhancingWeb2025,
  title = {{{WebEvolver}}: {{Enhancing Web Agent Self-Improvement}} with {{Coevolving World Model}}},
  shorttitle = {{{WebEvolver}}},
  author = {Fang, Tianqing and Zhang, Hongming and Zhang, Zhisong and Ma, Kaixin and Yu, Wenhao and Mi, Haitao and Yu, Dong},
  year = {2025},
  month = apr,
  number = {arXiv:2504.21024},
  eprint = {2504.21024},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.21024},
  urldate = {2025-06-04},
  abstract = {Agent self-improvement, where the backbone Large Language Model (LLM) of the agent are trained on trajectories sampled autonomously based on their own policies, has emerged as a promising approach for enhancing performance. Recent advancements, particularly in web environments, face a critical limitation: their performance will reach a stagnation point during autonomous learning cycles, hindering further improvement. We argue that this stems from limited exploration of the web environment and insufficient exploitation of pre-trained web knowledge in LLMs. To improve the performance of self-improvement, we propose a novel framework that introduces a co-evolving World Model LLM. This world model predicts the next observation based on the current observation and action within the web environment. Leveraging LLMs' pretrained knowledge of abundant web content, the World Model serves dual roles: (1) as a virtual web server generating self-instructed training data to continuously refine the agent's policy, and (2) as an imagination engine during inference, enabling look-ahead simulation to guide action selection for the agent LLM. Experiments in real-world web environments (Mind2Web-Live, WebVoyager, and GAIA-web) show a 10\% performance gain over existing self-evolving agents, demonstrating the efficacy and generalizability of our approach, without using any distillation from more powerful close-sourced models. Our work establishes the necessity of integrating world models into autonomous agent frameworks to unlock sustained adaptability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{fanMuffinChihuahuaChallenging2024,
  title = {Muffin or {{Chihuahua}}? {{Challenging Large Vision-Language Models}} with {{Multipanel VQA}}},
  shorttitle = {Muffin or {{Chihuahua}}?},
  author = {Fan, Yue and Gu, Jing and Zhou, Kaiwen and Yan, Qianqi and Jiang, Shan and Kuo, Ching-Chen and Guan, Xinze and Wang, Xin Eric},
  year = {2024},
  number = {arXiv:2401.15847},
  eprint = {2401.15847},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.15847},
  urldate = {2024-04-08},
  abstract = {Multipanel images, commonly seen as web screenshots, posters, etc., pervade our daily lives. These images, characterized by their composition of multiple subfigures in distinct layouts, effectively convey information to people. Toward building advanced multimodal AI applications, such as agents that understand complex scenes and navigate through webpages, the skill of multipanel visual reasoning is essential, and a comprehensive evaluation of models in this regard is important. Therefore, we introduce Multipanel Visual Question Answering (MultipanelVQA), a novel benchmark comprising 6,600 triplets of questions, answers, and multipanel images that specifically challenge models in comprehending multipanel images. Our evaluation shows that questions in the MultipanelVQA benchmark pose significant challenges to the state-of-the-art Large Vision Language Models (LVLMs) tested, even though humans can attain approximately 99{\textbackslash}\% accuracy on these questions. Distinctively, the MultipanelVQA benchmark features synthetically generated multipanel images specifically crafted to isolate and assess the impact of various factors, such as the layout, on LVLMs' multipanel image comprehension abilities. As a result, in addition to benchmarking the capabilities of LVLMs in understanding multipanel images, we analyze the potential causes for LVLMs' performance and offer insights for enhancement with the synthetic data. Code and data are released at https://sites.google.com/view/multipanelvqa/home.},
  archiveprefix = {arXiv},
  langid = {american}
}

@article{farhadiDomainAdaptationReinforcement2024,
  title = {Domain Adaptation in Reinforcement Learning: A Comprehensive and Systematic Study},
  shorttitle = {Domain Adaptation in Reinforcement Learning},
  author = {Farhadi, Amirfarhad and Mirzarezaee, Mitra and Sharifi, Arash and Teshnehlab, Mohammad},
  year = {2024},
  month = nov,
  journal = {Frontiers of Information Technology \& Electronic Engineering},
  volume = {25},
  number = {11},
  pages = {1446--1465},
  issn = {2095-9230},
  doi = {10.1631/FITEE.2300668},
  urldate = {2025-02-14},
  abstract = {Reinforcement learning (RL) has shown significant potential for dealing with complex decision-making problems. However, its performance relies heavily on the availability of a large amount of high-quality data. In many real-world situations, data distribution in the target domain may differ significantly from that in the source domain, leading to a significant drop in the performance of RL algorithms. Domain adaptation (DA) strategies have been proposed to address this issue by transferring knowledge from a source domain to a target domain. However, there have been no comprehensive and in-depth studies to evaluate these approaches. In this paper we present a comprehensive and systematic study of DA in RL. We first introduce the basic concepts and formulations of DA in RL and then review the existing DA methods used in RL. Our main objective is to fill the existing literature gap regarding DA in RL. To achieve this, we conduct a rigorous evaluation of state-of-the-art DA approaches. We aim to provide comprehensive insights into DA in RL and contribute to advancing knowledge in this field. The existing DA approaches are divided into seven categories based on application domains. The approaches in each category are discussed based on the important data adaptation metrics, and then their key characteristics are described. Finally, challenging issues and future research trends are highlighted to assist researchers in developing innovative improvements.},
  langid = {english},
  keywords = {,Artificial Intelligence,Domain adaptation,Machine learning,Reinforcement learning,TP391}
}

@inproceedings{fastEmergentCrowdscaleProgramming2014,
  title = {Emergent, Crowd-Scale Programming Practice in the {{IDE}}},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Fast, Ethan and Steffee, Daniel and Wang, Lucy and Brandt, Joel R. and Bernstein, Michael S.},
  year = {2014},
  month = apr,
  series = {{{CHI}} '14},
  pages = {2491--2500},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2556288.2556998},
  urldate = {2023-08-31},
  abstract = {While emergent behaviors are uncodified across many domains such as programming and writing, interfaces need explicit rules to support users. We hypothesize that by codifying emergent programming behavior, software engineering interfaces can support a far broader set of developer needs. To explore this idea, we built Codex, a knowledge base that records common practice for the Ruby programming language by indexing over three million lines of popular code. Codex enables new data-driven interfaces for programming systems: statistical linting, identifying code that is unlikely to occur in practice and may constitute a bug; pattern annotation, automatically discovering common programming idioms and annotating them with metadata using expert crowdsourcing; and library generation, constructing a utility package that encapsulates and reflects emergent software practice. We evaluate these applications to find Codex captures a broad swatch of programming practice, statistical linting detects problematic code snippets, and pattern annotation discovers nontrivial idioms such as basic HTTP authentication and database migration templates. Our work suggests that operationalizing practice-driven knowledge in structured domains such as programming can enable a new class of user interfaces.},
  isbn = {978-1-4503-2473-1}
}

@misc{fengGroupinGroupPolicyOptimization2025,
  title = {Group-in-{{Group Policy Optimization}} for {{LLM Agent Training}}},
  author = {Feng, Lang and Xue, Zhenghai and Liu, Tingcong and An, Bo},
  year = {2025},
  month = may,
  number = {arXiv:2505.10978},
  eprint = {2505.10978},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.10978},
  urldate = {2025-06-04},
  abstract = {Recent advances in group-based reinforcement learning (RL) have driven frontier large language models (LLMs) in single-turn tasks like mathematical reasoning. However, their scalability to long-horizon LLM agent training remains limited. Unlike static tasks, agent-environment interactions unfold over many steps and often yield sparse or delayed rewards, making credit assignment across individual steps significantly more challenging. In this work, we propose Group-in-Group Policy Optimization (GiGPO), a novel RL algorithm that achieves fine-grained credit assignment for LLM agents while preserving the appealing properties of group-based RL: critic-free, low memory, and stable convergence. GiGPO introduces a two-level structure for estimating relative advantage: (i) At the episode-level, GiGPO computes macro relative advantages based on groups of complete trajectories; (ii) At the step-level, GiGPO introduces an anchor state grouping mechanism that retroactively constructs step-level groups by identifying repeated environment states across trajectories. Actions stemming from the same state are grouped together, enabling micro relative advantage estimation. This hierarchical structure effectively captures both global trajectory quality and local step effectiveness without relying on auxiliary models or additional rollouts. We evaluate GiGPO on two challenging agent benchmarks, ALFWorld and WebShop, using Qwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct. Crucially, GiGPO delivers fine-grained per-step credit signals and achieves performance gains of {$>$} 12{\textbackslash}\% on ALFWorld and {$>$} 9{\textbackslash}\% on WebShop over the GRPO baseline: all while maintaining the same GPU memory overhead, identical LLM rollout, and incurring little to no additional time cost.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@misc{fengLLMdrivenDialogueState2023,
  title = {Towards {{LLM-driven Dialogue State Tracking}}},
  author = {Feng, Yujie and Lu, Zexin and Liu, Bo and Zhan, Liming and Wu, Xiao-Ming},
  year = {2023},
  month = oct,
  number = {arXiv:2310.14970},
  eprint = {2310.14970},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.14970},
  urldate = {2024-10-01},
  abstract = {Dialogue State Tracking (DST) is of paramount importance in ensuring accurate tracking of user goals and system actions within task-oriented dialogue systems. The emergence of large language models (LLMs) such as GPT3 and ChatGPT has sparked considerable interest in assessing their efficacy across diverse applications. In this study, we conduct an initial examination of ChatGPT's capabilities in DST. Our evaluation uncovers the exceptional performance of ChatGPT in this task, offering valuable insights to researchers regarding its capabilities and providing useful directions for designing and enhancing dialogue systems. Despite its impressive performance, ChatGPT has significant limitations including its closed-source nature, request restrictions, raising data privacy concerns, and lacking local deployment capabilities. To address these concerns, we present LDST, an LLM-driven DST framework based on smaller, open-source foundation models. By utilizing a novel domain-slot instruction tuning method, LDST achieves performance on par with ChatGPT. Comprehensive evaluations across three distinct experimental settings, we find that LDST exhibits remarkable performance improvements in both zero-shot and few-shot setting compared to previous SOTA methods. The source code is provided for reproducibility.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@article{fengPretrainedLanguageEmbeddingbased2020,
  title = {Pre-Trained {{Language Embedding-based Contextual Summary}} and {{Multi-scale Transmission Network}} for {{Aspect Extraction}}},
  author = {Feng, Cong and Rao, Yuan and Nazir, Ambreen and Wu, Lianwei and He, Long},
  year = {2020},
  month = jan,
  journal = {Procedia Computer Science},
  series = {2019 {{International Conference}} on {{Identification}}, {{Information}} and {{Knowledge}} in the {{Internet}} of {{Things}}},
  volume = {174},
  pages = {40--49},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2020.06.054},
  urldate = {2022-07-18},
  abstract = {With the development of IOT and 5G technology, people's demand for information acquisition is more inclined to accuracy, intelligence and timeliness. How to help designer obtain the real-time information of specific product reviews from the massive online consumers and upgrade the new design strategy has become a hot topic for research. In this paper, we define the problem as an aspect extraction task, and propose a novel deep learning model that comprises of three modules: pre-training language model embedding, multi-scale transmission network and contextual summary, which aims to provide an end-to-end solution without any additional supervision. To this end, we adopt BERT to overcome the disadvantage of traditional embedding methods, which cannot combine contextual information. Multi-scale transmission network is proposed to integrate the Bi-GRU and a group of CNN networks to extract sequential and local features of words respectively. Contextual summary is a tailor-made representation distilled from the input sentence, conditioned on each current word, and thus can assist aspect prediction. Experimental results over three benchmark SemEval datasets clearly illustrate that our model can achieve the state-of-the-art performance.},
  langid = {english}
}

@inproceedings{fereidouniGroundedLanguageAgent2024,
  title = {Grounded {{Language Agent}} for {{Product Search}} via {{Intelligent Web Interactions}}},
  booktitle = {Proceedings of the 1st {{Workshop}} on {{Customizable NLP}}: {{Progress}} and {{Challenges}} in {{Customizing NLP}} for a {{Domain}}, {{Application}}, {{Group}}, or {{Individual}} ({{CustomNLP4U}})},
  author = {Fereidouni, Moghis and Mosharrof, Adib and Siddique, A.b.},
  editor = {Kumar, Sachin and Balachandran, Vidhisha and Park, Chan Young and Shi, Weijia and Hayati, Shirley Anugrah and Tsvetkov, Yulia and Smith, Noah and Hajishirzi, Hannaneh and Kang, Dongyeop and Jurgens, David},
  year = {2024},
  month = nov,
  pages = {63--75},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.customnlp4u-1.7},
  urldate = {2024-12-11},
  abstract = {Recent research has focused on developing agents powered by large language models (LLMs) to accomplish complex high-level user intents. However, employing LLMs with billions of parameters (e.g., GPT-4) may incur substantial costs on top of handcrafting extensive prompts. To address this, we introduce a Grounded Language Agent for Intelligent Web Interactions, named GLAINTEL. GLAINTEL employs Flan-T5 as its backbone and is flexible in training in various settings: unsupervised learning, supervised learning, and unsupervised domain adaptation. Specifically, we tackle both the challenge of learning without human demonstrations and the opportunity to leverage human demonstrations effectively when those are available. Additionally, we explore unsupervised domain adaptation for cases where demonstrations are limited to a specific domain. Experimental evaluations across diverse setups demonstrate the effectiveness of GLAINTEL in unsupervised settings, outperforming in-context learning-based approaches that employ larger models with up to 540 billion parameters. Surprisingly, behavioral cloning-based methods that straightforwardly use human demonstrations do not outperform unsupervised variants of GLAINTEL. Additionally, we show that combining human demonstrations with reinforcement learning-based training yields results comparable to methods utilizing GPT-4. The code is available at: https://github.com/MultifacetedNLP/Web-Agents-Unsupervised}
}

@inproceedings{fernandesAgentsAutomatedUser2021,
  title = {Agents for {{Automated User Experience Testing}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Software Testing}}, {{Verification}} and {{Validation Workshops}} ({{ICSTW}})},
  author = {Fernandes, Pedro M. and Lopes, Manuel and Prada, Rui},
  year = {2021},
  month = apr,
  pages = {247--253},
  doi = {10.1109/ICSTW52544.2021.00049},
  urldate = {2024-09-11},
  abstract = {The automation of functional testing in software has allowed developers to continuously check for negative impacts on functionality throughout the iterative phases of development. This is not the case for User eXperience (UX), which has hitherto relied almost exclusively on testing with real users. User testing is a slow endeavour that can become a bottleneck for development of interactive systems. To address this problem, we here propose an agent based approach for automatic UX testing. We develop agents with basic problem solving skills and a core affect model, allowing us to model an artificial affective state as they traverse different levels of a game. Although this research is still at a primordial state, we believe the results here presented make a strong case for the use of intelligent agents endowed with affective computing models for automating UX testing.},
  keywords = {agents,artificial intelligence,Computational modeling,Conferences,Games,Robustness,Software,software testing,Software testing,user experience,User experience}
}

@inproceedings{folstadAnalysisPracticalUsability2012,
  title = {Analysis in Practical Usability Evaluation: A Survey Study},
  shorttitle = {Analysis in Practical Usability Evaluation},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {F{\o}lstad, Asbj{\o}rn and Law, Effie and Hornb{\ae}k, Kasper},
  year = {2012},
  month = may,
  series = {{{CHI}} '12},
  pages = {2127--2136},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2207676.2208365},
  urldate = {2024-09-10},
  abstract = {Analysis is a key part of conducting usability evaluations, yet rarely systematically studied. Thus, we lack direction on how to do research on supporting practitioners' analysis and lose an opportunity for practitioners to learn from each other. We have surveyed 155 usability practitioners on the analysis in their latest usability evaluation. Analysis is typically flexible and light-weight. At the same time, practitioners see a need to strengthen reliability in evaluation. Redesign is closely integrated with analysis; more than half of the respondents provide visual redesign suggestions in their evaluation deliverables. Analysis support from academic research, including tools, forms and structured formats, does not seem to have direct impact on analysis practice. We provide six recommendations for future research to better support analysis.},
  isbn = {978-1-4503-1015-4}
}

@article{fraserExtractingUMLSConcepts,
  title = {Extracting {{UMLS Concepts}} from {{Medical Text Using General}} and {{Domain-Specific Deep Learning Models}}},
  author = {Fraser, Kathleen C and Nejadgholi, Isar and Bruijn, Berry De and Li, Muqun and LaPlante, Astha and Abidine, Khaldoun Zine El},
  pages = {11},
  abstract = {Entity recognition is a critical first step to a number of clinical NLP applications, such as entity linking and relation extraction. We present the first attempt to apply state-of-theart entity recognition approaches on a newly released dataset, MedMentions. This dataset contains over 4000 biomedical abstracts, annotated for UMLS semantic types. In comparison to existing datasets, MedMentions contains a far greater number of entity types, and thus represents a more challenging but realistic scenario in a real-world setting. We explore a number of relevant dimensions, including the use of contextual versus non-contextual word embeddings, general versus domain-specific unsupervised pre-training, and different deep learning architectures. We contrast our results against the well-known i2b2 2010 entity recognition dataset, and propose a new method to combine general and domain-specific information. While producing a state-of-the-art result for the i2b2 2010 task (F1 = 0.90), our results on MedMentions are significantly lower (F1 = 0.63), suggesting there is still plenty of opportunity for improvement on this new data.},
  langid = {english}
}

@incollection{Frontmatter2012,
  title = {Frontmatter},
  booktitle = {Handbook of {{Human Factors}} and {{Ergonomics}}},
  year = {2012},
  pages = {i-xx},
  publisher = {John Wiley \& Sons, Ltd},
  doi = {10.1002/9781118131350.fmatter},
  urldate = {2024-09-13},
  abstract = {The prelims comprise: Half Title Title Copyright About the Editor Contributors Preface Contents},
  isbn = {978-1-118-13135-0},
  langid = {english}
}

@inproceedings{fuATNetAnsweringClozeStyle2019,
  title = {{{ATNet}}: {{Answering Cloze-Style Questions}} via {{Intra-attention}} and {{Inter-attention}}},
  shorttitle = {{{ATNet}}},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Fu, Chengzhen and Li, Yuntao and Zhang, Yan},
  editor = {Yang, Qiang and Zhou, Zhi-Hua and Gong, Zhiguo and Zhang, Min-Ling and Huang, Sheng-Jun},
  year = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {242--252},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-16145-3\_19},
  abstract = {This paper proposes a novel framework, named ATNet, for answering cloze-style questions over documents. Our model, in the encoder phase, projects all contextual embeddings into multiple latent semantic spaces, with representations of each space attending to a specific aspect of semantics. Long-term dependencies among the whole document are captured via the intra-attention module. A gate is produced to control the degree to which the retrieved dependency information is fused and the previous token embedding is exposed. Then, in the interaction phase, the context is aligned with the query across different semantic spaces to achieve the information aggregation. Specifically, we compute inter-attention based on a sophisticated feature set. Experiments and ablation studies demonstrate the effectiveness of ATNet.},
  isbn = {978-3-030-16145-3},
  langid = {english}
}

@inproceedings{fuEAReaderEnhance2019,
  title = {{{EA Reader}}: {{Enhance Attentive Reader}} for {{Cloze-Style Question Answering}} via {{Multi-Space Context Fusion}}},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Fu, Chengzhen and Zhang, Yan},
  year = {2019},
  month = jul,
  volume = {33},
  pages = {6375--6382},
  doi = {10.1609/aaai.v33i01.33016375},
  urldate = {2022-04-11},
  abstract = {\&lt;p\&gt;Query-document semantic interactions are essential for the success of many cloze-style question answering models. Recently, researchers have proposed several attention-based methods to predict the answer by focusing on appropriate subparts of the context document. In this paper, we design a novel module to produce the query-aware context vector, named Multi-Space based Context Fusion (MSCF), with the following considerations: (1) interactions are applied across multiple latent semantic spaces; (2) attention is measured at bit level, not at token level. Moreover, we extend MSCF to the multi-hop architecture. This unified model is called Enhanced Attentive Reader (EA Reader). During the iterative inference process, the reader is equipped with a novel memory update rule and maintains the understanding of documents through \&lt;em\&gt;read\&lt;/em\&gt;, \&lt;em\&gt;update\&lt;/em\&gt; and \&lt;em\&gt;write\&lt;/em\&gt; operations. We conduct extensive experiments on four real-world datasets. Our results demonstrate that EA Reader outperforms state-of-the-art models.\&lt;/p\&gt;},
  chapter = {AAAI Technical Track: Natural Language Processing}
}

@misc{furutaExposingLimitationsLanguage2024,
  title = {Exposing {{Limitations}} of {{Language Model Agents}} in {{Sequential-Task Compositions}} on the {{Web}}},
  author = {Furuta, Hiroki and Matsuo, Yutaka and Faust, Aleksandra and Gur, Izzeddin},
  year = {2024},
  month = feb,
  number = {arXiv:2311.18751},
  eprint = {2311.18751},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.18751},
  urldate = {2024-08-08},
  abstract = {Language model agents (LMA) recently emerged as a promising paradigm on muti-step decision making tasks, often outperforming humans and other reinforcement learning agents. Despite the promise, their performance on real-world applications that often involve combinations of tasks is still underexplored. In this work, we introduce a new benchmark, called CompWoB -- 50 new compositional web automation tasks reflecting more realistic assumptions. We show that while existing prompted LMAs (gpt-3.5-turbo or gpt-4) achieve 94.0\% average success rate on base tasks, their performance degrades to 24.9\% success rate on compositional tasks. On the other hand, transferred LMAs (finetuned only on base tasks) show less generalization gap, dropping from 85.4\% to 54.8\%. By balancing data distribution across tasks, we train a new model, HTML-T5++, that surpasses human-level performance (95.2\%) on MiniWoB, and achieves the best zero-shot performance on CompWoB (61.5\%). While these highlight the promise of small-scale finetuned and transferred models for task compositionality, their performance further degrades under different instruction compositions changing combinational order. In contrast to the recent remarkable success of LMA, our benchmark and detailed analysis emphasize the necessity of building LMAs that are robust and generalizable to task compositionality for real-world deployment.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{furutaMultimodalWebNavigation2024,
  title = {Multimodal {{Web Navigation}} with {{Instruction-Finetuned Foundation Models}}},
  author = {Furuta, Hiroki and Lee, Kuang-Huei and Nachum, Ofir and Matsuo, Yutaka and Faust, Aleksandra and Gu, Shixiang Shane and Gur, Izzeddin},
  year = {2024},
  month = feb,
  number = {arXiv:2305.11854},
  eprint = {2305.11854},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.11854},
  urldate = {2025-04-10},
  abstract = {The progress of autonomous web navigation has been hindered by the dependence on billions of exploratory interactions via online reinforcement learning, and domain-specific model designs that make it difficult to leverage generalization from rich out-of-domain data. In this work, we study data-driven offline training for web agents with vision-language foundation models. We propose an instruction-following multimodal agent, WebGUM, that observes both webpage screenshots and HTML pages and outputs web navigation actions, such as click and type. WebGUM is trained by jointly finetuning an instruction-finetuned language model and a vision encoder with temporal and local perception on a large corpus of demonstrations. We empirically demonstrate this recipe improves the agent's ability of grounded multimodal perception, HTML comprehension, and multi-step reasoning, outperforming prior works by a significant margin. On the MiniWoB, we improve over the previous best offline methods by more than 45.8\%, even outperforming online-finetuned SoTA, humans, and GPT-4-based agent. On the WebShop benchmark, our 3-billion-parameter model achieves superior performance to the existing SoTA, PaLM-540B. Furthermore, WebGUM exhibits strong positive transfer to the real-world planning tasks on the Mind2Web. We also collect 347K high-quality demonstrations using our trained models, 38 times larger than prior work, and make them available to promote future research in this direction.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{ganjiEaseCodeComplex2018,
  title = {Ease on {{Down}} the {{Code}}: {{Complex Collaborative Qualitative Coding Simplified}} with '{{Code Wizard}}'},
  shorttitle = {Ease on {{Down}} the {{Code}}},
  author = {Ganji, Abbas and Orand, Mania and McDonald, David W.},
  year = {2018},
  month = nov,
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {2},
  number = {CSCW},
  pages = {132:1--132:24},
  doi = {10.1145/3274401},
  urldate = {2024-09-13},
  abstract = {This paper describes the design and development of a preliminary qualitative coding tool as well as a method to improve the process of achieving inter-coder reliability (ICR) in small teams. Software applications that support qualitative coding do not sufficiently assist collaboration among coders and overlook some fundamental issues related to ICR. We propose a new dimension of collaborative coding called 'coders' certainty" and demonstrate its ability to illustrate valuable code disagreements that are missing from existing approaches. Through a case study, we describe the utility of our tool, Code Wizard, and how it helped a group of researchers effectively collaborate to code naturalistic observation data. We report the valuable lessons we learned from the development of our tool and method: (1) identifying coders' certainty constitutes an important part of determining the quality of data analysis and facilitates identifying overlapping and ambiguous codes, (2) making the details of coding process visible helps streamline the coding process and leads to a sense of ownership of the research results, and (3) there is valuable information hidden in coding disagreements that can be leveraged for improving the process of data analysis.}
}

@misc{ganModelasaServiceMaaSSurvey2023,
  title = {Model-as-a-{{Service}} ({{MaaS}}): {{A Survey}}},
  shorttitle = {Model-as-a-{{Service}} ({{MaaS}})},
  author = {Gan, Wensheng and Wan, Shicheng and Yu, Philip S.},
  year = {2023},
  month = nov,
  number = {arXiv:2311.05804},
  eprint = {2311.05804},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2311.05804},
  urldate = {2024-02-13},
  abstract = {Due to the increased number of parameters and data in the pre-trained model exceeding a certain level, a foundation model (e.g., a large language model) can significantly improve downstream task performance and emerge with some novel special abilities (e.g., deep learning, complex reasoning, and human alignment) that were not present before. Foundation models are a form of generative artificial intelligence (GenAI), and Model-as-a-Service (MaaS) has emerged as a groundbreaking paradigm that revolutionizes the deployment and utilization of GenAI models. MaaS represents a paradigm shift in how we use AI technologies and provides a scalable and accessible solution for developers and users to leverage pre-trained AI models without the need for extensive infrastructure or expertise in model training. In this paper, the introduction aims to provide a comprehensive overview of MaaS, its significance, and its implications for various industries. We provide a brief review of the development history of "X-as-a-Service" based on cloud computing and present the key technologies involved in MaaS. The development of GenAI models will become more democratized and flourish. We also review recent application studies of MaaS. Finally, we highlight several challenges and future issues in this promising area. MaaS is a new deployment and service paradigm for different AI-based models. We hope this review will inspire future research in the field of MaaS.},
  archiveprefix = {arXiv}
}

@misc{gaoCollabCoderGPTPoweredWorkflow2023,
  title = {{{CollabCoder}}: {{A GPT-Powered Workflow}} for {{Collaborative Qualitative Analysis}}},
  shorttitle = {{{CollabCoder}}},
  author = {Gao, Jie and Guo, Yuchen and Lim, Gionnieve and Zhang, Tianqin and Zhang, Zheng and Li, Toby Jia-Jun and Perrault, Simon Tangi},
  year = {2023},
  month = jul,
  number = {arXiv:2304.07366},
  eprint = {2304.07366},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2304.07366},
  urldate = {2023-09-10},
  abstract = {The Collaborative Qualitative Analysis (CQA) process can be time-consuming and resource-intensive, requiring multiple discussions among team members to refine codes and ideas before reaching a consensus. To address these challenges, we introduce CollabCoder, a system leveraging Large Language Models (LLMs) to support three CQA stages: independent open coding, iterative discussions, and the development of a final codebook. In the independent open coding phase, CollabCoder provides AI-generated code suggestions on demand, and allows users to record coding decision-making information (e.g. keywords and certainty) as support for the process. During the discussion phase, CollabCoder helps to build mutual understanding and productive discussion by sharing coding decision-making information with the team. It also helps to quickly identify agreements and disagreements through quantitative metrics, in order to build a final consensus. During the code grouping phase, CollabCoder employs a top-down approach for primary code group recommendations, reducing the cognitive burden of generating the final codebook. An evaluation involving 16 users confirmed the usability and effectiveness of CollabCoder and offered empirical insights into the LLMs' roles in CQA.},
  archiveprefix = {arXiv},
  langid = {english}
}

@misc{gaoCollabCoderLowerbarrierRigorous2024,
  title = {{{CollabCoder}}: {{A Lower-barrier}}, {{Rigorous Workflow}} for {{Inductive Collaborative Qualitative Analysis}} with {{Large Language Models}}},
  shorttitle = {{{CollabCoder}}},
  author = {Gao, Jie and Guo, Yuchen and Lim, Gionnieve and Zhang, Tianqin and Zhang, Zheng and Li, Toby Jia-Jun and Perrault, Simon Tangi},
  year = {2024},
  month = jan,
  number = {arXiv:2304.07366},
  eprint = {2304.07366},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2304.07366},
  urldate = {2024-09-12},
  abstract = {Collaborative Qualitative Analysis (CQA) can enhance qualitative analysis rigor and depth by incorporating varied viewpoints. Nevertheless, ensuring a rigorous CQA procedure itself can be both demanding and costly. To lower this bar, we take a theoretical perspective to design the CollabCoder workflow, that integrates Large Language Models (LLMs) into key inductive CQA stages: independent open coding, iterative discussions, and final codebook creation. In the open coding phase, CollabCoder offers AI-generated code suggestions and records decision-making data. During discussions, it promotes mutual understanding by sharing this data within the coding team and using quantitative metrics to identify coding (dis)agreements, aiding in consensus-building. In the code grouping stage, CollabCoder provides primary code group suggestions, lightening the cognitive load of finalizing the codebook. A 16-user evaluation confirmed the effectiveness of CollabCoder, demonstrating its advantages over existing software and providing empirical insights into the role of LLMs in the CQA practice.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Human-Computer Interaction}
}

@inproceedings{gaoCollabCoderLowerbarrierRigorous2024a,
  title = {{{CollabCoder}}: {{A Lower-barrier}}, {{Rigorous Workflow}} for {{Inductive Collaborative Qualitative Analysis}} with {{Large Language Models}}},
  shorttitle = {{{CollabCoder}}},
  booktitle = {Proceedings of the {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Gao, Jie and Guo, Yuchen and Lim, Gionnieve and Zhang, Tianqin and Zhang, Zheng and Li, Toby Jia-Jun and Perrault, Simon Tangi},
  year = {2024},
  month = may,
  series = {{{CHI}} '24},
  pages = {1--29},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3613904.3642002},
  urldate = {2024-09-13},
  abstract = {Collaborative Qualitative Analysis (CQA) can enhance qualitative analysis rigor and depth by incorporating varied viewpoints. Nevertheless, ensuring a rigorous CQA procedure itself can be both complex and costly. To lower this bar, we take a theoretical perspective to design a one-stop, end-to-end workflow, CollabCoder, that integrates Large Language Models (LLMs) into key inductive CQA stages. In the independent open coding phase, CollabCoder offers AI-generated code suggestions and records decision-making data. During the iterative discussion phase, it promotes mutual understanding by sharing this data within the coding team and using quantitative metrics to identify coding (dis)agreements, aiding in consensus-building. In the codebook development phase, CollabCoder provides primary code group suggestions, lightening the workload of developing a codebook from scratch. A 16-user evaluation confirmed the effectiveness of CollabCoder, demonstrating its advantages over the existing CQA platform. All related materials of CollabCoder, including code and further extensions, will be included in: https://gaojie058.github.io/CollabCoder/.},
  isbn = {979-8-4007-0330-0}
}

@misc{gaoLargeLanguageModels2023,
  title = {Large {{Language Models Empowered Agent-based Modeling}} and {{Simulation}}: {{A Survey}} and {{Perspectives}}},
  shorttitle = {Large {{Language Models Empowered Agent-based Modeling}} and {{Simulation}}},
  author = {Gao, Chen and Lan, Xiaochong and Li, Nian and Yuan, Yuan and Ding, Jingtao and Zhou, Zhilun and Xu, Fengli and Li, Yong},
  year = {2023},
  month = dec,
  number = {arXiv:2312.11970},
  eprint = {2312.11970},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.11970},
  urldate = {2024-08-08},
  abstract = {Agent-based modeling and simulation has evolved as a powerful tool for modeling complex systems, offering insights into emergent behaviors and interactions among diverse agents. Integrating large language models into agent-based modeling and simulation presents a promising avenue for enhancing simulation capabilities. This paper surveys the landscape of utilizing large language models in agent-based modeling and simulation, examining their challenges and promising future directions. In this survey, since this is an interdisciplinary field, we first introduce the background of agent-based modeling and simulation and large language model-empowered agents. We then discuss the motivation for applying large language models to agent-based simulation and systematically analyze the challenges in environment perception, human alignment, action generation, and evaluation. Most importantly, we provide a comprehensive overview of the recent works of large language model-empowered agent-based modeling and simulation in multiple scenarios, which can be divided into four domains: cyber, physical, social, and hybrid, covering simulation of both real-world and virtual environments. Finally, since this area is new and quickly evolving, we discuss the open problems and promising future directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Multiagent Systems}
}

@misc{gaoRetrievalAugmentedGenerationLarge2024,
  title = {Retrieval-{{Augmented Generation}} for {{Large Language Models}}: {{A Survey}}},
  shorttitle = {Retrieval-{{Augmented Generation}} for {{Large Language Models}}},
  author = {Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Meng and Wang, Haofen},
  year = {2024},
  month = mar,
  number = {arXiv:2312.10997},
  eprint = {2312.10997},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.10997},
  urldate = {2024-06-11},
  abstract = {Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@misc{gaoUIShiftEnhancingVLMbased2025,
  title = {{{UIShift}}: {{Enhancing VLM-based GUI Agents}} through {{Self-supervised Reinforcement Learning}}},
  shorttitle = {{{UIShift}}},
  author = {Gao, Longxi and Zhang, Li and Xu, Mengwei},
  year = {2025},
  month = may,
  number = {arXiv:2505.12493},
  eprint = {2505.12493},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.12493},
  urldate = {2025-06-04},
  abstract = {Training effective Vision Language Models (VLMs) for GUI agents typically relies on supervised fine-tuning (SFT) over large-scale annotated datasets, where the collection process is labor-intensive and error-prone. In this work, we propose a self-supervised inverse dynamics task to enable VLMs to learn from GUI transition pairs by inferring the action that caused that transition. This training task offers two advantages: (1) It enables VLMs to ignore variations unrelated to user actions (e.g., background refreshes, ads) and to focus on true affordances such as buttons and input fields within complex GUIs. (2) The training data can be easily obtained from existing GUI trajectories without requiring human annotation, and it can be easily scaled through automatic offline exploration. Using this training task, we propose UI-shift, a framework for enhancing VLM-based GUI agents through self-supervised reinforcement learning (RL). With only 2K training samples sourced from existing datasets, two VLMs -- Qwen2.5-VL-3B and Qwen2.5-VL-7B -- trained with UI-Shift achieve competitive or superior performance on grounding tasks (ScreenSpot-series benchmarks) and GUI automation tasks (AndroidControl), compared to SFT baselines and GUI-specific models that explicitly elicit reasoning abilities during RL. Our findings suggest a potential direction for enhancing VLMs for GUI agents by leveraging more self-supervised training data in the future.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence}
}

@inproceedings{gebreegziabherPaTATHumanAICollaborative2023,
  title = {{{PaTAT}}: {{Human-AI Collaborative Qualitative Coding}} with {{Explainable Interactive Rule Synthesis}}},
  shorttitle = {{{PaTAT}}},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Gebreegziabher, Simret Araya and Zhang, Zheng and Tang, Xiaohang and Meng, Yihao and Glassman, Elena L. and Li, Toby Jia-Jun},
  year = {2023},
  month = apr,
  series = {{{CHI}} '23},
  pages = {1--19},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3544548.3581352},
  urldate = {2024-09-12},
  abstract = {Over the years, the task of AI-assisted data annotation has seen remarkable advancements. However, a specific type of annotation task, the qualitative coding performed during thematic analysis, has characteristics that make effective human-AI collaboration difficult. Informed by a formative study, we designed PaTAT, a new AI-enabled tool that uses an interactive program synthesis approach to learn flexible and expressive patterns over user-annotated codes in real-time as users annotate data. To accommodate the ambiguous, uncertain, and iterative nature of thematic analysis, the use of user-interpretable patterns allows users to understand and validate what the system has learned, make direct fixes, and easily revise, split, or merge previously annotated codes. This new approach also helps human users to learn data characteristics and form new theories in addition to facilitating the ``learning'' of the AI model. PaTAT's usefulness and effectiveness were evaluated in a lab user study.},
  isbn = {978-1-4503-9421-5}
}

@misc{gehrmannGEMBenchmarkNatural2021,
  title = {The {{GEM Benchmark}}: {{Natural Language Generation}}, Its {{Evaluation}} and {{Metrics}}},
  shorttitle = {The {{GEM Benchmark}}},
  author = {Gehrmann, Sebastian and Adewumi, Tosin and Aggarwal, Karmanya and Ammanamanchi, Pawan Sasanka and Anuoluwapo, Aremu and Bosselut, Antoine and Chandu, Khyathi Raghavi and Clinciu, Miruna and Das, Dipanjan and Dhole, Kaustubh D. and Du, Wanyu and Durmus, Esin and Du{\v s}ek, Ond{\v r}ej and Emezue, Chris and Gangal, Varun and Garbacea, Cristina and Hashimoto, Tatsunori and Hou, Yufang and Jernite, Yacine and Jhamtani, Harsh and Ji, Yangfeng and Jolly, Shailza and Kale, Mihir and Kumar, Dhruv and Ladhak, Faisal and Madaan, Aman and Maddela, Mounica and Mahajan, Khyati and Mahamood, Saad and Majumder, Bodhisattwa Prasad and Martins, Pedro Henrique and {McMillan-Major}, Angelina and Mille, Simon and {van Miltenburg}, Emiel and Nadeem, Moin and Narayan, Shashi and Nikolaev, Vitaly and Niyongabo, Rubungo Andre and Osei, Salomey and Parikh, Ankur and {Perez-Beltrachini}, Laura and Rao, Niranjan Ramesh and Raunak, Vikas and Rodriguez, Juan Diego and Santhanam, Sashank and Sedoc, Jo{\~a}o and Sellam, Thibault and Shaikh, Samira and Shimorina, Anastasia and Cabezudo, Marco Antonio Sobrevilla and Strobelt, Hendrik and Subramani, Nishant and Xu, Wei and Yang, Diyi and Yerukola, Akhila and Zhou, Jiawei},
  year = {2021},
  month = apr,
  number = {arXiv:2102.01672},
  eprint = {2102.01672},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2102.01672},
  urldate = {2022-08-24},
  abstract = {We introduce GEM, a living benchmark for natural language Generation (NLG), its Evaluation, and Metrics. Measuring progress in NLG relies on a constantly evolving ecosystem of automated metrics, datasets, and human evaluation standards. Due to this moving target, new models often still evaluate on divergent anglo-centric corpora with well-established, but flawed, metrics. This disconnect makes it challenging to identify the limitations of current models and opportunities for progress. Addressing this limitation, GEM provides an environment in which models can easily be applied to a wide set of tasks and in which evaluation strategies can be tested. Regular updates to the benchmark will help NLG research become more multilingual and evolve the challenge alongside models. This paper serves as the description of the data for which we are organizing a shared task at our ACL 2021 Workshop and to which we invite the entire NLG community to participate.},
  archiveprefix = {arXiv}
}

@article{gelenbeSimulationLearningAgents2001,
  title = {Simulation with Learning Agents},
  author = {Gelenbe, E. and Seref, E. and Xu, Z.},
  year = {2001},
  month = feb,
  journal = {Proceedings of the IEEE},
  volume = {89},
  number = {2},
  pages = {148--157},
  issn = {1558-2256},
  doi = {10.1109/5.910851},
  urldate = {2024-08-27},
  abstract = {We propose that learning agents (LAs) be incorporated into simulation environments in order to model the adaptive behavior of humans. These LAs adapt to specific circumstances and events during the simulation run. They would select tasks to be accomplished among a given set of tasks as the simulation progresses, or synthesize tasks for themselves based on their observations of the environment and on information they may receive from other agents. We investigate an approach in which agents are assigned goals when the simulation starts and then pursue these goals autonomously and adaptively. During the simulation, agents progressively improve their ability to accomplish their goals effectively and safely. Agents learn from their own observations and from the experience of other agents with whom they exchange information. Each LA starts with a given representation of the simulation environment from which it progressively constructs its own internal representation and uses it to make decisions. The paper describes how learning neural networks can support this approach and shows that goal based learning may be used effectively used in this context. An example simulation is presented in which agents represent manned vehicles; they are assigned the goal of traversing a dangerous metropolitan grid safely and rapidly using goal based reinforcement learning with neural networks and compared to three other algorithms.},
  keywords = {Computational modeling,Computer science,Discrete event simulation,Humans,Neural networks,Random number generation,Stress,Vehicle dynamics,Vehicle safety}
}

@misc{GeneralInductiveApproach,
  title = {A {{General Inductive Approach}} for {{Analyzing Qualitative Evaluation Data}} - {{David R}}. {{Thomas}}, 2006},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1098214005283748?casa_token=BNEoBAwg3o4AAAAA:r4EoiQTqNkV6vq1-LxRLAqrvlYM6pSjupfKrSyrXClbC9C_QznSMamSlSMatOko6vTeKdUIzenQ&casa_token=IRI1tEDMXPgAAAAA:xVlA1jY3M4bUWCFl3gTJgDT-1f4vuyZD6ihzWQmAhAjkLkL5SzMAHRf78_nAThuR2Th8gOYulcw},
  urldate = {2024-09-13}
}

@inproceedings{gengGrammarConstrainedDecodingStructured2023,
  title = {Grammar-{{Constrained Decoding}} for {{Structured NLP Tasks}} without {{Finetuning}}},
  booktitle = {The 2023 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Geng, Saibo and Josifoski, Martin and Peyrard, Maxime and West, Robert},
  year = {2023},
  month = dec,
  url = {https://openreview.net/forum?id=KkHY1WGDII},
  urldate = {2024-09-24},
  abstract = {Despite their impressive performance, large language models (LMs) still struggle with reliably generating complex output structures when not finetuned to follow the required output format exactly. To address this issue, grammar-constrained decoding (GCD) can be used to control the generation of LMs, guaranteeing that the output follows a given structure. Most existing GCD methods are, however, limited to specific tasks, such as parsing or code generation. In this work, we demonstrate that formal grammars can describe the output space for a much wider range of tasks and argue that GCD can serve as a unified framework for structured NLP tasks in general. For increased flexibility, we introduce input-dependent grammars, which allow the grammar to depend on the input and thus enable the generation of different output structures for different inputs. We then empirically demonstrate the power and flexibility of GCD-enhanced LMs on (1) information extraction, (2) entity disambiguation, and (3) constituency parsing. Our results indicate that grammar-constrained LMs substantially outperform unconstrained LMs or even beat task-specific finetuned models. Grammar constraints thus hold great promise for harnessing off-the-shelf LMs for a wide range of structured NLP tasks, especially where training data is scarce or finetuning is expensive. Code and data: https://github.com/epfl-dlab/GCD.},
  langid = {english}
}

@inproceedings{geovanan.macarioAnnotatingDataSupport2010,
  title = {Annotating Data to Support Decision-Making: A Case Study},
  shorttitle = {Annotating Data to Support Decision-Making},
  booktitle = {Proceedings of the 6th {{Workshop}} on {{Geographic Information Retrieval}}},
  author = {Geovana N. Mac{\'a}rio, Carla and {dos Santos}, Jefersson A. and Medeiros, Claudia Bauzer and {da S. Torres}, Ricardo},
  year = {2010},
  month = feb,
  series = {{{GIR}} '10},
  pages = {1--7},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1722080.1722106},
  urldate = {2024-03-27},
  abstract = {Georeferenced data are a key factor in many decision-making systems. However, their interpretation is user and context dependent so that, for each situation, data analysts have to interpret them, a time-consuming task. One approach to alleviate this task, is the use of semantic annotations to store the produced information. Annotating data is however hard to perform and prone to errors, especially when executed manually. This difficulty increases with the amount of data to annotate. Moreover, annotation requires multi-disciplinary collaboration of researchers, with access to heterogeneous and distributed data sources and scientific computations. This paper illustrates our solution to approach this problem by means of a case study in agriculture. It shows how our implementation of a framework to automate the annotation of geospatial data can be used to process real data from remote sensing images and other official Brazilian data sources.},
  isbn = {978-1-60558-826-1}
}

@article{gilardiChatGPTOutperformsCrowdWorkers2023,
  title = {{{ChatGPT Outperforms Crowd-Workers}} for {{Text-Annotation Tasks}}},
  author = {Gilardi, Fabrizio and Alizadeh, Meysam and Kubli, Ma{\"e}l},
  year = {2023},
  month = jul,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {120},
  number = {30},
  eprint = {2303.15056},
  primaryclass = {cs},
  pages = {e2305016120},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2305016120},
  urldate = {2023-10-26},
  abstract = {Many NLP applications require manual data annotations for a variety of tasks, notably to train classifiers or evaluate the performance of unsupervised models. Depending on the size and degree of complexity, the tasks may be conducted by crowd-workers on platforms such as MTurk as well as trained annotators, such as research assistants. Using a sample of 2,382 tweets, we demonstrate that ChatGPT outperforms crowd-workers for several annotation tasks, including relevance, stance, topics, and frames detection. Specifically, the zero-shot accuracy of ChatGPT exceeds that of crowd-workers for four out of five tasks, while ChatGPT's intercoder agreement exceeds that of both crowd-workers and trained annotators for all tasks. Moreover, the per-annotation cost of ChatGPT is less than \$0.003 -- about twenty times cheaper than MTurk. These results show the potential of large language models to drastically increase the efficiency of text classification.},
  archiveprefix = {arXiv}
}

@incollection{gilbertToolsAnalyzingQualitative2014,
  title = {Tools for {{Analyzing Qualitative Data}}: {{The History}} and {{Relevance}} of {{Qualitative Data Analysis Software}}},
  shorttitle = {Tools for {{Analyzing Qualitative Data}}},
  booktitle = {Handbook of {{Research}} on {{Educational Communications}} and {{Technology}}},
  author = {Gilbert, Linda S. and Jackson, Kristi and {di Gregorio}, Silvana},
  editor = {Spector, J. Michael and Merrill, M. David and Elen, Jan and Bishop, M. J.},
  year = {2014},
  pages = {221--236},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-1-4614-3185-5\_18},
  urldate = {2024-09-13},
  abstract = {The most common question from novices regarding the use of software in qualitative research is ``which program should I use?'' when they would be better served by asking ``what analytical tasks will I be engaged in, and what are the different ways I can leverage technology to do them well?'' In this chapter, we first provide an overview of tasks involved in analyzing qualitative data, with a focus on increasingly complex projects, before we turn to the software meant to support these tasks. One genre of software, known as Qualitative Data Analysis Software (QDAS or QDA software), is specifically designed to support qualitative research, as opposed to tools primarily used for the collection of data (such as audio or video recorders), or presentation of findings (such as presentation or modeling software). We briefly review the historical development of QDA software---including associated methodological questions and issues---before identifying the increasingly diverse array of expected features and functions in most of the current software programs. We then summarize the ``user experience'' literature and subsequently discuss the boundaries between cadres of qualitative researchers who do use software, and those who do not. Finally, we address potential directions as these programs are being influenced by Web 2.0 developments.},
  isbn = {978-1-4614-3185-5},
  langid = {english}
}

@inproceedings{glorotUnderstandingDifficultyTraining2010,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  booktitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Glorot, Xavier and Bengio, Yoshua},
  year = {2010},
  month = mar,
  pages = {249--256},
  publisher = {{JMLR Workshop and Conference Proceedings}},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v9/glorot10a.html},
  urldate = {2023-09-18},
  abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
  langid = {english}
}

@inproceedings{gordonJuryLearningIntegrating2022,
  title = {Jury {{Learning}}: {{Integrating Dissenting Voices}} into {{Machine Learning Models}}},
  shorttitle = {Jury {{Learning}}},
  booktitle = {{{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Gordon, Mitchell L. and Lam, Michelle S. and Park, Joon Sung and Patel, Kayur and Hancock, Jeff and Hashimoto, Tatsunori and Bernstein, Michael S.},
  year = {2022},
  month = apr,
  pages = {1--19},
  publisher = {ACM},
  address = {New Orleans LA USA},
  doi = {10.1145/3491102.3502004},
  urldate = {2023-10-16},
  isbn = {978-1-4503-9157-3},
  langid = {english}
}

@misc{goswamiMOMENTFamilyOpen2024,
  title = {{{MOMENT}}: {{A Family}} of {{Open Time-series Foundation Models}}},
  shorttitle = {{{MOMENT}}},
  author = {Goswami, Mononito and Szafer, Konrad and Choudhry, Arjun and Cai, Yifu and Li, Shuo and Dubrawski, Artur},
  year = {2024},
  month = feb,
  number = {arXiv:2402.03885},
  eprint = {2402.03885},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2402.03885},
  urldate = {2024-02-15},
  abstract = {We introduce MOMENT, a family of open-source foundation models for general-purpose time-series analysis. Pre-training large models on time-series data is challenging due to (1) the absence of a large and cohesive public time-series repository, and (2) diverse time-series characteristics which make multi-dataset training onerous. Additionally, (3) experimental benchmarks to evaluate these models, especially in scenarios with limited resources, time, and supervision, are still in their nascent stages. To address these challenges, we compile a large and diverse collection of public time-series, called the Time-series Pile, and systematically tackle time-series-specific challenges to unlock large-scale multi-dataset pre-training. Finally, we build on recent work to design a benchmark to evaluate time-series foundation models on diverse tasks and datasets in limited supervision settings. Experiments on this benchmark demonstrate the effectiveness of our pre-trained models with minimal data and task-specific fine-tuning. Finally, we present several interesting empirical observations about large pre-trained time-series models. Our code is available anonymously at anonymous.4open.science/r/BETT-773F/.},
  archiveprefix = {arXiv}
}

@misc{gouNavigatingDigitalWorld2024,
  title = {Navigating the {{Digital World}} as {{Humans Do}}: {{Universal Visual Grounding}} for {{GUI Agents}}},
  shorttitle = {Navigating the {{Digital World}} as {{Humans Do}}},
  author = {Gou, Boyu and Wang, Ruohan and Zheng, Boyuan and Xie, Yanan and Chang, Cheng and Shu, Yiheng and Sun, Huan and Su, Yu},
  year = {2024},
  month = oct,
  number = {arXiv:2410.05243},
  eprint = {2410.05243},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2410.05243},
  urldate = {2024-11-01},
  abstract = {Multimodal large language models (MLLMs) are transforming the capabilities of graphical user interface (GUI) agents, facilitating their transition from controlled simulations to complex, real-world applications across various platforms. However, the effectiveness of these agents hinges on the robustness of their grounding capability. Current GUI agents predominantly utilize text-based representations such as HTML or accessibility trees, which, despite their utility, often introduce noise, incompleteness, and increased computational overhead. In this paper, we advocate a human-like embodiment for GUI agents that perceive the environment entirely visually and directly take pixel-level operations on the GUI. The key is visual grounding models that can accurately map diverse referring expressions of GUI elements to their coordinates on the GUI across different platforms. We show that a simple recipe, which includes web-based synthetic data and slight adaptation of the LLaVA architecture, is surprisingly effective for training such visual grounding models. We collect the largest dataset for GUI visual grounding so far, containing 10M GUI elements and their referring expressions over 1.3M screenshots, and use it to train UGround, a strong universal visual grounding model for GUI agents. Empirical results on six benchmarks spanning three categories (grounding, offline agent, and online agent) show that 1) UGround substantially outperforms existing visual grounding models for GUI agents, by up to 20\% absolute, and 2) agents with UGround outperform state-of-the-art agents, despite the fact that existing agents use additional text-based input while ours only uses visual perception. These results provide strong support for the feasibility and promises of GUI agents that navigate the digital world as humans do.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition}
}

@misc{goyalThinkYouSpeak2023,
  title = {Think before You Speak: {{Training Language Models With Pause Tokens}}},
  shorttitle = {Think before You Speak},
  author = {Goyal, Sachin and Ji, Ziwei and Rawat, Ankit Singh and Menon, Aditya Krishna and Kumar, Sanjiv and Nagarajan, Vaishnavh},
  year = {2023},
  month = oct,
  number = {arXiv:2310.02226},
  eprint = {2310.02226},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2310.02226},
  urldate = {2023-10-05},
  abstract = {Language models generate responses by producing a series of tokens in immediate succession: the \$(K+1){\textasciicircum}\{th\}\$ token is an outcome of manipulating \$K\$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, \$K+10\$ hidden vectors, before it outputs the \$(K+1){\textasciicircum}\{th\}\$ token? We operationalize this idea by performing training and inference on language models with a (learnable) \${\textbackslash}textit\{pause\}\$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate \${\textbackslash}textit\{pause-training\}\$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of \$18{\textbackslash}\%\$ EM score on the QA task of SQuAD, \$8{\textbackslash}\%\$ on CommonSenseQA and \$1{\textbackslash}\%\$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.},
  archiveprefix = {arXiv}
}

@inproceedings{GraphCareEnhancingHealthcare2023,
  title = {{{GraphCare}}: {{Enhancing Healthcare Predictions}} with {{Personalized Knowledge Graphs}}},
  shorttitle = {{{GraphCare}}},
  booktitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  year = {2023},
  month = oct,
  url = {https://openreview.net/forum?id=tVTN7Zs0ml},
  urldate = {2024-01-11},
  abstract = {Clinical predictive models often rely on patients' electronic health records (EHR), but integrating medical knowledge to enhance predictions and decision-making is challenging. This is because personalized predictions require personalized knowledge graphs (KGs), which are difficult to generate from patient EHR data. To address this, we propose GraphCare, an open-world framework that uses external KGs to improve EHR-based predictions. Our method extracts knowledge from large language models (LLMs) and external biomedical KGs to build patient-specific KGs, which are then used to train our proposed Bi-attention AugmenTed (BAT) graph neural network (GNN) for healthcare predictions. On two public datasets, MIMIC-III and MIMIC-IV, GraphCare surpasses baselines in four vital healthcare prediction tasks: mortality, readmission, length of stay (LOS), and drug recommendation. On MIMIC-III, it boosts AUROC by 17.6\% and 6.6\% for mortality and readmission, and F1-score by 7.9\% and 10.8\% for LOS and drug recommendation, respectively. Notably, GraphCare demonstrates a substantial edge in scenarios with limited data availability. Our findings highlight the potential of using external KGs in healthcare prediction tasks and demonstrate the promise of GraphCare in generating personalized KGs for promoting personalized medicine.},
  langid = {english}
}

@misc{Greenland,
  title = {Greenland},
  url = {https://console.harmony.a2z.com/greenland/console?ctx=6db366b3-8d1f-5078-6708-dc73d08504a1&hero=all},
  urldate = {2025-02-26}
}

@inproceedings{gruverLargeLanguageModels2023,
  title = {Large {{Language Models Are Zero-Shot Time Series Forecasters}}},
  booktitle = {Thirty-Seventh {{Conference}} on {{Neural Information Processing Systems}}},
  author = {Gruver, Nate and Finzi, Marc Anton and Qiu, Shikai and Wilson, Andrew Gordon},
  year = {2023},
  month = nov,
  url = {https://openreview.net/forum?id=md68e8iZK1},
  urldate = {2024-01-10},
  abstract = {By encoding time series as a string of numerical digits, we can frame time series forecasting as next-token prediction in text. Developing this approach, we find that large language models (LLMs) such as GPT-3 and LLaMA-2 can surprisingly zero-shot extrapolate time series at a level comparable to or exceeding the performance of purpose-built time series models trained on the downstream tasks. To facilitate this performance, we propose procedures for effectively tokenizing time series data and converting discrete distributions over tokens into highly flexible densities over continuous values. We argue the success of LLMs for time series stems from their ability to naturally represent multimodal distributions, in conjunction with biases for simplicity, and repetition, which align with the salient features in many time series, such as repeated seasonal trends. We also show how LLMs can naturally handle missing data without imputation through non-numerical text, accommodate textual side information, and answer questions to help explain predictions. While we find that increasing model size generally improves performance on time series, we show GPT-4 can perform worse than GPT-3 because of how it tokenizes numbers, and poor uncertainty calibration, which is likely the result of alignment interventions such as RLHF.},
  langid = {english}
}

@misc{guanLongTextGeneration2021,
  title = {Long {{Text Generation}} by {{Modeling Sentence-Level}} and {{Discourse-Level Coherence}}},
  author = {Guan, Jian and Mao, Xiaoxi and Fan, Changjie and Liu, Zitao and Ding, Wenbiao and Huang, Minlie},
  year = {2021},
  month = may,
  number = {arXiv:2105.08963},
  eprint = {2105.08963},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2105.08963},
  urldate = {2022-11-23},
  abstract = {Generating long and coherent text is an important but challenging task, particularly for open-ended language generation tasks such as story generation. Despite the success in modeling intra-sentence coherence, existing generation models (e.g., BART) still struggle to maintain a coherent event sequence throughout the generated text. We conjecture that this is because of the difficulty for the decoder to capture the high-level semantics and discourse structures in the context beyond token-level co-occurrence. In this paper, we propose a long text generation model, which can represent the prefix sentences at sentence level and discourse level in the decoding process. To this end, we propose two pretraining objectives to learn the representations by predicting inter-sentence semantic similarity and distinguishing between normal and shuffled sentence orders. Extensive experiments show that our model can generate more coherent texts than state-of-the-art baselines.},
  archiveprefix = {arXiv}
}

@inproceedings{guanLongTextGeneration2021a,
  title = {Long {{Text Generation}} by {{Modeling Sentence-Level}} and {{Discourse-Level Coherence}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Guan, Jian and Mao, Xiaoxi and Fan, Changjie and Liu, Zitao and Ding, Wenbiao and Huang, Minlie},
  year = {2021},
  month = aug,
  pages = {6379--6393},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.acl-long.499},
  urldate = {2022-11-15},
  abstract = {Generating long and coherent text is an important but challenging task, particularly for open-ended language generation tasks such as story generation. Despite the success in modeling intra-sentence coherence, existing generation models (e.g., BART) still struggle to maintain a coherent event sequence throughout the generated text. We conjecture that this is because of the difficulty for the decoder to capture the high-level semantics and discourse structures in the context beyond token-level co-occurrence. In this paper, we propose a long text generation model, which can represent the prefix sentences at sentence level and discourse level in the decoding process. To this end, we propose two pretraining objectives to learn the representations by predicting inter-sentence semantic similarity and distinguishing between normal and shuffled sentence orders. Extensive experiments show that our model can generate more coherent texts than state-of-the-art baselines.}
}

@article{guDomainSpecificLanguageModel2021,
  title = {Domain-{{Specific Language Model Pretraining}} for {{Biomedical Natural Language Processing}}},
  author = {Gu, Yu and Tinn, Robert and Cheng, Hao and Lucas, Michael and Usuyama, Naoto and Liu, Xiaodong and Naumann, Tristan and Gao, Jianfeng and Poon, Hoifung},
  year = {2021},
  month = oct,
  journal = {ACM Transactions on Computing for Healthcare},
  volume = {3},
  number = {1},
  pages = {2:1--2:23},
  issn = {2691-1957},
  doi = {10.1145/3458754},
  urldate = {2022-04-12},
  abstract = {Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. In this article, we challenge this assumption by showing that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models. To facilitate this investigation, we compile a comprehensive biomedical NLP benchmark from publicly available datasets. Our experiments show that domain-specific pretraining serves as a solid foundation for a wide range of biomedical NLP tasks, leading to new state-of-the-art results across the board. Further, in conducting a thorough evaluation of modeling choices, both for pretraining and task-specific fine-tuning, we discover that some common practices are unnecessary with BERT models, such as using complex tagging schemes in named entity recognition. To help accelerate research in biomedical NLP, we have released our state-of-the-art pretrained and task-specific models for the community, and created a leaderboard featuring our BLURB benchmark (short for Biomedical Language Understanding \& Reasoning Benchmark) at https://aka.ms/BLURB.},
  keywords = {Biomedical,domain-specific pretraining,NLP}
}

@misc{GuidanceaiGuidance2024,
  title = {Guidance-Ai/Guidance},
  year = {2024},
  month = sep,
  url = {https://github.com/guidance-ai/guidance},
  urldate = {2024-09-24},
  abstract = {A guidance language for controlling large language models.},
  copyright = {MIT},
  howpublished = {guidance-ai}
}

@article{GuJiYuShenJingWangLuoDeJiQiYueDuLiJieZongShu2020,
  title = {{}},
  author = {,  and ,  and ,  and ,  and , },
  year = {2020},
  journal = {},
  volume = {31},
  number = {7},
  pages = {2095--2126},
  issn = {1000-9825},
  doi = {10.13328/j.cnki.jos.006048},
  abstract = {,.,.,,,.:,;,,,BERT;,,,;.},
  langid = {chinese},
  annotation = {26 citations(CNKI)[3-28-2022]{$<$}, EI, CSCD{$>$}}
}

@inproceedings{guoCalibrationModernNeural2017a,
  title = {On Calibration of Modern Neural Networks},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}} - {{Volume}} 70},
  author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
  year = {2017},
  month = aug,
  series = {{{ICML}}'17},
  pages = {1321--1330},
  publisher = {JMLR.org},
  address = {Sydney, NSW, Australia},
  urldate = {2024-01-02},
  abstract = {Confidence calibration - the problem of predicting probability estimates representative of the true correctness likelihood - is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling - a single-parameter variant of Platt Scaling - is surprisingly effective at calibrating predictions.}
}

@misc{guoConditionalTextGeneration2020,
  title = {Conditional {{Text Generation}} for {{Harmonious Human-Machine Interaction}}},
  author = {Guo, Bin and Wang, Hao and Ding, Yasan and Wu, Wei and Hao, Shaoyang and Sun, Yueqi and Yu, Zhiwen},
  year = {2020},
  month = dec,
  number = {arXiv:1909.03409},
  eprint = {1909.03409},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/1909.03409},
  urldate = {2022-08-24},
  abstract = {In recent years, with the development of deep learning, text generation technology has undergone great changes and provided many kinds of services for human beings, such as restaurant reservation and daily communication. The automatically generated text is becoming more and more fluent so researchers begin to consider more anthropomorphic text generation technology, that is the conditional text generation, including emotional text generation, personalized text generation, and so on. Conditional Text Generation (CTG) has thus become a research hotspot. As a promising research field, we find that many efforts have been paid to exploring it. Therefore, we aim to give a comprehensive review of the new research trends of CTG. We first summary several key techniques and illustrate the technical evolution route in the field of neural text generation, based on the concept model of CTG. We further make an investigation of existing CTG fields and propose several general learning models for CTG. Finally, we discuss the open issues and promising research directions of CTG.},
  archiveprefix = {arXiv}
}

@misc{guoContinuousTrainingFinetuning2023,
  title = {Continuous {{Training}} and {{Fine-tuning}} for {{Domain-Specific Language Models}} in {{Medical Question Answering}}},
  author = {Guo, Zhen and Hua, Yining},
  year = {2023},
  month = oct,
  number = {arXiv:2311.00204},
  eprint = {2311.00204},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2311.00204},
  urldate = {2024-01-20},
  abstract = {Large language models exhibit promising general capabilities but often lack specialized knowledge for domain-specific tasks. Developing domain experts from a base model enables a range of applications without prohibitive training costs. This work demonstrates a method using continuous training and instruction fine-tuning to rapidly adapt Llama 2 base models to the Chinese medical domain. We first conduct continuous training on 1B tokens from Chinese medical references to teach relevant vocabulary and knowledge. The models are then fine-tuned on 54K examples sourced from the Chinese National Medical Licensing Examination. Experiments on Chinese medical data confirm the effectiveness of this approach, producing a model comparable to GPT-3.5-turbo while using way less computational resource. The resulting domain-specific model could be useful for various Chinese medical applications. More broadly, this provides a template for domain-specific training of large language models in areas where pre-trained models lack the required expertise, such as law, science, and engineering.},
  archiveprefix = {arXiv}
}

@misc{guoImprovingSmallLanguage2023,
  title = {Improving {{Small Language Models}} on {{PubMedQA}} via {{Generative Data Augmentation}}},
  author = {Guo, Zhen and Wang, Peiqi and Wang, Yanwei and Yu, Shangdi},
  year = {2023},
  month = aug,
  number = {arXiv:2305.07804},
  eprint = {2305.07804},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2305.07804},
  urldate = {2024-01-20},
  abstract = {Large Language Models (LLMs) have made remarkable advancements in the field of natural language processing. However, their increasing size poses challenges in terms of computational cost. On the other hand, Small Language Models (SLMs) are known for their efficiency, but they often struggle with limited capacity and training data, especially in specific domains. In this paper, we introduce a novel method aimed at improving SLMs in the medical domain using LLM-based generative data augmentation. The objective of our approach is to develop more efficient and capable models that are specifically tailored for specialized applications. Through experiments conducted on the PubMedQA dataset, we demonstrate the effectiveness of LLMs in refining and diversifying existing question-answer pairs. This refinement process leads to improved performance in a significantly smaller model after fine-tuning. Notably, our best SLM, with under 1.6 billion parameters, outperforms the few-shot GPT-4 on the PubMedQA dataset. Our code and generated data are publicly available to facilitate further explorations.},
  archiveprefix = {arXiv}
}

@misc{guoIntegratingLargeLanguage2024,
  title = {Integrating {{Large Language Models}} with {{Graphical Session-Based Recommendation}}},
  author = {Guo, Naicheng and Cheng, Hongwei and Liang, Qianqiao and Chen, Linxun and Han, Bing},
  year = {2024},
  month = feb,
  number = {arXiv:2402.16539},
  eprint = {2402.16539},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.16539},
  urldate = {2024-09-20},
  abstract = {With the rapid development of Large Language Models (LLMs), various explorations have arisen to utilize LLMs capability of context understanding on recommender systems. While pioneering strategies have primarily transformed traditional recommendation tasks into challenges of natural language generation, there has been a relative scarcity of exploration in the domain of session-based recommendation (SBR) due to its specificity. SBR has been primarily dominated by Graph Neural Networks, which have achieved many successful outcomes due to their ability to capture both the implicit and explicit relationships between adjacent behaviors. The structural nature of graphs contrasts with the essence of natural language, posing a significant adaptation gap for LLMs. In this paper, we introduce large language models with graphical Session-Based recommendation, named LLMGR, an effective framework that bridges the aforementioned gap by harmoniously integrating LLMs with Graph Neural Networks (GNNs) for SBR tasks. This integration seeks to leverage the complementary strengths of LLMs in natural language understanding and GNNs in relational data processing, leading to a more powerful session-based recommender system that can understand and recommend items within a session. Moreover, to endow the LLM with the capability to empower SBR tasks, we design a series of prompts for both auxiliary and major instruction tuning tasks. These prompts are crafted to assist the LLM in understanding graph-structured data and align textual information with nodes, effectively translating nuanced user interactions into a format that can be understood and utilized by LLM architectures. Extensive experiments on three real-world datasets demonstrate that LLMGR outperforms several competitive baselines, indicating its effectiveness in enhancing SBR tasks and its potential as a research direction for future exploration.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning}
}

@misc{guoLongT5EfficientTextToText2022,
  title = {{{LongT5}}: {{Efficient Text-To-Text Transformer}} for {{Long Sequences}}},
  shorttitle = {{{LongT5}}},
  author = {Guo, Mandy and Ainslie, Joshua and Uthus, David and Ontanon, Santiago and Ni, Jianmo and Sung, Yun-Hsuan and Yang, Yinfei},
  year = {2022},
  month = may,
  number = {arXiv:2112.07916},
  eprint = {2112.07916},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.07916},
  urldate = {2023-12-15},
  abstract = {Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the performance of Transformer-based neural models. In this paper, we present a new model, called LongT5, with which we explore the effects of scaling both the input length and model size at the same time. Specifically, we integrated attention ideas from long-input transformers (ETC), and adopted pre-training strategies from summarization pre-training (PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call \{{\textbackslash}em Transient Global\} (TGlobal), which mimics ETC's local/global attention mechanism, but without requiring additional side-inputs. We are able to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on question answering tasks.},
  archiveprefix = {arXiv}
}

@article{guoModelingUsersSearch2017,
  title = {Modeling Users' Search Sessions for High Utility Query Recommendation},
  author = {Guo, Jiafeng and Zhu, Xiaofei and Lan, Yanyan and Cheng, Xueqi},
  year = {2017},
  month = feb,
  journal = {Information Retrieval Journal},
  volume = {20},
  number = {1},
  pages = {4--24},
  issn = {1573-7659},
  doi = {10.1007/s10791-016-9287-1},
  urldate = {2024-09-19},
  abstract = {Query recommendation has long been considered a key feature of search engines, which can improve users' search experience by providing useful query suggestions for their search tasks. Most existing approaches on query recommendation aim to recommend relevant queries, i.e., alternative queries similar to a user's initial query. However, the ultimate goal of query recommendation is to assist users to reformulate queries so that they can accomplish their search task successfully and quickly. Only considering relevance in query recommendation is apparently not directly toward this goal. In this paper, we argue that it is more important to directly recommend queries with high utility, i.e., queries that can better satisfy users' information needs. For this purpose, we attempt to infer query utility from users' sequential search behaviors recorded in their search sessions. Specifically, we propose a dynamic Bayesian network, referred as Query Utility Model (QUM), to capture query utility by simultaneously modeling users' reformulation and click behaviors. We then recommend queries with high utility to help users better accomplish their search tasks. We empirically evaluated the performance of our approach on a publicly released query log by comparing with the state-of-the-art methods. The experimental results show that, by recommending high utility queries, our approach is far more effective in helping users find relevant search results and thus satisfying their information needs.},
  langid = {english},
  keywords = {Artificial Intelligence,Dynamic Bayesian network,Query recommendation,Query utility,Search behavior}
}

@misc{guoWhatCanLarge2023,
  title = {What Can {{Large Language Models}} Do in Chemistry? {{A}} Comprehensive Benchmark on Eight Tasks},
  shorttitle = {What Can {{Large Language Models}} Do in Chemistry?},
  author = {Guo, Taicheng and Guo, Kehan and Nan, Bozhao and Liang, Zhenwen and Guo, Zhichun and Chawla, Nitesh V. and Wiest, Olaf and Zhang, Xiangliang},
  year = {2023},
  month = dec,
  number = {arXiv:2305.18365},
  eprint = {2305.18365},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.18365},
  urldate = {2024-01-14},
  abstract = {Large Language Models (LLMs) with strong abilities in natural language processing tasks have emerged and have been applied in various kinds of areas such as science, finance and software engineering. However, the capability of LLMs to advance the field of chemistry remains unclear. In this paper, rather than pursuing state-of-the-art performance, we aim to evaluate capabilities of LLMs in a wide range of tasks across the chemistry domain. We identify three key chemistry-related capabilities including understanding, reasoning and explaining to explore in LLMs and establish a benchmark containing eight chemistry tasks. Our analysis draws on widely recognized datasets facilitating a broad exploration of the capacities of LLMs within the context of practical chemistry. Five LLMs (GPT-4, GPT-3.5, Davinci-003, Llama and Galactica) are evaluated for each chemistry task in zero-shot and few-shot in-context learning settings with carefully selected demonstration examples and specially crafted prompts. Our investigation found that GPT-4 outperformed other models and LLMs exhibit different competitive levels in eight chemistry tasks. In addition to the key findings from the comprehensive benchmark analysis, our work provides insights into the limitation of current LLMs and the impact of in-context learning settings on LLMs' performance across various chemistry tasks. The code and datasets used in this study are available at https://github.com/ChemFoundationModels/ChemLLMBench.},
  archiveprefix = {arXiv}
}

@misc{guptaSelectiveSelfRehearsalFineTuning2024,
  title = {Selective {{Self-Rehearsal}}: {{A Fine-Tuning Approach}} to {{Improve Generalization}} in {{Large Language Models}}},
  shorttitle = {Selective {{Self-Rehearsal}}},
  author = {Gupta, Sonam and Nandwani, Yatin and Yehudai, Asaf and Mishra, Mayank and Pandey, Gaurav and Raghu, Dinesh and Joshi, Sachindra},
  year = {2024},
  month = sep,
  number = {arXiv:2409.04787},
  eprint = {2409.04787},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2409.04787},
  urldate = {2024-09-10},
  abstract = {Fine-tuning Large Language Models (LLMs) on specific datasets is a common practice to improve performance on target tasks. However, this performance gain often leads to overfitting, where the model becomes too specialized in either the task or the characteristics of the training data, resulting in a loss of generalization. This paper introduces Selective Self-Rehearsal (SSR), a fine-tuning approach that achieves performance comparable to the standard supervised fine-tuning (SFT) while improving generalization. SSR leverages the fact that there can be multiple valid responses to a query. By utilizing the model's correct responses, SSR reduces model specialization during the fine-tuning stage. SSR first identifies the correct model responses from the training set by deploying an appropriate LLM as a judge. Then, it fine-tunes the model using the correct model responses and the gold response for the remaining samples. The effectiveness of SSR is demonstrated through experiments on the task of identifying unanswerable queries across various datasets. The results show that standard SFT can lead to an average performance drop of up to 16.7\% on multiple benchmarks, such as MMLU and TruthfulQA. In contrast, SSR results in close to 2\% drop on average, indicating better generalization capabilities compared to standard SFT.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{gurcanLLMAugmentedAgentBasedModelling2024,
  title = {{{LLM-Augmented Agent-Based Modelling}} for {{Social Simulations}}: {{Challenges}} and {{Opportunities}}},
  shorttitle = {{{LLM-Augmented Agent-Based Modelling}} for {{Social Simulations}}},
  author = {Gurcan, Onder},
  year = {2024},
  month = may,
  number = {arXiv:2405.06700},
  eprint = {2405.06700},
  primaryclass = {physics},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.06700},
  urldate = {2024-09-05},
  abstract = {As large language models (LLMs) continue to make significant strides, their better integration into agent-based simulations offers a transformational potential for understanding complex social systems. However, such integration is not trivial and poses numerous challenges. Based on this observation, in this paper, we explore architectures and methods to systematically develop LLM-augmented social simulations and discuss potential research directions in this field. We conclude that integrating LLMs with agent-based simulations offers a powerful toolset for researchers and scientists, allowing for more nuanced, realistic, and comprehensive models of complex systems and human behaviours.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Physics - Physics and Society}
}

@article{gureckisPsiTurkOpensourceFramework2016,
  title = {{{psiTurk}}: {{An}} Open-Source Framework for Conducting Replicable Behavioral Experiments Online},
  shorttitle = {{{psiTurk}}},
  author = {Gureckis, Todd M. and Martin, Jay and McDonnell, John and Rich, Alexander S. and Markant, Doug and Coenen, Anna and Halpern, David and Hamrick, Jessica B. and Chan, Patricia},
  year = {2016},
  month = sep,
  journal = {Behavior Research Methods},
  volume = {48},
  number = {3},
  pages = {829--842},
  issn = {1554-3528},
  doi = {10.3758/s13428-015-0642-8},
  urldate = {2024-09-13},
  abstract = {Online data collection has begun to revolutionize the behavioral sciences. However, conducting carefully controlled behavioral experiments online introduces a number of new of technical and scientific challenges. The project described in this paper, psiTurk, is an open-source platform which helps researchers develop experiment designs which can be conducted over the Internet. The tool primarily interfaces with Amazon's Mechanical Turk, a popular crowd-sourcing labor market. This paper describes the basic architecture of the system and introduces new users to the overall goals. psiTurk aims to reduce the technical hurdles for researchers developing online experiments while improving the transparency and collaborative nature of the behavioral sciences.},
  langid = {english},
  keywords = {Amazon Mechanical Turk,Artificial Intelligence,Internet experiments,Online experiments,Open science,psiTurk}
}

@inproceedings{gurRealWorldWebAgentPlanning2023,
  title = {A {{Real-World WebAgent}} with {{Planning}}, {{Long Context Understanding}}, and {{Program Synthesis}}},
  booktitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  author = {Gur, Izzeddin and Furuta, Hiroki and Huang, Austin V. and Safdari, Mustafa and Matsuo, Yutaka and Eck, Douglas and Faust, Aleksandra},
  year = {2023},
  month = oct,
  url = {https://openreview.net/forum?id=9JQtrumvg8},
  urldate = {2024-07-15},
  abstract = {Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation. However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML. We introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions. WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those. We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization. We empirically demonstrate that our modular recipe improves the success on real websites by over 50\%, and that HTML-T5 is the best model to solve various HTML understanding tasks; achieving 18.7\% higher success rate than the prior method on MiniWoB web automation benchmark, and SoTA performance on Mind2Web, an offline task planning evaluation.},
  langid = {english}
}

@inproceedings{gururanganDonStopPretraining2020,
  title = {Don't {{Stop Pretraining}}: {{Adapt Language Models}} to {{Domains}} and {{Tasks}}},
  shorttitle = {Don't {{Stop Pretraining}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A.},
  year = {2020},
  pages = {8342--8360},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.acl-main.740},
  urldate = {2022-04-12},
  abstract = {Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.}
}

@misc{gururanganDontStopPretraining2020,
  title = {Don't {{Stop Pretraining}}: {{Adapt Language Models}} to {{Domains}} and {{Tasks}}},
  shorttitle = {Don't {{Stop Pretraining}}},
  author = {Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A.},
  year = {2020},
  month = may,
  number = {arXiv:2004.10964},
  eprint = {2004.10964},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2004.10964},
  urldate = {2025-02-14},
  abstract = {Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@article{guuREALMRetrievalAugmentedLanguage2020,
  title = {{{REALM}}: {{Retrieval-Augmented Language Model Pre-Training}}},
  shorttitle = {{{REALM}}},
  author = {Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei},
  year = {2020},
  month = feb,
  journal = {ArXiv},
  url = {https://www.semanticscholar.org/paper/REALM%3A-Retrieval-Augmented-Language-Model-Guu-Lee/832fff14d2ed50eb7969c4c4b976c35776548f56},
  urldate = {2024-05-01},
  abstract = {Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts.  To capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents.  We demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16\% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.},
  keywords = {No DOI found}
}

@article{GuYingJieGuiXiaoLinLiDeFuShenYiLiaoDongJiYuShenJingWangLuoDeJiQiYueDuLiJieZongShu2020,
  title = {},
  author = { and {Ying-Jie}, {\relax GU} and {Xiao-Lin}, {\relax GUI} and {De-Fu}, {\relax LI} and Yi, Shen and Dong, Liao},
  year = {2020},
  month = apr,
  journal = {},
  volume = {31},
  number = {7},
  pages = {2095--2126},
  url = {http://www.jos.org.cn/jos/article/abstract/6048},
  urldate = {2021-12-25},
  abstract = {...BERT.;The task of machine reading comprehension is to make the machine understand natural language text and correctly answer text-related questions. Due to the limitation of the dataset scale, most of the early machine reading comprehension methods were modeled based on manual features and traditional machine learning methods. In recent years, with the development of knowledge bases and crowdsourcing, high quality large-scale datasets have been proposed by researchers, which has brought a new opportunity for the advance of neural network models and machine reading comprehension. In this survey, an exhaustive review on the state-of-the-art research efforts on machine reading comprehension based on neural network is made. First, an overview of machine reading comprehension, including development process, problem formulation, and evaluation metric, is given. Then, a comprehensive review is conducted of related technologies in the most fashionable neural reading comprehension framework including the embedding layer, encoder layer, interaction layer, and output layer as well as the latest BERT pre-training model and its advantages are discussed. After that, this paper concludes the recent research progress of machine reading comprehension datasets and neural reading comprehension model, and gives a comparison and analysis of the most representative datasets and neural network models in detail. Finally, the research challenges and future direction of machine reading comprehension are presented.},
  annotation = {00000}
}

@book{hackosUserTaskAnalysis1998,
  title = {User and {{Task Analysis}} for {{Interface Design}}},
  author = {Hackos, JoAnn T. and Redish, Janice C.},
  year = {1998},
  month = feb,
  edition = {First Edition},
  publisher = {John Wiley \& Sons, Inc.},
  address = {New York Weinheim},
  abstract = {"Hackos and Redish wisely offer us the three things we most need about user and task analysis: practical advice, practical advice, and practical advice." -Ben Shneiderman, University of Maryland  "This book is well written, thorough, and loaded with techniques, examples, and resources that bring analysis to everyone." -Marcia L. Conner, Director of Usability \& Learnability PeopleSoft, Inc.  User and Task Analysis for Interface Design helps you design a great user interface by focusing on the most important step in the process -the first one. You learn to go out and observe your users at work, whether they are employees of your company or people in customer organizations. You learn to find out what your users really need, not by asking them what they want, but by going through a process of understanding what they are trying to accomplish.  JoAnn Hackos and Janice (Ginny) Redish, internationally known experts in usable design, take you through a step-by-step process to conduct a user and task analysis. You learn: * How interface designers use user and task analysis to build successful interfaces * Why knowledge of users, their tasks, and their environments is critical to successful design * How to prepare and set up your site visits * How to select and train your user and task analysis team * What observations to make, questions to ask, and questions to avoid * How to record and report what you have learned to your development team members * How to turn the information you've gathered into design ideas * How to create paper prototypes of your interface design * How to conduct usability tests with your prototypes to find out if you're on the right track.  This book includes many examples of design successes and challenges for products of every kind.},
  isbn = {978-0-471-17831-6},
  langid = {english}
}

@misc{haCloChatUnderstandingHow2024,
  title = {{{CloChat}}: {{Understanding How People Customize}}, {{Interact}}, and {{Experience Personas}} in {{Large Language Models}}},
  shorttitle = {{{CloChat}}},
  author = {Ha, Juhye and Jeon, Hyeon and Han, DaEun and Seo, Jinwook and Oh, Changhoon},
  year = {2024},
  number = {arXiv:2402.15265},
  eprint = {2402.15265},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.15265},
  urldate = {2024-04-07},
  abstract = {Large language models (LLMs) have facilitated significant strides in generating conversational agents, enabling seamless, contextually relevant dialogues across diverse topics. However, the existing LLM-driven conversational agents have fixed personalities and functionalities, limiting their adaptability to individual user needs. Creating personalized agent personas with distinct expertise or traits can address this issue. Nonetheless, we lack knowledge of how people customize and interact with agent personas. In this research, we investigated how users customize agent personas and their impact on interaction quality, diversity, and dynamics. To this end, we developed CloChat, an interface supporting easy and accurate customization of agent personas in LLMs. We conducted a study comparing how participants interact with CloChat and ChatGPT. The results indicate that participants formed emotional bonds with the customized agents, engaged in more dynamic dialogues, and showed interest in sustaining interactions. These findings contribute to design implications for future systems with conversational agents using LLMs.},
  archiveprefix = {arXiv},
  langid = {american}
}

@article{hammontreeRemoteUsabilityTesting1994,
  title = {Remote Usability Testing},
  author = {Hammontree, Monty and Weiler, Paul and Nayak, Nandini},
  year = {1994},
  month = jul,
  journal = {interactions},
  volume = {1},
  number = {3},
  pages = {21--25},
  issn = {1072-5520},
  doi = {10.1145/182966.182969},
  urldate = {2024-09-13}
}

@misc{HandbookHumanFactors,
  title = {Handbook of {{Human Factors}} and {{Ergonomics}} {\textbar} {{Wiley Online Books}}},
  url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9781118131350#page=1274},
  urldate = {2024-09-13}
}

@inproceedings{hanOpenKEOpenToolkit2018,
  title = {{{OpenKE}}: {{An Open Toolkit}} for {{Knowledge Embedding}}},
  shorttitle = {{{OpenKE}}},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}: {{System Demonstrations}}},
  author = {Han, Xu and Cao, Shulin and Lv, Xin and Lin, Yankai and Liu, Zhiyuan and Sun, Maosong and Li, Juanzi},
  year = {2018},
  month = nov,
  pages = {139--144},
  publisher = {Association for Computational Linguistics},
  address = {Brussels, Belgium},
  doi = {10.18653/v1/d18-2024},
  urldate = {2021-12-06},
  abstract = {We release an open toolkit for knowledge embedding (OpenKE), which provides a unified framework and various fundamental models to embed knowledge graphs into a continuous low-dimensional space. OpenKE prioritizes operational efficiency to support quick model validation and large-scale knowledge representation learning. Meanwhile, OpenKE maintains sufficient modularity and extensibility to easily incorporate new models into the framework. Besides the toolkit, the embeddings of some existing large-scale knowledge graphs pre-trained by OpenKE are also available, which can be directly applied for many applications including information retrieval, personalized recommendation and question answering. The toolkit, documentation, and pre-trained embeddings are all released on http://openke.thunlp.org/.}
}

@article{HAPPIERAIbasedScientific2018,
  title = {{{HAPPIER}}: {{An AI-based Scientific Discovery Support Tool}} for {{Facilitating Target Identification}} in {{Drug Discovery}}},
  year = {2018},
  langid = {english}
}

@inproceedings{haqueDeepLearningSuicide2021,
  title = {Deep {{Learning}} for {{Suicide}} and {{Depression Identification}} with {{Unsupervised Label Correction}}},
  booktitle = {Artificial {{Neural Networks}} and {{Machine Learning}} -- {{ICANN}} 2021},
  author = {Haque, Ayaan and Reddi, Viraaj and Giallanza, Tyler},
  editor = {Farka{\v s}, Igor and Masulli, Paolo and Otte, Sebastian and Wermter, Stefan},
  year = {2021},
  pages = {436--447},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-86383-8\_35},
  abstract = {Early detection of suicidal ideation in depressed individuals can allow for adequate medical attention and support, which can be life-saving. Recent NLP research focuses on classifying, from given text, if an individual is suicidal or clinically healthy. However, there have been no major attempts to differentiate between depression and suicidal ideation, which is a separate and important clinical challenge. Due to the scarce availability of EHR data, suicide notes, or other verified sources, web query data has emerged as a promising alternative. Online sources, such as Reddit, allow for anonymity, prompting honest disclosure of symptoms, making it a plausible source even in a clinical setting. However, online datasets also result in inherent noise in web-scraped labels, which necessitates a noise-removal process to improve performance. Thus, we propose SDCNL, a deep neural network approach for suicide versus depression classification. We utilize online content to train our algorithm, and to verify and correct noisy labels, we propose a novel unsupervised label correction method which, unlike previous work, does not require prior noise distribution information. Our extensive experimentation with various deep word embedding models and classifiers display strong performance of SDCNL as a new clinical application for a challenging problem (We make our supplemental, dataset, web-scraping script, and code (with hyperparameters) available at https://github.com/ayaanzhaque/SDCNL).},
  isbn = {978-3-030-86383-8},
  langid = {english}
}

@inproceedings{harshaAutomatedResumeScreener2022,
  title = {Automated {{Resume Screener}} Using {{Natural Language Processing}}({{NLP}})},
  booktitle = {2022 6th {{International Conference}} on {{Trends}} in {{Electronics}} and {{Informatics}} ({{ICOEI}})},
  author = {Harsha, Tumula Mani and Moukthika, Gangaraju Sai and Sai, Dudipalli Siva and Pravallika, Mannuru Naga Rajeswari and Anamalamudi, Satish and Enduri, MuraliKrishna},
  year = {2022},
  month = apr,
  pages = {1772--1777},
  doi = {10.1109/ICOEI53556.2022.9777194},
  abstract = {Resume Screening is the process of evaluating the resume of the job seekers based on a specific requirement. It is used to identify the candidate eligibility for a job by matching all the requirements needed for the offered role with their resume information such as education qualification, skill sets, technical stuff etc. Resume Screening is a crucial stage in candidate's selection for a job role, it is the stage where the decision making is done whether to move the candidate to the next level of hiring process or not. Traditionally, this process is performed manually, but companies often receive thousands of resumes for job applications. In order to reduce the human involvement and errors, many new ways were introduced in this process. This paper discusses about one such process which is very efficient in performing Resume screening. It includes Natural Language Processing (NLP), an automated Machine Learning Algorithm for screening the resumes. This paper explains the end to end working of a python application which efficiently screens the resumes of the candidates based on the organization's requirement.}
}

@incollection{heConstructingKnowledgeGraph2020,
  title = {Constructing {{Knowledge Graph}} for {{Social Networks}} in {{A Deep}} and {{Holistic Way}}},
  booktitle = {Companion {{Proceedings}} of the {{Web Conference}} 2020},
  author = {He, Qi and Yang, Jaewon and Shi, Baoxu},
  year = {2020},
  month = apr,
  pages = {307--308},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3366424.3383112},
  urldate = {2022-07-12},
  abstract = {Online social networks such as Facebook and LinkedIn have been an integrated part of everyday life. To improve the user experience and power the products around the social network, Knowledge Graphs (KG) are used as a standard way to extract and organize the knowledge in social networks. This tutorial focuses on how to build KGs for social networks by developing deep NLP models, and holistic optimization of KGs and the social network. Building KG for social networks poses two challenges: 1) input data for each member in the social network is noisy, implicit and in multilingual, so a deep understanding of the input data is needed; 2) KG and the social network influence each other via multiple organic feedback loops, so a holistic view on both networks is needed. We will share the lessons we learned from tackling the above challenges in the past seven years on building the Knowledge Graph for the LinkedIn social network. To address the first challenge of noisy and implicit input data, we present how to train high precision language understanding models by adding small clean data to the noisy data. By doing so, we enhance the-state-of-the-art NLP models such as BERT for building KG. To address multilingual aspect of the input data, we explain how to expand a single-language KG to multilingual KGs by applying transfer learning. For the second challenge of modeling interactions between social network and KG, we launch new products to get explicit feedback on KG from users, and refine KG by learning deep embeddings from the social network. Lastly, we present how we use our KG to empower more than 20+ products at LinkedIn with high business impacts.},
  isbn = {978-1-4503-7024-0}
}

@misc{hedderichSurveyRecentApproaches2021,
  title = {A {{Survey}} on {{Recent Approaches}} for {{Natural Language Processing}} in {{Low-Resource Scenarios}}},
  author = {Hedderich, Michael A. and Lange, Lukas and Adel, Heike and Str{\"o}tgen, Jannik and Klakow, Dietrich},
  year = {2021},
  month = apr,
  number = {arXiv:2010.12309},
  eprint = {2010.12309},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2010.12309},
  urldate = {2025-02-14},
  abstract = {Deep neural networks and huge language models are becoming omnipresent in natural language applications. As they are known for requiring large amounts of training data, there is a growing body of work to improve the performance in low-resource settings. Motivated by the recent fundamental changes towards neural models and the popular pre-train and fine-tune paradigm, we survey promising approaches for low-resource natural language processing. After a discussion about the different dimensions of data availability, we give a structured overview of methods that enable learning when training data is sparse. This includes mechanisms to create additional labeled data like data augmentation and distant supervision as well as transfer learning settings that reduce the need for target supervision. A goal of our survey is to explain how these methods differ in their requirements as understanding them is essential for choosing a technique suited for a specific low-resource setting. Further key aspects of this work are to highlight open issues and to outline promising directions for future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{heEfficientAgentTraining2025,
  title = {Efficient {{Agent Training}} for {{Computer Use}}},
  author = {He, Yanheng and Jin, Jiahe and Liu, Pengfei},
  year = {2025},
  month = may,
  number = {arXiv:2505.13909},
  eprint = {2505.13909},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.13909},
  urldate = {2025-06-04},
  abstract = {Scaling up high-quality trajectory data has long been a critical bottleneck for developing human-like computer use agents. We introduce PC Agent-E, an efficient agent training framework that significantly reduces reliance on large-scale human demonstrations. Starting with just 312 human-annotated computer use trajectories, we further improved data quality by synthesizing diverse action decisions with Claude 3.7 Sonnet. Trained on these enriched trajectories, our PC Agent-E model achieved a remarkable 141\% relative improvement, surpassing the strong Claude 3.7 Sonnet with extended thinking on WindowsAgentArena-V2, an improved benchmark we also released. Furthermore, PC Agent-E demonstrates strong generalizability to different operating systems on OSWorld. Our findings suggest that strong computer use capabilities can be stimulated from a small amount of high-quality trajectory data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@inproceedings{heLargeLanguageModels2023,
  title = {Large {{Language Models}} as {{Zero-Shot Conversational Recommenders}}},
  booktitle = {Proceedings of the 32nd {{ACM International Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {He, Zhankui and Xie, Zhouhang and Jha, Rahul and Steck, Harald and Liang, Dawen and Feng, Yesu and Majumder, Bodhisattwa Prasad and Kallus, Nathan and Mcauley, Julian},
  year = {2023},
  month = oct,
  series = {{{CIKM}} '23},
  pages = {720--730},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3583780.3614949},
  urldate = {2024-09-24},
  abstract = {In this paper, we present empirical studies on conversational recommendation tasks using representative large language models in a zero-shot setting with three primary contributions. (1) Data: To gain insights into model behavior in "in-the-wild" conversational recommendation scenarios, we construct a new dataset of recommendation-related conversations by scraping a popular discussion website. This is the largest public real-world conversational recommendation dataset to date. (2) Evaluation: On the new dataset and two existing conversational recommendation datasets, we observe that even without fine-tuning, large language models can outperform existing fine-tuned conversational recommendation models. (3) Analysis: We propose various probing tasks to investigate the mechanisms behind the remarkable performance of large language models in conversational recommendation. We analyze both the large language models' behaviors and the characteristics of the datasets, providing a holistic understanding of the models' effectiveness, limitations and suggesting directions for the design of future conversational recommenders.},
  isbn = {979-8-4007-0124-5}
}

@incollection{helbingAgentBasedModeling2012,
  title = {Agent-{{Based Modeling}}},
  booktitle = {Social {{Self-Organization}}: {{Agent-Based Simulations}} and {{Experiments}} to {{Study Emergent Social Behavior}}},
  author = {Helbing, Dirk},
  editor = {Helbing, Dirk},
  year = {2012},
  pages = {25--70},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-24004-1\_2},
  urldate = {2024-10-10},
  abstract = {Since the advent of computers, the natural and engineering sciences have enormously progressed. Computer simulations allow one to understand interactions of physical particles and make sense of astronomical observations, to describe many chemical properties ab initio, and to design energy-efficient aircrafts and safer cars. Today, the use of computational devices is pervasive. Offices, administrations, financial trading, economic exchange, the control of infrastructure networks, and a large share of our communication would not be conceivable without the use of computers anymore. Hence, it would be very surprising, if computers could not make a contribution to a better understanding of social and economic systems.},
  isbn = {978-3-642-24004-1},
  langid = {english},
  keywords = {Goal Function,Route Choice,Stylize Fact,System Behavior,Traffic Flow}
}

@misc{hendrycksCUADExpertAnnotatedNLP2021,
  title = {{{CUAD}}: {{An Expert-Annotated NLP Dataset}} for {{Legal Contract Review}}},
  shorttitle = {{{CUAD}}},
  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},
  year = {2021},
  month = nov,
  number = {arXiv:2103.06268},
  eprint = {2103.06268},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.06268},
  urldate = {2023-10-23},
  abstract = {Many specialized domains remain untouched by deep learning, as large labeled datasets require expensive expert annotators. We address this bottleneck within the legal domain by introducing the Contract Understanding Atticus Dataset (CUAD), a new dataset for legal contract review. CUAD was created with dozens of legal experts from The Atticus Project and consists of over 13,000 annotations. The task is to highlight salient portions of a contract that are important for a human to review. We find that Transformer models have nascent performance, but that this performance is strongly influenced by model design and training dataset size. Despite these promising results, there is still substantial room for improvement. As one of the only large, specialized NLP benchmarks annotated by experts, CUAD can serve as a challenging research benchmark for the broader NLP community.},
  archiveprefix = {arXiv}
}

@inproceedings{hermannTeachingMachinesRead2015,
  title = {Teaching {{Machines}} to {{Read}} and {{Comprehend}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hermann, Karl Moritz and Kocisky, Tomas and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
  year = {2015},
  volume = {28},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2015/hash/afdec7005cc9f14302cd0474fd0f3c96-Abstract.html},
  urldate = {2022-01-14},
  annotation = {02578}
}

@article{hertzumEvaluatorEffectChilling2003,
  title = {The {{Evaluator Effect}}: {{A Chilling Fact About Usability Evaluation Methods}}},
  shorttitle = {The {{Evaluator Effect}}},
  author = {Hertzum, Morten and Jacobsen, Niels Ebbe},
  year = {2003},
  month = feb,
  journal = {International Journal of Human-Computer Interaction},
  publisher = {Lawrence Erlbaum Associates, Inc.},
  doi = {10.1207/S15327590IJHC1501\_14},
  urldate = {2024-09-11},
  abstract = {Computer professionals have a need for robust, easy-to-use usability evaluation methods (UEMs) to help them systematically improve the usability of computer artifacts. However, cognitive walkthrou...},
  copyright = {Copyright Taylor and Francis Group, LLC},
  langid = {english}
}

@misc{heSurveyUserBehavior2023,
  title = {A {{Survey}} on {{User Behavior Modeling}} in {{Recommender Systems}}},
  author = {He, Zhicheng and Liu, Weiwen and Guo, Wei and Qin, Jiarui and Zhang, Yingxue and Hu, Yaochen and Tang, Ruiming},
  year = {2023},
  month = feb,
  number = {arXiv:2302.11087},
  eprint = {2302.11087},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.11087},
  urldate = {2024-09-18},
  abstract = {User Behavior Modeling (UBM) plays a critical role in user interest learning, which has been extensively used in recommender systems. Crucial interactive patterns between users and items have been exploited, which brings compelling improvements in many recommendation tasks. In this paper, we attempt to provide a thorough survey of this research topic. We start by reviewing the research background of UBM. Then, we provide a systematic taxonomy of existing UBM research works, which can be categorized into four different directions including Conventional UBM, Long-Sequence UBM, Multi-Type UBM, and UBM with Side Information. Within each direction, representative models and their strengths and weaknesses are comprehensively discussed. Besides, we elaborate on the industrial practices of UBM methods with the hope of providing insights into the application value of existing UBM solutions. Finally, we summarize the survey and discuss the future prospects of this field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning,H.0}
}

@misc{heWebVoyagerBuildingEndtoEnd2024,
  title = {{{WebVoyager}}: {{Building}} an {{End-to-End Web Agent}} with {{Large Multimodal Models}}},
  shorttitle = {{{WebVoyager}}},
  author = {He, Hongliang and Yao, Wenlin and Ma, Kaixin and Yu, Wenhao and Dai, Yong and Zhang, Hongming and Lan, Zhenzhong and Yu, Dong},
  year = {2024},
  month = jun,
  number = {arXiv:2401.13919},
  eprint = {2401.13919},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2401.13919},
  urldate = {2024-07-12},
  abstract = {The rapid advancement of large language models (LLMs) has led to a new era marked by the development of autonomous applications in real-world scenarios, which drives innovation in creating advanced web agents. Existing web agents typically only handle one input modality and are evaluated only in simplified web simulators or static web snapshots, greatly limiting their applicability in real-world scenarios. To bridge this gap, we introduce WebVoyager, an innovative Large Multimodal Model (LMM) powered web agent that can complete user instructions end-to-end by interacting with real-world websites. Moreover, we establish a new benchmark by compiling real-world tasks from 15 popular websites and introduce an automatic evaluation protocol leveraging multimodal understanding abilities of GPT-4V to evaluate open-ended web agents. We show that WebVoyager achieves a 59.1\% task success rate on our benchmark, significantly surpassing the performance of both GPT-4 (All Tools) and the WebVoyager (text-only) setups, underscoring the exceptional capability of WebVoyager. The proposed automatic evaluation metric achieves 85.3\% agreement with human judgment, indicating its effectiveness in providing reliable and accurate assessments of web agents.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@article{hibbardDevelopmentPatientActivation2004,
  title = {Development of the {{Patient Activation Measure}} ({{PAM}}): {{Conceptualizing}} and {{Measuring Activation}} in {{Patients}} and {{Consumers}}},
  shorttitle = {Development of the {{Patient Activation Measure}} ({{PAM}})},
  author = {Hibbard, Judith H. and Stockard, Jean and Mahoney, Eldon R. and Tusler, Martin},
  year = {2004},
  month = aug,
  journal = {Health Services Research},
  volume = {39},
  number = {4p1},
  pages = {1005--1026},
  issn = {0017-9124, 1475-6773},
  doi = {10.1111/j.1475-6773.2004.00269.x},
  urldate = {2024-04-23},
  abstract = {Background.               Controlling costs and achieving health care quality improvements require the participation of activated and informed consumers and patients.                                         Objectives.               We describe a process for conceptualizing and operationalizing what it means to be ``activated'' and delineate the process we used to develop a measure for assessing ``activation,'' and the psychometric properties of that measure.                                         Methods.               We used the convergence of the findings from a national expert consensus panel and patient focus groups to define the concept and identify the domains of activation. These domains were operationalized by constructing a large item pool. Items were pilot-tested and initial psychometric analysis performed using Rasch methodology. The third stage refined and extended the measure. The fourth stage used a national probability sample to assess the measure's psychometric performance overall and within different subpopulations.                                         Study Sample.               Convenience samples of patients with and without chronic illness, and a national probability sample (               N               =1,515) are included at different stages in the research.                                         Conclusions.               The Patient Activation Measure is a valid, highly reliable, unidimensional, probabilistic Guttman-like scale that reflects a developmental model of activation. Activation appears to involve four stages: (1) believing the patient role is important, (2) having the confidence and knowledge necessary to take action, (3) actually taking action to maintain and improve one's health, and (4) staying the course even under stress. The measure has good psychometric properties indicating that it can be used at the individual patient level to tailor intervention and assess changes.},
  langid = {american},
  pmcid = {PMC1361049},
  pmid = {15230939}
}

@inproceedings{hidasiRecurrentNeuralNetworks2018,
  title = {Recurrent {{Neural Networks}} with {{Top-k Gains}} for {{Session-based Recommendations}}},
  booktitle = {Proceedings of the 27th {{ACM International Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Hidasi, Bal{\'a}zs and Karatzoglou, Alexandros},
  year = {2018},
  month = oct,
  series = {{{CIKM}} '18},
  pages = {843--852},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3269206.3271761},
  urldate = {2024-09-24},
  abstract = {RNNs have been shown to be excellent models for sequential data and in particular for data that is generated by users in an session-based manner. The use of RNNs provides impressive performance benefits over classical methods in session-based recommendations. In this work we introduce novel ranking loss functions tailored to RNNs in the recommendation setting. The improved performance of these losses over alternatives, along with further tricks and refinements described in this work, allow for an overall improvement of up to 35\% in terms of MRR and Recall@20 over previous session-based RNN solutions and up to 53\% over classical collaborative filtering approaches. Unlike data augmentation-based improvements, our method does not increase training times significantly. We further demonstrate the performance gain of the RNN over baselines in an online A/B test.},
  isbn = {978-1-4503-6014-2}
}

@misc{hidasiSessionbasedRecommendationsRecurrent2016,
  title = {Session-Based {{Recommendations}} with {{Recurrent Neural Networks}}},
  author = {Hidasi, Bal{\'a}zs and Karatzoglou, Alexandros and Baltrunas, Linas and Tikk, Domonkos},
  year = {2016},
  month = mar,
  number = {arXiv:1511.06939},
  eprint = {1511.06939},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1511.06939},
  urldate = {2024-09-24},
  abstract = {We apply recurrent neural networks (RNN) on a new domain, namely recommender systems. Real-life recommender systems often face the problem of having to base recommendations only on short session-based data (e.g. a small sportsware website) instead of long user histories (as in the case of Netflix). In this situation the frequently praised matrix factorization approaches are not accurate. This problem is usually overcome in practice by resorting to item-to-item recommendations, i.e. recommending similar items. We argue that by modeling the whole session, more accurate recommendations can be provided. We therefore propose an RNN-based approach for session-based recommendations. Our approach also considers practical aspects of the task and introduces several modifications to classic RNNs such as a ranking loss function that make it more viable for this specific problem. Experimental results on two data-sets show marked improvements over widely used approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@inproceedings{hillGoldilocksPrincipleReading2016,
  title = {The {{Goldilocks Principle}}: {{Reading Children}}'s {{Books}} with {{Explicit Memory Representations}}},
  shorttitle = {The {{Goldilocks Principle}}},
  booktitle = {4th {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2016, {{San Juan}}, {{Puerto Rico}}, {{May}} 2-4, 2016, {{Conference Track Proceedings}}},
  author = {Hill, Felix and Bordes, Antoine and Chopra, Sumit and Weston, Jason},
  editor = {Bengio, Yoshua and LeCun, Yann},
  year = {2016},
  url = {http://arxiv.org/abs/1511.02301},
  urldate = {2022-01-20},
  annotation = {00549}
}

@misc{hoDenoisingDiffusionProbabilistic2020,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  year = {2020},
  month = dec,
  number = {arXiv:2006.11239},
  eprint = {2006.11239},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.11239},
  urldate = {2023-11-14},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
  archiveprefix = {arXiv}
}

@misc{hortonLargeLanguageModels2023,
  title = {Large {{Language Models}} as {{Simulated Economic Agents}}: {{What Can We Learn}} from {{Homo Silicus}}?},
  shorttitle = {Large {{Language Models}} as {{Simulated Economic Agents}}},
  author = {Horton, John J.},
  year = {2023},
  month = jan,
  number = {arXiv:2301.07543},
  eprint = {2301.07543},
  primaryclass = {econ, q-fin},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.07543},
  urldate = {2023-06-25},
  abstract = {Newly-developed large language models (LLM) -- because of how they are trained and designed -- are implicit computational models of humans -- a homo silicus. These models can be used the same way economists use homo economicus: they can be given endowments, information, preferences, and so on and then their behavior can be explored in scenarios via simulation. I demonstrate this approach using OpenAI's GPT3 with experiments derived from Charness and Rabin (2002), Kahneman, Knetsch and Thaler (1986) and Samuelson and Zeckhauser (1988). The findings are qualitatively similar to the original results, but it is also trivially easy to try variations that offer fresh insights. Departing from the traditional laboratory paradigm, I also create a hiring scenario where an employer faces applicants that differ in experience and wage ask and then analyze how a minimum wage affects realized wages and the extent of labor-labor substitution.},
  archiveprefix = {arXiv}
}

@article{horvitzReflectionsChallengesPromises,
  title = {Reflections on {{Challenges}} and {{Promises}} of {{Mixed-Initiative Interaction}}},
  author = {Horvitz, Eric},
  abstract = {Research on mixed-initiative interaction and assistance is still in its infancy but is poised to blossom into a wellspring of innovation that promises to change the way we work with computing systems---and the way that computing systems work with us. I share reflections about the opportunities ahead for developing computational systems with the ability to engage people in a deeply collaborative manner, founded on their ability to support fluid mixed-initiative problem solving.},
  langid = {english}
}

@inproceedings{houLargeLanguageModels2024,
  title = {Large {{Language Models}} Are {{Zero-Shot Rankers}} for~{{Recommender Systems}}},
  booktitle = {Advances in {{Information Retrieval}}},
  author = {Hou, Yupeng and Zhang, Junjie and Lin, Zihan and Lu, Hongyu and Xie, Ruobing and McAuley, Julian and Zhao, Wayne Xin},
  editor = {Goharian, Nazli and Tonellotto, Nicola and He, Yulan and Lipani, Aldo and McDonald, Graham and Macdonald, Craig and Ounis, Iadh},
  year = {2024},
  pages = {364--381},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-56060-6\_24},
  abstract = {Recently, large language models~(LLMs) (e.g., GPT-4) have demonstrated impressive general-purpose task-solving abilities, including the potential to approach recommendation tasks. Along this line of research, this work aims to investigate the capacity of LLMs that act as the ranking model for recommender systems. We first formalize the recommendation problem as a conditional ranking task, considering sequential interaction histories as conditions and the items retrieved by other candidate generation models as candidates. To solve the ranking task by LLMs, we carefully design the prompting template and conduct extensive experiments on two widely-used datasets. We show that LLMs have promising zero-shot ranking abilities but (1) struggle to perceive the order of historical interactions, and (2) can be biased by popularity or item positions in the prompts. We demonstrate that these issues can be alleviated using specially designed prompting and bootstrapping strategies. Equipped with these insights, zero-shot LLMs can even challenge conventional recommendation models when ranking candidates are retrieved by multiple candidate generators. The code and processed datasets are available at https://github.com/RUCAIBox/LLMRank.},
  isbn = {978-3-031-56060-6},
  langid = {english}
}

@misc{HowACHWorks2014,
  title = {How {{ACH}} Works: {{A}} Developer Perspective - {{Part}} 3},
  shorttitle = {How {{ACH}} Works},
  year = {2014},
  month = jul,
  journal = {Gusto Engineering},
  url = {https://engineering.gusto.com/how-ach-works-a-developer-perspective-part-3/},
  urldate = {2023-10-06},
  abstract = {At Gusto [https://gusto.com], we rely heavily on the ACH network to pay employees and to remit various payroll taxes to federal and state agencies on behalf of our clients. In part 1 [https://engineering.gusto.com/how-ach-works-a-developer-perspective-part-1/] of this post, I outlined the basics of how we originate},
  langid = {english}
}

@misc{huaInteractiveSpeculativePlanning2024,
  title = {Interactive {{Speculative Planning}}: {{Enhance Agent Efficiency}} through {{Co-design}} of {{System}} and {{User Interface}}},
  shorttitle = {Interactive {{Speculative Planning}}},
  author = {Hua, Wenyue and Wan, Mengting and Vadrevu, Shashank and Nadel, Ryan and Zhang, Yongfeng and Wang, Chi},
  year = {2024},
  month = sep,
  number = {arXiv:2410.00079},
  eprint = {2410.00079},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.00079},
  urldate = {2025-04-06},
  abstract = {Agents, as user-centric tools, are increasingly deployed for human task delegation, assisting with a broad spectrum of requests by generating thoughts, engaging with user proxies, and producing action plans. However, agents based on large language models (LLMs) often face substantial planning latency due to two primary factors: the efficiency limitations of the underlying LLMs due to their large size and high demand, and the structural complexity of the agents due to the extensive generation of intermediate thoughts to produce the final output. Given that inefficiency in service provision can undermine the value of automation for users, this paper presents a human-centered efficient agent planning method -- Interactive Speculative Planning -- aiming at enhancing the efficiency of agent planning through both system design and human-AI interaction. Our approach advocates for the co-design of the agent system and user interface, underscoring the importance of an agent system that can fluidly manage user interactions and interruptions. By integrating human interruptions as a fundamental component of the system, we not only make it more user-centric but also expedite the entire process by leveraging human-in-the-loop interactions to provide accurate intermediate steps. Code and data will be released.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Multiagent Systems}
}

@inproceedings{huangAutoScraperProgressiveUnderstanding2024,
  title = {{{AutoScraper}}: {{A Progressive Understanding Web Agent}} for {{Web Scraper Generation}}},
  shorttitle = {{{AutoScraper}}},
  booktitle = {Proceedings of the 2024 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Huang, Wenhao and Gu, Zhouhong and Peng, Chenghao and Liang, Jiaqing and Li, Zhixu and Xiao, Yanghua and Wen, Liqian and Chen, Zulong},
  editor = {{Al-Onaizan}, Yaser and Bansal, Mohit and Chen, Yun-Nung},
  year = {2024},
  month = nov,
  pages = {2371--2389},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.141},
  urldate = {2024-12-11},
  abstract = {Web scraping is a powerful technique that extracts data from websites, enabling automated data collection, enhancing data analysis capabilities, and minimizing manual data entry efforts. Existing methods, wrappers-based methods suffer from limited adaptability and scalability when faced with a new website, while language agents, empowered by large language models (LLMs), exhibit poor reusability in diverse web environments. In this work, we introduce the paradigm of generating web scrapers with LLMs and propose AutoScraper, a two-stage framework that can handle diverse and changing web environments more efficiently. AutoScraper leverages the hierarchical structure of HTML and similarity across different web pages for generating web scrapers. Besides, we propose a new executability metric for better measuring the performance of web scraper generation tasks. We conduct comprehensive experiments with multiple LLMs and demonstrate the effectiveness of our framework. Our work is now open-source.}
}

@misc{huangCalibratingLongformGenerations2024,
  title = {Calibrating {{Long-form Generations}} from {{Large Language Models}}},
  author = {Huang, Yukun and Liu, Yixin and Thirukovalluru, Raghuveer and Cohan, Arman and Dhingra, Bhuwan},
  year = {2024},
  month = feb,
  number = {arXiv:2402.06544},
  eprint = {2402.06544},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2402.06544},
  urldate = {2024-02-12},
  abstract = {To enhance Large Language Models' (LLMs) reliability, calibration is essential -- the model's assessed confidence scores should align with the actual likelihood of its responses being correct. However, current confidence elicitation methods and calibration metrics typically rely on a binary true/false assessment of response correctness. This approach does not apply to long-form generation, where an answer can be partially correct. Addressing this gap, we introduce a unified calibration framework, in which both the correctness of the LLMs' responses and their associated confidence levels are treated as distributions across a range of scores. Within this framework, we develop three metrics to precisely evaluate LLM calibration and further propose two confidence elicitation methods based on self-consistency and self-evaluation. Our experiments, which include long-form QA and summarization tasks, demonstrate that larger models don't necessarily guarantee better calibration, that calibration performance is found to be metric-dependent, and that self-consistency methods excel in factoid datasets. We also find that calibration can be enhanced through techniques such as fine-tuning, integrating relevant source documents, scaling the temperature, and combining self-consistency with self-evaluation. Lastly, we showcase a practical application of our system: selecting and cascading open-source models and ChatGPT to optimize correctness given a limited API budget. This research not only challenges existing notions of LLM calibration but also offers practical methodologies for improving trustworthiness in long-form generation.},
  archiveprefix = {arXiv}
}

@misc{huangChatGPTBetterHuman2023,
  title = {Is {{ChatGPT}} Better than {{Human Annotators}}? {{Potential}} and {{Limitations}} of {{ChatGPT}} in {{Explaining Implicit Hate Speech}}},
  shorttitle = {Is {{ChatGPT}} Better than {{Human Annotators}}?},
  author = {Huang, Fan and Kwak, Haewoon and An, Jisun},
  year = {2023},
  month = mar,
  eprint = {2302.07736},
  primaryclass = {cs},
  doi = {10.1145/3543873.3587368},
  urldate = {2023-03-29},
  abstract = {Recent studies have alarmed that many online hate speeches are implicit. With its subtle nature, the explainability of the detection of such hateful speech has been a challenging problem. In this work, we examine whether ChatGPT can be used for providing natural language explanations (NLEs) for implicit hateful speech detection. We design our prompt to elicit concise ChatGPT-generated NLEs and conduct user studies to evaluate their qualities by comparison with human-written NLEs. We discuss the potential and limitations of ChatGPT in the context of implicit hateful speech research.},
  archiveprefix = {arXiv}
}

@article{huangImprovedKnowledgeBase2016,
  title = {Improved {{Knowledge Base Completion}} by {{Path-Augmented TransR Model}}},
  author = {Huang, Wenhao and Li, Ge and Jin, Zhi},
  year = {2016},
  month = oct,
  journal = {arXiv:1610.04073 [cs]},
  eprint = {1610.04073},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1610.04073},
  urldate = {2022-02-06},
  abstract = {Knowledge base completion aims to infer new relations from existing information. In this paper, we propose path-augmented TransR (PTransR) model to improve the accuracy of link prediction. In our approach, we base PTransR model on TransR, which is the best one-hop model at present. Then we regularize TransR with information of relation paths. In our experiment, we evaluate PTransR on the task of entity prediction. Experimental results show that PTransR outperforms previous models.},
  archiveprefix = {arXiv},
  annotation = {00011}
}

@misc{huangScaleTrackScalingBacktracking2025,
  title = {{{ScaleTrack}}: {{Scaling}} and Back-Tracking {{Automated GUI Agents}}},
  shorttitle = {{{ScaleTrack}}},
  author = {Huang, Jing and Zeng, Zhixiong and Han, Wenkang and Zhong, Yufeng and Zheng, Liming and Fu, Shuai and Chen, Jingyuan and Ma, Lin},
  year = {2025},
  month = may,
  number = {arXiv:2505.00416},
  eprint = {2505.00416},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.00416},
  urldate = {2025-06-04},
  abstract = {Automated GUI agents aims to facilitate user interaction by automatically performing complex tasks in digital environments, such as web, mobile, desktop devices. It receives textual task instruction and GUI description to generate executable actions ({\textbackslash}emph\{e.g.\}, click) and operation boxes step by step. Training a GUI agent mainly involves grounding and planning stages, in which the GUI grounding focuses on finding the execution coordinates according to the task, while the planning stage aims to predict the next action based on historical actions. However, previous work suffers from the limitations of insufficient training data for GUI grounding, as well as the ignorance of backtracking historical behaviors for GUI planning. To handle the above challenges, we propose ScaleTrack, a training framework by scaling grounding and backtracking planning for automated GUI agents. We carefully collected GUI samples of different synthesis criterions from a wide range of sources, and unified them into the same template for training GUI grounding models. Moreover, we design a novel training strategy that predicts the next action from the current GUI image, while also backtracking the historical actions that led to the GUI image. In this way, ScaleTrack explains the correspondence between GUI images and actions, which effectively describes the evolution rules of the GUI environment. Extensive experimental results demonstrate the effectiveness of ScaleTrack. Data and code will be available at url.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence}
}

@phdthesis{HuJiQiYueDuLiJieYuWenBenWenDaJiShuYanJiu2019,
  type = {{}},
  title = {{}},
  author = {, },
  year = {2019},
  url = {https://kns.cnki.net/KCMS/detail/detail.aspx?dbcode=CDFD&dbname=CDFDLAST2022&filename=1021828429.nh&v=},
  abstract = {,,,,,,,,,,,:1);2);3),;4),;5),,,-:,,,,,-,,,,,------,,,,,,+,,,,2.0,,-,--,-,,,,,-,,,,,},
  collaborator = {,  and , },
  langid = {chinese},
  school = {},
  annotation = {2 citations(CNKI)[3-24-2022]}
}

@misc{huLoRALowRankAdaptation2021,
  title = {{{LoRA}}: {{Low-Rank Adaptation}} of {{Large Language Models}}},
  shorttitle = {{{LoRA}}},
  author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and {Allen-Zhu}, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  year = {2021},
  month = oct,
  number = {arXiv:2106.09685},
  eprint = {2106.09685},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.09685},
  urldate = {2024-06-06},
  abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{huPlanningorientedAutonomousDriving2023,
  title = {Planning-Oriented {{Autonomous Driving}}},
  author = {Hu, Yihan and Yang, Jiazhi and Chen, Li and Li, Keyu and Sima, Chonghao and Zhu, Xizhou and Chai, Siqi and Du, Senyao and Lin, Tianwei and Wang, Wenhai and Lu, Lewei and Jia, Xiaosong and Liu, Qiang and Dai, Jifeng and Qiao, Yu and Li, Hongyang},
  year = {2023},
  month = mar,
  number = {arXiv:2212.10156},
  eprint = {2212.10156},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.10156},
  urldate = {2023-06-23},
  abstract = {Modern autonomous driving system is characterized as modular tasks in sequential order, i.e., perception, prediction, and planning. In order to perform a wide diversity of tasks and achieve advanced-level intelligence, contemporary approaches either deploy standalone models for individual tasks, or design a multi-task paradigm with separate heads. However, they might suffer from accumulative errors or deficient task coordination. Instead, we argue that a favorable framework should be devised and optimized in pursuit of the ultimate goal, i.e., planning of the self-driving car. Oriented at this, we revisit the key components within perception and prediction, and prioritize the tasks such that all these tasks contribute to planning. We introduce Unified Autonomous Driving (UniAD), a comprehensive framework up-to-date that incorporates full-stack driving tasks in one network. It is exquisitely devised to leverage advantages of each module, and provide complementary feature abstractions for agent interaction from a global perspective. Tasks are communicated with unified query interfaces to facilitate each other toward planning. We instantiate UniAD on the challenging nuScenes benchmark. With extensive ablations, the effectiveness of using such a philosophy is proven by substantially outperforming previous state-of-the-arts in all aspects. Code and models are public.},
  archiveprefix = {arXiv}
}

@inproceedings{huTaichiProgrammingLanguage2020,
  title = {The {{Taichi}} Programming Language},
  booktitle = {{{ACM SIGGRAPH}} 2020 {{Courses}}},
  author = {Hu, Yuanming},
  year = {2020},
  month = aug,
  series = {{{SIGGRAPH}} '20},
  pages = {1--50},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3388769.3407493},
  urldate = {2022-02-20},
  isbn = {978-1-4503-7972-4}
}

@misc{huWebCoTEnhancingWeb2025,
  title = {{{WebCoT}}: {{Enhancing Web Agent Reasoning}} by {{Reconstructing Chain-of-Thought}} in {{Reflection}}, {{Branching}}, and {{Rollback}}},
  shorttitle = {{{WebCoT}}},
  author = {Hu, Minda and Fang, Tianqing and Zhang, Jianshu and Ma, Junyu and Zhang, Zhisong and Zhou, Jingyan and Zhang, Hongming and Mi, Haitao and Yu, Dong and King, Irwin},
  year = {2025},
  month = may,
  number = {arXiv:2505.20013},
  eprint = {2505.20013},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.20013},
  urldate = {2025-06-04},
  abstract = {Web agents powered by Large Language Models (LLMs) show promise for next-generation AI, but their limited reasoning in uncertain, dynamic web environments hinders robust deployment. In this paper, we identify key reasoning skills essential for effective web agents, i.e., reflection \& lookahead, branching, and rollback, and curate trajectory data that exemplifies these abilities by reconstructing the agent's (inference-time) reasoning algorithms into chain-of-thought rationales. We conduct experiments in the agent self-improving benchmark, OpenWebVoyager, and demonstrate that distilling salient reasoning patterns into the backbone LLM via simple fine-tuning can substantially enhance its performance. Our approach yields significant improvements across multiple benchmarks, including WebVoyager, Mind2web-live, and SimpleQA (web search), highlighting the potential of targeted reasoning skill enhancement for web agents.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{HyattPaymentDetails,
  title = {Hyatt {\textbar} {{Payment Details}}},
  url = {https://www.hyatt.com/payment/details/PDcQ57N0O0WxJuzP79YaBysRjPIt0IRls6HWDhWVBTChWlkSoVGlzRRXdgiHopXsp03ENqmK02JPvpyyVXWoa%2BCiB1%2Fw6CKfsSpbv%2Fj9izYkmSK9yUpcx%2BXU7A1Sx0lqt5biEj7E8ct7Cc5hCiqduJbNVQ%2Bg6IbdALU%2F1Ljv3oewY%2B4tT7h8gIFzxNP2I1tpcQUkvr4n0g8bNuZzn0J0ZEm8JE5p4w8f4XP9czwl%2B%2F%2Ft7Me3RNyyInNv%2BBDJ6f6aAwWuTcu39hsGhFy7uECod25BiTiFGAUcNk%2F6vjkuAtpF5N4gPWbVNUr7eMLlXzw%2FjdlyJNllCvdJuHVS3zt4ebhkPGWx%2F9d04JS5OiNel%2BscLAgc9XARANUxxZ4L44UTitvGGX9YDF0P95FmZWvV%2FbXiuYi2xE4an%2Fv8dW5YcUaGYp8oxLCyj7FwCMVyHEgoOVKi5HEobwbybck5TaKoRUt%2Fe51LRK8GGR9Z2ZVkoQJv4UGtEqA24Fi75QsYi3RES8%2B%2FGg0y4ceI10WMr7e4vYQUKqnS3mvWPtW4Ax3oZ7JAgVasb0CkB9QHwPFXZOif},
  urldate = {2025-04-27}
}

@misc{idreesFrameworkRealisticSimulation2023,
  title = {A {{Framework}} for {{Realistic Simulation}} of {{Daily Human Activity}}},
  author = {Idrees, Ifrah and Singh, Siddharth and Xu, Kerui and Glas, Dylan F.},
  year = {2023},
  month = nov,
  number = {arXiv:2311.15400},
  eprint = {2311.15400},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2311.15400},
  urldate = {2024-01-27},
  abstract = {For social robots like Astro which interact with and adapt to the daily movements of users within the home, realistic simulation of human activity is needed for feature development and testing. This paper presents a framework for simulating daily human activity patterns in home environments at scale, supporting manual configurability of different personas or activity patterns, variation of activity timings, and testing on multiple home layouts. We introduce a method for specifying day-to-day variation in schedules and present a bidirectional constraint propagation algorithm for generating schedules from templates. We validate the expressive power of our framework through a use case scenario analysis and demonstrate that our method can be used to generate data closely resembling human behavior from three public datasets and a self-collected dataset. Our contribution supports systematic testing of social robot behaviors at scale, enables procedural generation of synthetic datasets of human movement in different households, and can help minimize bias in training data, leading to more robust and effective robots for home environments.},
  archiveprefix = {arXiv}
}

@misc{InstancesEC2Useast1,
  title = {Instances {\textbar} {{EC2}} {\textbar} Us-East-1},
  url = {https://us-east-1.console.aws.amazon.com/ec2/home?region=us-east-1#Instances:instanceState=running;search=:p4;v=3;$case=tags:true%5C,client:false;$regex=tags:false%5C,client:false},
  urldate = {2024-11-06}
}

@misc{IntroducingClaude35,
  title = {Introducing {{Claude}} 3.5 {{Sonnet}} {\textbackslash} {{Anthropic}}},
  url = {https://www.anthropic.com/news/claude-3-5-sonnet},
  urldate = {2024-10-28}
}

@inproceedings{iongOpenWebAgentOpenToolkit2024,
  title = {{{OpenWebAgent}}: {{An Open Toolkit}} to {{Enable Web Agents}} on {{Large Language Models}}},
  shorttitle = {{{OpenWebAgent}}},
  booktitle = {Proceedings of the 62nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 3: {{System Demonstrations}})},
  author = {Iong, Iat Long and Liu, Xiao and Chen, Yuxuan and Lai, Hanyu and Yao, Shuntian and Shen, Pengbo and Yu, Hao and Dong, Yuxiao and Tang, Jie},
  editor = {Cao, Yixin and Feng, Yang and Xiong, Deyi},
  year = {2024},
  month = aug,
  pages = {72--81},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-demos.8},
  urldate = {2024-12-11},
  abstract = {We introduce OpenWebAgent, an open toolkit designed to optimize web automation by integrating both large language models (LLMs) and large multimodal models (LMMs). This toolkit focuses on enhancing human-computer interactions on the web, simplifying complex tasks through an advanced HTML parser, a rapid action generation module, and an intuitive user interface. At the core of OpenWebAgent is an innovative web agent framework that uses a modular design to allow developers to seamlessly integrate a variety of models and tools to process web information and automate tasks on the web. This enables the development of powerful, task-oriented web agents, significantly enhancing user experience and operational efficiency on the web. The OpenWebAgent framework, Chrome plugin, and demo video are available at https://github.com/THUDM/OpenWebAgent/.}
}

@article{ishidaWeNeedZero2021,
  title = {Do {{We Need Zero Training Loss After Achieving Zero Training Error}}?},
  author = {Ishida, Takashi and Yamane, Ikko and Sakai, Tomoya and Niu, Gang and Sugiyama, Masashi},
  year = {2021},
  month = mar,
  journal = {arXiv:2002.08709 [cs, stat]},
  eprint = {2002.08709},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2002.08709},
  urldate = {2021-12-12},
  abstract = {Overparameterized deep networks have the capacity to memorize training data with zero {\textbackslash}emph\{training error\}. Even after memorization, the {\textbackslash}emph\{training loss\} continues to approach zero, making the model overconfident and the test performance degraded. Since existing regularizers do not directly aim to avoid zero training loss, it is hard to tune their hyperparameters in order to maintain a fixed/preset level of training loss. We propose a direct solution called {\textbackslash}emph\{flooding\} that intentionally prevents further reduction of the training loss when it reaches a reasonably small value, which we call the {\textbackslash}emph\{flood level\}. Our approach makes the loss float around the flood level by doing mini-batched gradient descent as usual but gradient ascent if the training loss is below the flood level. This can be implemented with one line of code and is compatible with any stochastic optimizer and other regularizers. With flooding, the model will continue to "random walk" with the same non-zero training loss, and we expect it to drift into an area with a flat loss landscape that leads to better generalization. We experimentally show that flooding improves performance and, as a byproduct, induces a double descent curve of the test loss.},
  archiveprefix = {arXiv}
}

@misc{izacardFewshotLearningRetrieval2022,
  title = {Few-Shot {{Learning}} with {{Retrieval Augmented Language Models}}},
  author = {Izacard, Gautier and Lewis, Patrick and Lomeli, Maria and Hosseini, Lucas and Petroni, Fabio and Schick, Timo and {Dwivedi-Yu}, Jane and Joulin, Armand and Riedel, Sebastian and Grave, Edouard},
  year = {2022},
  month = aug,
  number = {arXiv:2208.03299},
  eprint = {2208.03299},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2208.03299},
  urldate = {2022-08-24},
  abstract = {Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42\% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3\% despite having 50x fewer parameters.},
  archiveprefix = {arXiv}
}

@misc{jacqminYouFollowMe2022,
  title = {"{{Do}} You Follow Me?": {{A Survey}} of {{Recent Approaches}} in {{Dialogue State Tracking}}},
  shorttitle = {"{{Do}} You Follow Me?},
  author = {Jacqmin, L{\'e}o and {Rojas-Barahona}, Lina M. and Favre, Benoit},
  year = {2022},
  month = jul,
  number = {arXiv:2207.14627},
  eprint = {2207.14627},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2207.14627},
  urldate = {2024-10-01},
  abstract = {While communicating with a user, a task-oriented dialogue system has to track the user's needs at each turn according to the conversation history. This process called dialogue state tracking (DST) is crucial because it directly informs the downstream dialogue policy. DST has received a lot of interest in recent years with the text-to-text paradigm emerging as the favored approach. In this review paper, we first present the task and its associated datasets. Then, considering a large number of recent publications, we identify highlights and advances of research in 2021-2022. Although neural approaches have enabled significant progress, we argue that some critical aspects of dialogue systems such as generalizability are still underexplored. To motivate future studies, we propose several research avenues.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{jainCoarseTuningModelsCode2023,
  title = {Coarse-{{Tuning Models}} of {{Code}} with {{Reinforcement Learning Feedback}}},
  author = {Jain, Abhinav and Adiole, Chima and Chaudhuri, Swarat and Reps, Thomas and Jermaine, Chris},
  year = {2023},
  month = dec,
  number = {arXiv:2305.18341},
  eprint = {2305.18341},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.18341},
  urldate = {2024-02-20},
  abstract = {Large Language Models (LLMs) pre-trained on code have recently emerged as the dominant approach to program synthesis. However, these models are trained using next-token prediction, which ignores the syntax and semantics of code. We propose RLCF, that further trains a pre-trained LLM via reinforcement learning, using feedback from a grounding function that scores the quality of the code. The grounding function uses (i) compiler-derived feedback on whether the code it generates passes a set of correctness checks; and (ii) feedback from a different LLM that compares the generated code to a reference code. RLCF is model- and language-agnostic. We empirically evaluate it on the MBJP and MathQA tasks for Java. Our experiments show that RLCF raises the odds that an LLM-generated program compiles, is executable, and produces the right output on tests, often allowing LLMs to match the performance of 2x-8x larger LLMs.},
  archiveprefix = {arXiv}
}

@inproceedings{jainNoTitleFound2019,
  title = {[{{No}} Title Found]},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North}}},
  author = {Jain, Sarthak and Wallace, Byron C.},
  year = {2019},
  pages = {3543--3556},
  publisher = {Association for Computational Linguistics},
  address = {Minneapolis, Minnesota},
  doi = {10.18653/v1/N19-1357},
  urldate = {2023-01-30},
  abstract = {Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful ``explanations'' for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do.},
  langid = {english}
}

@article{jiangHowCanWe2021,
  title = {How {{Can We Know When Language Models Know}}? {{On}} the {{Calibration}} of {{Language Models}} for {{Question Answering}}},
  shorttitle = {How {{Can We Know When Language Models Know}}?},
  author = {Jiang, Zhengbao and Araki, Jun and Ding, Haibo and Neubig, Graham},
  year = {2021},
  month = sep,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {9},
  pages = {962--977},
  issn = {2307-387X},
  doi = {10.1162/tacl\_a\_00407},
  urldate = {2023-12-20},
  abstract = {Recent works have shown that language models (LM) capture different types of knowledge regarding facts or common sense. However, because no model is perfect, they still fail to provide appropriate answers in many cases. In this paper, we ask the question, ``How can we know when language models know, with confidence, the answer to a particular query?'' We examine this question from the point of view of calibration, the property of a probabilistic model's predicted probabilities actually being well correlated with the probabilities of correctness. We examine three strong generative models---T5, BART, and GPT-2---and study whether their probabilities on QA tasks are well calibrated, finding the answer is a relatively emphatic no. We then examine methods to calibrate such models to make their confidence scores correlate better with the likelihood of correctness through fine-tuning, post-hoc probability modification, or adjustment of the predicted outputs or inputs. Experiments on a diverse range of datasets demonstrate the effectiveness of our methods. We also perform analysis to study the strengths and limitations of these methods, shedding light on further improvements that may be made in methods for calibrating LMs. We have released the code at https://github.com/jzbjyb/lm-calibration.}
}

@misc{jiangPersonaLLMInvestigatingAbility2024,
  title = {{{PersonaLLM}}: {{Investigating}} the {{Ability}} of {{Large Language Models}} to {{Express Personality Traits}}},
  shorttitle = {{{PersonaLLM}}},
  author = {Jiang, Hang and Zhang, Xiajie and Cao, Xubo and Breazeal, Cynthia and Roy, Deb and Kabbara, Jad},
  year = {2024},
  month = apr,
  number = {arXiv:2305.02547},
  eprint = {2305.02547},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.02547},
  urldate = {2024-12-03},
  abstract = {Despite the many use cases for large language models (LLMs) in creating personalized chatbots, there has been limited research on evaluating the extent to which the behaviors of personalized LLMs accurately and consistently reflect specific personality traits. We consider studying the behavior of LLM-based agents which we refer to as LLM personas and present a case study with GPT-3.5 and GPT-4 to investigate whether LLMs can generate content that aligns with their assigned personality profiles. To this end, we simulate distinct LLM personas based on the Big Five personality model, have them complete the 44-item Big Five Inventory (BFI) personality test and a story writing task, and then assess their essays with automatic and human evaluations. Results show that LLM personas' self-reported BFI scores are consistent with their designated personality types, with large effect sizes observed across five traits. Additionally, LLM personas' writings have emerging representative linguistic patterns for personality traits when compared with a human writing corpus. Furthermore, human evaluation shows that humans can perceive some personality traits with an accuracy of up to 80\%. Interestingly, the accuracy drops significantly when the annotators were informed of AI authorship.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction}
}

@misc{jiangPuttingItAll2025,
  title = {Putting {{It All}} into {{Context}}: {{Simplifying Agents}} with {{LCLMs}}},
  shorttitle = {Putting {{It All}} into {{Context}}},
  author = {Jiang, Mingjian and Ruan, Yangjun and Lastras, Luis and Kapanipathi, Pavan and Hashimoto, Tatsunori},
  year = {2025},
  month = may,
  number = {arXiv:2505.08120},
  eprint = {2505.08120},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.08120},
  urldate = {2025-06-04},
  abstract = {Recent advances in language model (LM) agents have demonstrated significant potential for automating complex real-world tasks. To make progress on these difficult tasks, LM agent architectures have become increasingly complex, often incorporating multi-step retrieval tools, multiple agents, and scaffolding adapted to the underlying LM. In this work, we investigate whether all of this complexity is necessary, or if parts of these scaffolds can be removed on challenging tasks like SWE-bench. We show that in the case of SWE-bench, simply putting the entire environment into the context of a long context language model (LCLM) and properly prompting the model makes it competitive with carefully tuned, complex agent scaffolds. We show that a Gemini-1.5-Pro model without any scaffolding or tools achieves 38\% on SWE-Bench-Verified, comparable with approaches using carefully tuned agent scaffolds (32\%). While the unscaffolded approach with Gemini-1.5-Pro falls short of the strongest agentic architectures, we demonstrate that the more capable Gemini-2.5-Pro using the same unscaffolded approach directly attains a 50.8\% solve rate. Additionally, a two-stage approach combining Gemini-1.5-Pro with Claude-3.7 achieves a competitive 48.6\% solve rate.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{jiangSurveyLargeLanguage2024,
  title = {A {{Survey}} on {{Large Language Model Hallucination}} via a {{Creativity Perspective}}},
  author = {Jiang, Xuhui and Tian, Yuxing and Hua, Fengrui and Xu, Chengjin and Wang, Yuanzhuo and Guo, Jian},
  year = {2024},
  month = feb,
  number = {arXiv:2402.06647},
  eprint = {2402.06647},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2402.06647},
  urldate = {2024-05-01},
  abstract = {Hallucinations in large language models (LLMs) are always seen as limitations. However, could they also be a source of creativity? This survey explores this possibility, suggesting that hallucinations may contribute to LLM application by fostering creativity. This survey begins with a review of the taxonomy of hallucinations and their negative impact on LLM reliability in critical applications. Then, through historical examples and recent relevant theories, the survey explores the potential creative benefits of hallucinations in LLMs. To elucidate the value and evaluation criteria of this connection, we delve into the definitions and assessment methods of creativity. Following the framework of divergent and convergent thinking phases, the survey systematically reviews the literature on transforming and harnessing hallucinations for creativity in LLMs. Finally, the survey discusses future research directions, emphasizing the need to further explore and refine the application of hallucinations in creative processes within LLMs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction}
}

@inproceedings{jiaRepresentationJobSkillArtificial2018,
  title = {Representation of {{Job-Skill}} in {{Artificial Intelligence}} with {{Knowledge Graph Analysis}}},
  booktitle = {2018 {{IEEE Symposium}} on {{Product Compliance Engineering}} - {{Asia}} ({{ISPCE-CN}})},
  author = {Jia, Shanshan and Liu, Xiaoan and Zhao, Ping and Liu, Chang and Sun, Lianying and Peng, Tao},
  year = {2018},
  month = dec,
  pages = {1--6},
  doi = {10.1109/ISPCE-CN.2018.8805749},
  abstract = {This study analyses the relationship of different key skills of artificial intelligence (AI) used in the job market. For this, we represent it with a knowledge graph and use a Long Short-term Memory Network to study interactions between these key skills. First, a knowledge graph is build with a rule-based method about these skills in the job markets. Then, the graph is visualized to discover knowledge relationship. Jobs in AI can be classified into two categories: general algorithm jobs and specific focus jobs. Skills of different jobs in AI are very different. Python and Linux are the most necessary key skills for all jobs in AI. Revealing all these key skills in jobs in AI is useful and provide a guideline for job-seekers, companies and universities.}
}

@inproceedings{jiKnowledgeGraphEmbedding2015,
  title = {Knowledge {{Graph Embedding}} via {{Dynamic Mapping Matrix}}},
  booktitle = {Proceedings of the 53rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 7th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Ji, Guoliang and He, Shizhu and Xu, Liheng and Liu, Kang and Zhao, Jun},
  year = {2015},
  month = jul,
  pages = {687--696},
  publisher = {Association for Computational Linguistics},
  address = {Beijing, China},
  doi = {10.3115/v1/P15-1067},
  urldate = {2021-12-07}
}

@article{jinBiomedicalQuestionAnswering2021,
  title = {Biomedical Question Answering: {{A}} Comprehensive Review},
  shorttitle = {Biomedical Question Answering},
  author = {Jin, Qiao and Yuan, Zheng and Xiong, Guangzhi and Yu, Qianlan and Tan, Chuanqi and Chen, Mosha and Huang, Songfang and Liu, Xiaozhong and Yu, Sheng},
  year = {2021},
  journal = {arXiv preprint arXiv:2102.05281},
  eprint = {2102.05281},
  archiveprefix = {arXiv}
}

@inproceedings{jinCogKGEKnowledgeGraph2022,
  title = {{{CogKGE}}: {{A Knowledge Graph Embedding Toolkit}} and {{Benchmark}} for {{Representing Multi-source}} and {{Heterogeneous Knowledge}}},
  shorttitle = {{{CogKGE}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{System Demonstrations}}},
  author = {Jin, Zhuoran and Men, Tianyi and Yuan, Hongbang and He, Zhitao and Sui, Dianbo and Wang, Chenhao and Xue, Zhipeng and Chen, Yubo and Zhao, Jun},
  year = {2022},
  month = may,
  pages = {166--173},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-demo.16},
  urldate = {2022-07-20},
  abstract = {In this paper, we propose CogKGE, a knowledge graph embedding (KGE) toolkit, which aims to represent multi-source and heterogeneous knowledge. For multi-source knowledge, unlike existing methods that mainly focus on entity-centric knowledge, CogKGE also supports the representations of event-centric, commonsense and linguistic knowledge. For heterogeneous knowledge, besides structured triple facts, CogKGE leverages additional unstructured information, such as text descriptions, node types and temporal information, to enhance the meaning of embeddings. Designing CogKGE aims to provide a unified programming framework for KGE tasks and a series of knowledge representations for downstream tasks. As a research framework, CogKGE consists of five parts, including core, data, model, knowledge and adapter module. As a knowledge discovery toolkit, CogKGE provides pre-trained embedders to discover new facts, cluster entities and check facts. Furthermore, we construct two benchmark datasets for further research on multi-source heterogeneous KGE tasks: EventKG240K and CogNet360K. We also release an online system to discover knowledge visually. Source code, datasets and pre-trained embeddings are publicly available at GitHub, with a short instruction video.}
}

@article{jinColdstartActiveLearning2022,
  title = {Cold-Start Active Learning for Image Classification},
  author = {Jin, Qiuye and Yuan, Mingzhi and Li, Shiman and Wang, Haoran and Wang, Manning and Song, Zhijian},
  year = {2022},
  month = nov,
  journal = {Information Sciences},
  volume = {616},
  pages = {16--36},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2022.10.066},
  urldate = {2023-12-15},
  abstract = {Active learning (AL) aims to select valuable samples for labeling from an unlabeled sample pool to build a training dataset with minimal annotation cost. Traditional methods always require partially and initially labeled samples to start active selection and then query annotations of samples incrementally through several iterations. However, this scheme is not effective in the deep learning scenario. On the one hand, initially labeled sample sets are not always available in the beginning. On the other hand, the performance of the traditional model is usually poor in the early iterations due to limited training feedback. For the first time, we propose a cold-start AL model based on representative (CALR) sampling, which selects valuable samples without the need for an initial labeled set or the iterative feedback of the target models. Experiments on three image classification datasets, CIFAR-10, CIFAR-100 and Caltech-256, showed that CALR achieved a new state-of-the-art performance of AL in cold-start settings. Especially in low annotation budget conditions, our method can achieve up to a 10\% performance increase compared to traditional methods. Furthermore, CALR can be combined with warm-start methods to improve the start-up efficiency while further breaking the performance ceiling of AL, which makes CALR have a broader application scenario.}
}

@article{jinWhatDiseaseDoes2020,
  title = {What {{Disease}} Does This {{Patient Have}}? {{A Large-scale Open Domain Question Answering Dataset}} from {{Medical Exams}}},
  shorttitle = {What {{Disease}} Does This {{Patient Have}}?},
  author = {Jin, Di and Pan, Eileen and Oufattole, Nassim and Weng, Wei-Hung and Fang, Hanyi and Szolovits, Peter},
  year = {2020},
  month = sep,
  journal = {arXiv:2009.13081 [cs]},
  eprint = {2009.13081},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2009.13081},
  urldate = {2021-11-30},
  abstract = {Open domain question answering (OpenQA) tasks have been recently attracting more and more attention from the natural language processing (NLP) community. In this work, we present the first free-form multiple-choice OpenQA dataset for solving medical problems, MedQA, collected from the professional medical board exams. It covers three languages: English, simplified Chinese, and traditional Chinese, and contains 12,723, 34,251, and 14,123 questions for the three languages, respectively. We implement both rule-based and popular neural methods by sequentially combining a document retriever and a machine comprehension model. Through experiments, we find that even the current best method can only achieve 36.7{\textbackslash}\%, 42.0{\textbackslash}\%, and 70.1{\textbackslash}\% of test accuracy on the English, traditional Chinese, and simplified Chinese questions, respectively. We expect MedQA to present great challenges to existing OpenQA systems and hope that it can serve as a platform to promote much stronger OpenQA models from the NLP community in the future.},
  archiveprefix = {arXiv},
  langid = {english}
}

@misc{jinWhatDiseaseDoes2020a,
  title = {What {{Disease}} Does This {{Patient Have}}? {{A Large-scale Open Domain Question Answering Dataset}} from {{Medical Exams}}},
  shorttitle = {What {{Disease}} Does This {{Patient Have}}?},
  author = {Jin, Di and Pan, Eileen and Oufattole, Nassim and Weng, Wei-Hung and Fang, Hanyi and Szolovits, Peter},
  year = {2020},
  month = sep,
  number = {arXiv:2009.13081},
  eprint = {2009.13081},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2009.13081},
  urldate = {2023-10-26},
  abstract = {Open domain question answering (OpenQA) tasks have been recently attracting more and more attention from the natural language processing (NLP) community. In this work, we present the first free-form multiple-choice OpenQA dataset for solving medical problems, MedQA, collected from the professional medical board exams. It covers three languages: English, simplified Chinese, and traditional Chinese, and contains 12,723, 34,251, and 14,123 questions for the three languages, respectively. We implement both rule-based and popular neural methods by sequentially combining a document retriever and a machine comprehension model. Through experiments, we find that even the current best method can only achieve 36.7{\textbackslash}\%, 42.0{\textbackslash}\%, and 70.1{\textbackslash}\% of test accuracy on the English, traditional Chinese, and simplified Chinese questions, respectively. We expect MedQA to present great challenges to existing OpenQA systems and hope that it can serve as a platform to promote much stronger OpenQA models from the NLP community in the future.},
  archiveprefix = {arXiv}
}

@misc{jinWhatDiseaseDoes2021,
  title = {What {{Disease}} Does This {{Patient Have}}? {{A Large-scale Open Domain Question Answering Dataset}} from {{Medical Exams}}},
  shorttitle = {What {{Disease}} Does This {{Patient Have}}?},
  author = {Jin, Di and Pan, Eileen and Oufattole, Nassim and Weng, Wei-Hung and Fang, Hanyi and Szolovits, Peter},
  year = {2021},
  month = may,
  publisher = {MATHEMATICS \& COMPUTER SCIENCE},
  doi = {10.20944/preprints202105.0498.v1},
  urldate = {2023-10-26},
  abstract = {Open domain question answering (OpenQA) tasks have been recently attracting more and more attention from the natural language processing (NLP) community. In this work, we present the first free-form multiple-choice OpenQA dataset for solving medical problems, MedQA, collected from the professional medical board exams. It covers three languages: English, simplified Chinese, and traditional Chinese, and contains 12,723, 34,251, and 14,123 questions for the three languages, respectively. We implement both rule-based and popular neural methods by sequentially combining a document retriever and a machine comprehension model. Through experiments, we find that even the current best method can only achieve 36.7\%, 42.0\%, and 70.1\% of test accuracy on the English, traditional Chinese, and simplified Chinese questions, respectively. We expect MedQA to present great challenges to existing OpenQA systems and hope that it can serve as a platform to promote much stronger OpenQA models from the NLP community in the future.},
  archiveprefix = {MATHEMATICS \& COMPUTER SCIENCE}
}

@article{joySourceCodePlagiarism2010,
  title = {Source Code Plagiarism---a Student Perspective},
  author = {Joy, Mike and Cosma, Georgina and Yau, Jane Yin-Kim and Sinclair, Jane},
  year = {2010},
  journal = {IEEE Transactions on Education},
  volume = {54},
  number = {1},
  pages = {125--132},
  publisher = {IEEE},
  doi = {10.1109/TE.2010.2046664}
}

@article{kaasaEORTCCoreQuality1995,
  title = {The {{EORTC}} Core Quality of Life Questionnaire ({{QLQ-C30}}): Validity and Reliability When Analysed with Patients Treated with Palliative Radiotherapy},
  shorttitle = {The {{EORTC}} Core Quality of Life Questionnaire ({{QLQ-C30}})},
  author = {Kaasa, S. and Bjordal, K. and Aaronson, N. and Moum, T. and Wist, E. and Hagen, S. and Kvikstad, A.},
  year = {1995},
  month = dec,
  journal = {European Journal of Cancer},
  volume = {31},
  number = {13-14},
  pages = {2260--2263},
  issn = {09598049},
  doi = {10.1016/0959-8049(95)00296-0},
  abstract = {The EORTC Core Quality of Life questionnaire (EORTC QLQ-C30) is designed to measure cancer patients' physical, psychological and social functions. The questionnaire is composed of multi-item scales and single items. 247 patients completed the EORTC QLQ-C30 before palliative radiotherapy and 181 after palliative radiotherapy. The questionnaire was well accepted with a high completion rate in the present patient population consisting of advanced cancer patients with short life expectancy. In addition, the questionnaire was found to be useful to detect the effect of palliative radiotherapy over time. The scale reliability was excellent for all scales except the role functioning scale. Excellent criterion validity was found for the emotional functioning scale where it was correlated with GHQ-20. Performance of the questionnaire was improved after the second evaluation as compared with the first. The present study shows that the EORTC QLQ-C30 is found to be practical and valid in measuring quality of life in patients with advanced disease.},
  langid = {american},
  pmid = {8652253}
}

@inproceedings{kaddariBiomedicalQuestionAnswering2020,
  title = {Biomedical Question Answering: {{A}} Survey of Methods and Datasets},
  shorttitle = {Biomedical Question Answering},
  booktitle = {2020 {{Fourth International Conference On Intelligent Computing}} in {{Data Sciences}} ({{ICDS}})},
  author = {Kaddari, Zakaria and Mellah, Youssef and Berrich, Jamal and Bouchentouf, Toumi and Belkasmi, Mohammed G.},
  year = {2020},
  pages = {1--8},
  publisher = {IEEE},
  doi = {10.1109/icds50568.2020.9268742}
}

@inproceedings{kadlecTextUnderstandingAttention2016,
  title = {Text {{Understanding}} with the {{Attention Sum Reader Network}}},
  booktitle = {Proceedings of the 54th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Kadlec, Rudolf and Schmid, Martin and Bajgar, Ondrej and Kleindienst, Jan},
  year = {2016},
  pages = {908--918},
  publisher = {Association for Computational Linguistics},
  address = {Berlin, Germany},
  doi = {10.18653/v1/P16-1086},
  urldate = {2022-01-09},
  annotation = {00295}
}

@misc{kagayaRAPRetrievalAugmentedPlanning2024,
  title = {{{RAP}}: {{Retrieval-Augmented Planning}} with {{Contextual Memory}} for {{Multimodal LLM Agents}}},
  shorttitle = {{{RAP}}},
  author = {Kagaya, Tomoyuki and Yuan, Thong Jing and Lou, Yuxuan and Karlekar, Jayashree and Pranata, Sugiri and Kinose, Akira and Oguri, Koki and Wick, Felix and You, Yang},
  year = {2024},
  month = feb,
  number = {arXiv:2402.03610},
  eprint = {2402.03610},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.03610},
  urldate = {2024-07-15},
  abstract = {Owing to recent advancements, Large Language Models (LLMs) can now be deployed as agents for increasingly complex decision-making applications in areas including robotics, gaming, and API integration. However, reflecting past experiences in current decision-making processes, an innate human behavior, continues to pose significant challenges. Addressing this, we propose Retrieval-Augmented Planning (RAP) framework, designed to dynamically leverage past experiences corresponding to the current situation and context, thereby enhancing agents' planning capabilities. RAP distinguishes itself by being versatile: it excels in both text-only and multimodal environments, making it suitable for a wide range of tasks. Empirical evaluations demonstrate RAP's effectiveness, where it achieves SOTA performance in textual scenarios and notably enhances multimodal LLM agents' performance for embodied tasks. These results highlight RAP's potential in advancing the functionality and applicability of LLM agents in complex, real-world applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@book{kahnemanThinkingFastSlow2013,
  title = {Thinking, Fast and Slow},
  author = {Kahneman, Daniel},
  year = {2013},
  series = {Psychology/Economics},
  edition = {First paperback edition},
  publisher = {{Farrar, Straus and Giroux}},
  address = {New York},
  isbn = {978-0-374-53355-7},
  langid = {english}
}

@inproceedings{kalaiCalibratedLanguageModels2024,
  title = {Calibrated {{Language Models Must Hallucinate}}},
  booktitle = {Proceedings of the 56th {{Annual ACM Symposium}} on {{Theory}} of {{Computing}}},
  author = {Kalai, Adam Tauman and Vempala, Santosh S.},
  year = {2024},
  month = jun,
  pages = {160--171},
  publisher = {ACM},
  address = {Vancouver BC Canada},
  doi = {10.1145/3618260.3649777},
  urldate = {2024-09-12},
  isbn = {979-8-4007-0383-6},
  langid = {english}
}

@misc{kaplanScalingLawsNeural2020,
  title = {Scaling {{Laws}} for {{Neural Language Models}}},
  author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  year = {2020},
  month = jan,
  number = {arXiv:2001.08361},
  eprint = {2001.08361},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2001.08361},
  urldate = {2023-05-19},
  abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
  archiveprefix = {arXiv}
}

@misc{kapoorOmniACTDatasetBenchmark2024,
  title = {{{OmniACT}}: {{A Dataset}} and {{Benchmark}} for {{Enabling Multimodal Generalist Autonomous Agents}} for {{Desktop}} and {{Web}}},
  shorttitle = {{{OmniACT}}},
  author = {Kapoor, Raghav and Butala, Yash Parag and Russak, Melisa and Koh, Jing Yu and Kamble, Kiran and Alshikh, Waseem and Salakhutdinov, Ruslan},
  year = {2024},
  month = jul,
  number = {arXiv:2402.17553},
  eprint = {2402.17553},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.17553},
  urldate = {2024-08-08},
  abstract = {For decades, human-computer interaction has fundamentally been manual. Even today, almost all productive work done on the computer necessitates human input at every step. Autonomous virtual agents represent an exciting step in automating many of these menial tasks. Virtual agents would empower users with limited technical proficiency to harness the full possibilities of computer systems. They could also enable the efficient streamlining of numerous computer tasks, ranging from calendar management to complex travel bookings, with minimal human intervention. In this paper, we introduce OmniACT, the first-of-a-kind dataset and benchmark for assessing an agent's capability to generate executable programs to accomplish computer tasks. Our scope extends beyond traditional web automation, covering a diverse range of desktop applications. The dataset consists of fundamental tasks such as "Play the next song", as well as longer horizon tasks such as "Send an email to John Doe mentioning the time and place to meet". Specifically, given a pair of screen image and a visually-grounded natural language task, the goal is to generate a script capable of fully executing the task. We run several strong baseline language model agents on our benchmark. The strongest baseline, GPT-4, performs the best on our benchmark However, its performance level still reaches only 15\% of the human proficiency in generating executable scripts capable of completing the task, demonstrating the challenge of our task for conventional web agents. Our benchmark provides a platform to measure and evaluate the progress of language model agents in automating computer tasks and motivates future work towards building multimodal models that bridge large language models and the visual grounding of computer screens.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Human-Computer Interaction}
}

@article{karabacakEmbracingLargeLanguage2023,
  title = {Embracing {{Large Language Models}} for {{Medical Applications}}: {{Opportunities}} and {{Challenges}}},
  shorttitle = {Embracing {{Large Language Models}} for {{Medical Applications}}},
  author = {Karabacak, Mert and Margetis, Konstantinos},
  year = {2023},
  month = may,
  journal = {Cureus},
  issn = {2168-8184},
  doi = {10.7759/cureus.39305},
  urldate = {2024-06-11},
  abstract = {Large language models (LLMs) have the potential to revolutionize the field of medicine by, among other applications, improving diagnostic accuracy and supporting clinical decision-making. However, the successful integration of LLMs in medicine requires addressing challenges and considerations specific to the medical domain. This viewpoint article provides a comprehensive overview of key aspects for the successful implementation of LLMs in medicine, including transfer learning, domain-specific fine-tuning, domain adaptation, reinforcement learning with expert input, dynamic training, interdisciplinary collaboration, education and training, evaluation metrics, clinical validation, ethical considerations, data privacy, and regulatory frameworks. By adopting a multifaceted approach and fostering interdisciplinary collaboration, LLMs can be developed, validated, and integrated into medical practice responsibly, effectively, and ethically, addressing the needs of various medical disciplines and diverse patient populations. Ultimately, this approach will ensure that LLMs enhance patient care and improve overall health outcomes for all.},
  langid = {english}
}

@article{karimiEffectPriorKnowledge2015,
  title = {The Effect of Prior Knowledge and Decision-Making Style on the Online Purchase Decision-Making Process: {{A}} Typology of Consumer Shopping Behaviour},
  shorttitle = {The Effect of Prior Knowledge and Decision-Making Style on the Online Purchase Decision-Making Process},
  author = {Karimi, Sahar and Papamichail, K. Nadia and Holland, Christopher P.},
  year = {2015},
  month = sep,
  journal = {Decision Support Systems},
  volume = {77},
  pages = {137--147},
  issn = {0167-9236},
  doi = {10.1016/j.dss.2015.06.004},
  urldate = {2025-03-20},
  abstract = {This paper provides an empirical typology of online decision-making purchasing behaviour. The study explores how the online purchase process is affected by individual decision-making style and knowledge of product. Drawing from the decision analysis and consumer behaviour literatures, we present a typology of online purchase decision-making behaviour and introduce four archetypes of online consumers. A number of experiments have been conducted in two online settings: retail banking and mobile networks. Based on an extensive video analysis, we have captured four process-related dimensions (number of cycles, duration, number of alternatives and number of criteria) using a business process modelling approach. Significant differences in all process-related dimensions were found across the four archetypes. The study improves understanding of the different types of online consumers and their process outcomes. The findings are useful for online retailers seeking to improve the way they support the four archetypes of online shoppers throughout the decision-making purchasing process.},
  keywords = {B2C E-commerce,Online purchase decision-making processes,Online shopping}
}

@inproceedings{kasaiLowresourceDeepEntity2019,
  title = {Low-Resource {{Deep Entity Resolution}} with {{Transfer}} and {{Active Learning}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Kasai, Jungo and Qian, Kun and Gurajada, Sairam and Li, Yunyao and Popa, Lucian},
  editor = {Korhonen, Anna and Traum, David and M{\`a}rquez, Llu{\'i}s},
  year = {2019},
  month = jul,
  pages = {5851--5861},
  publisher = {Association for Computational Linguistics},
  address = {Florence, Italy},
  doi = {10.18653/v1/P19-1586},
  urldate = {2023-11-13},
  abstract = {Entity resolution (ER) is the task of identifying different representations of the same real-world entities across databases. It is a key step for knowledge base creation and text mining. Recent adaptation of deep learning methods for ER mitigates the need for dataset-specific feature engineering by constructing distributed representations of entity records. While these methods achieve state-of-the-art performance over benchmark data, they require large amounts of labeled data, which are typically unavailable in realistic ER applications. In this paper, we develop a deep learning-based method that targets low-resource settings for ER through a novel combination of transfer learning and active learning. We design an architecture that allows us to learn a transferable model from a high-resource setting to a low-resource one. To further adapt to the target dataset, we incorporate active learning that carefully selects a few informative examples to fine-tune the transferred model. Empirical evaluation demonstrates that our method achieves comparable, if not better, performance compared to state-of-the-art learning-based methods while using an order of magnitude fewer labels.}
}

@misc{KDDCup2024,
  title = {{{KDD Cup}} 2024 {{Workshop}}: {{A Multi-task Online Shopping Challenge}} for {{Large Language Models}}},
  url = {https://amazon-kddcup24.github.io//},
  urldate = {2025-03-15}
}

@inproceedings{kePretrainingMetaLearning2021,
  title = {Pre-Training with {{Meta Learning}} for {{Chinese Word Segmentation}}},
  booktitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Ke, Zhen and Shi, Liang and Sun, Songtao and Meng, Erli and Wang, Bin and Qiu, Xipeng},
  year = {2021},
  pages = {5514--5523},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.naacl-main.436},
  urldate = {2022-06-09},
  abstract = {Recent researches show that pre-trained models (PTMs) are beneficial to Chinese Word Segmentation (CWS). However, PTMs used in previous works usually adopt language modeling as pre-training tasks, lacking task-specific prior segmentation knowledge and ignoring the discrepancy between pre-training tasks and downstream CWS tasks. In this paper, we propose a CWS-specific pre-trained model MetaSeg, which employs a unified architecture and incorporates meta learning algorithm into a multi-criteria pre-training task. Empirical results show that MetaSeg could utilize common prior segmentation knowledge from different existing criteria and alleviate the discrepancy between pre-trained models and downstream CWS tasks. Besides, MetaSeg can achieve new state-of-the-art performance on twelve widely-used CWS datasets and significantly improve model performance in low-resource settings.}
}

@article{kerasidouTrustRelianceMedical2022,
  title = {Before and beyond Trust: Reliance in Medical {{AI}}},
  shorttitle = {Before and beyond Trust},
  author = {Kerasidou, Charalampia (Xaroula) and Kerasidou, Angeliki and Buscher, Monika and Wilkinson, Stephen},
  year = {2022},
  month = nov,
  journal = {Journal of Medical Ethics},
  volume = {48},
  number = {11},
  pages = {852--856},
  issn = {0306-6800, 1473-4257},
  doi = {10.1136/medethics-2020-107095},
  urldate = {2024-05-02},
  abstract = {Artificial intelligence (AI) is changing healthcare and the practice of medicine as data-driven science and machine-learning technologies, in particular, are contributing to a variety of medical and clinical tasks. Such advancements have also raised many questions, especially about public trust. As a response to these concerns there has been a concentrated effort from public bodies, policy-makers and technology companies leading the way in AI to address what is identified as a "public trust deficit". This paper argues that a focus on trust as the basis upon which a relationship between this new technology and the public is built is, at best, ineffective, at worst, inappropriate or even dangerous, as it diverts attention from what is actually needed to actively warrant trust. Instead of agonising about how to facilitate trust, a type of relationship which can leave those trusting vulnerable and exposed, we argue that efforts should be focused on the difficult and dynamic process of ensuring reliance underwritten by strong legal and regulatory frameworks. From there, trust could emerge but not merely as a means to an end. Instead, as something to work in practice towards; that is, the deserved result of an ongoing ethical relationship where there is the appropriate, enforceable and reliable regulatory infrastructure in place for problems, challenges and power asymmetries to be continuously accounted for and appropriately redressed.},
  langid = {english}
}

@inproceedings{keyProceedCareReimagining2021,
  title = {Proceed with {{Care}}: {{Reimagining Home IoT Through}} a {{Care Perspective}}},
  shorttitle = {Proceed with {{Care}}},
  booktitle = {Proceedings of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Key, Cayla and Browne, Fiona and Taylor, Nick and Rogers, Jon},
  year = {2021},
  month = may,
  series = {{{CHI}} '21},
  pages = {1--15},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3411764.3445602},
  urldate = {2023-08-31},
  abstract = {As the internet is increasingly embedded in the everyday things in our homes, we notice a need for greater focus on the role care plays in those relationships---and therefore an opportunity to realize unseen potential in reimagining home Internet of Things (IoT). In this paper we report on our inquiry of home dwellers' relationships to caring for their everyday things and homes (referred to as thingcare). Findings from our design ethnography reveal four thematic qualities of their relationships to thingcare: Care Spectacle, Care Liminality, Ontological Braiding, and Care Condition. Using these themes as touchstones, we co-speculated to produce four speculative IoT concepts to explore what care as a design ethic might look like for IoT and reflect on nascent opportunities and challenges for domestic IoT design. We conclude by considering structures of power and privilege embedded within care practices that critically open new design imaginaries for IoT.},
  isbn = {978-1-4503-8096-6}
}

@inproceedings{kimAutoIntentAutomatedIntent2024,
  title = {Auto-{{Intent}}: {{Automated Intent Discovery}} and {{Self-Exploration}} for {{Large Language Model Web Agents}}},
  shorttitle = {Auto-{{Intent}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2024},
  author = {Kim, Jaekyeom and Kim, Dong-Ki and Logeswaran, Lajanugen and Sohn, Sungryull and Lee, Honglak},
  editor = {{Al-Onaizan}, Yaser and Bansal, Mohit and Chen, Yun-Nung},
  year = {2024},
  month = nov,
  pages = {16531--16541},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.findings-emnlp.964},
  urldate = {2024-12-11},
  abstract = {In this paper, we introduce Auto-Intent, a method to adapt a pre-trained large language model (LLM) as an agent for a target domain without direct fine-tuning, where we empirically focus on web navigation tasks. Our approach first discovers the underlying intents from target domain demonstrations unsupervisedly, in a highly compact form (up to three words). With the extracted intents, we train our intent predictor to predict the next intent given the agent's past observations and actions. In particular, we propose a self-exploration approach where top-k probable intent predictions are provided as a hint to the pre-trained LLM agent, which leads to enhanced decision-making capabilities. Auto-Intent substantially improves the performance of GPT-3.5, 4 and Llama-3.1-70B, 405B agents on the large-scale real-website navigation benchmarks from Mind2Web and online navigation tasks from WebArena with its cross-benchmark generalization from Mind2Web.}
}

@inproceedings{kimBridgingLanguageGap2024,
  title = {Bridging the {{Language Gap}}: {{Domain-Specific Dataset Construction}} for {{Medical LLMs}}},
  shorttitle = {Bridging the {{Language Gap}}},
  booktitle = {Generalizing from {{Limited Resources}} in the {{Open World}}},
  author = {Kim, Chae Yeon and Kim, Song Yeon and Cho, Seung Hwan and Kim, Young-Min},
  editor = {Guo, Jinyang and Ma, Yuqing and Ding, Yifu and Gong, Ruihao and Zheng, Xingyu and He, Changyi and Lu, Yantao and Liu, Xianglong},
  year = {2024},
  pages = {134--146},
  publisher = {Springer Nature},
  address = {Singapore},
  doi = {10.1007/978-981-97-6125-8\_11},
  abstract = {The advent of large language models (LLMs) has transformed the field of natural language processing (NLP), demonstrating impressive capabilities across a variety of tasks such as text generation, translation, and question answering. However, their effectiveness in specialized domains is constrained by the lack of domain-specific data. This paper presents an effective methodology for constructing domain-specific datasets using domain-specific corpora, thus overcoming the challenges posed by linguistic and cultural differences in non-English-speaking regions. By leveraging mining techniques, this methodology facilitates the construction of datasets tailored to local languages and cultures. A Korean medical corpus served as the foundation for dataset construction, leading to the development of a medical language model that demonstrated high performance and versatility across various NLP tasks. A bidirectional encoder representation from transformer-based comparative analysis revealed comparable performance. The objective is to streamline LLM applications across diverse domains, thereby enhancing language model efficiency. In the future, our efforts will be directed towards implementing the proposed methodology across diverse domains and investigating strategies for extracting domain-specific tasks and vocabulary to enhance the quality of domain datasets.},
  isbn = {978-981-97-6125-8},
  langid = {english},
  keywords = {Domain Dataset,Large Language Model,Mining}
}

@misc{kimHealthLLMLargeLanguage2024,
  title = {Health-{{LLM}}: {{Large Language Models}} for {{Health Prediction}} via {{Wearable Sensor Data}}},
  shorttitle = {Health-{{LLM}}},
  author = {Kim, Yubin and Xu, Xuhai and McDuff, Daniel and Breazeal, Cynthia and Park, Hae Won},
  year = {2024},
  month = jan,
  number = {arXiv:2401.06866},
  eprint = {2401.06866},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2401.06866},
  urldate = {2024-01-25},
  abstract = {Large language models (LLMs) are capable of many natural language tasks, yet they are far from perfect. In health applications, grounding and interpreting domain-specific and non-linguistic data is important. This paper investigates the capacity of LLMs to deliver multi-modal health predictions based on contextual information (e.g. user demographics, health knowledge) and physiological data (e.g. resting heart rate, sleep minutes). We present a comprehensive evaluation of eight state-of-the-art LLMs with diverse prompting and fine-tuning techniques on six public health datasets (PM-Data, LifeSnaps, GLOBEM, AW\_FB, MIT-BIH \& MIMIC-III). Our experiments cover thirteen consumer health prediction tasks in mental health, activity, metabolic, sleep, and cardiac assessment. Our fine-tuned model, Health-Alpaca exhibits comparable performance to larger models (GPT-3.5 and GPT-4), achieving the best performance in 5 out of 13 tasks. Ablation studies highlight the effectiveness of context enhancement strategies, and generalization capability of the fine-tuned models across training datasets and the size of training samples. Notably, we observe that our context enhancement can yield up to 23.8\% improvement in performance. While constructing contextually rich prompts (combining user context, health knowledge and temporal information) exhibits synergistic improvement, the inclusion of health knowledge context in prompts significantly enhances overall performance.},
  archiveprefix = {arXiv}
}

@misc{kimLanguageModelsCan2023,
  title = {Language {{Models}} Can {{Solve Computer Tasks}}},
  author = {Kim, Geunwoo and Baldi, Pierre and McAleer, Stephen},
  year = {2023},
  month = nov,
  number = {arXiv:2303.17491},
  eprint = {2303.17491},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.17491},
  urldate = {2024-07-15},
  abstract = {Agents capable of carrying out general tasks on a computer can improve efficiency and productivity by automating repetitive tasks and assisting in complex problem-solving. Ideally, such agents should be able to solve new computer tasks presented to them through natural language commands. However, previous approaches to this problem require large amounts of expert demonstrations and task-specific reward functions, both of which are impractical for new tasks. In this work, we show that a pre-trained large language model (LLM) agent can execute computer tasks guided by natural language using a simple prompting scheme where the agent Recursively Criticizes and Improves its output (RCI). The RCI approach significantly outperforms existing LLM methods for automating computer tasks and surpasses supervised learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++ benchmark. We compare multiple LLMs and find that RCI with the InstructGPT-3+RLHF LLM is state-of-the-art on MiniWoB++, using only a handful of demonstrations per task rather than tens of thousands, and without a task-specific reward function. Furthermore, we demonstrate RCI prompting's effectiveness in enhancing LLMs' reasoning abilities on a suite of natural language reasoning tasks, outperforming chain of thought (CoT) prompting with external feedback. We find that RCI combined with CoT performs better than either separately. Our code can be found here: https://github.com/posgnu/rci-agent.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning}
}

@inproceedings{kimRaDARetrievalaugmentedWeb2024,
  title = {{{RaDA}}: {{Retrieval-augmented Web Agent Planning}} with {{LLMs}}},
  shorttitle = {{{RaDA}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2024},
  author = {Kim, Minsoo and Bursztyn, Victor and Koh, Eunyee and Guo, Shunan and Hwang, Seung-won},
  editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
  year = {2024},
  month = aug,
  pages = {13511--13525},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.findings-acl.802},
  urldate = {2024-12-11},
  abstract = {Agents powered by large language models (LLMs) inherit important limitations, such as the restricted context length, dependency on human-engineered exemplars (e.g., for task decomposition), and insufficient generalization. To address these challenges, we propose RaDA, a novel planning method for Web agents that does not require manual exemplars, efficiently leverages the LLMs' context, and enhances generalization. RaDA disentangles planning into two stages: for a new given task, during Retrieval-augmented Task Decomposition (RaD), it decomposes tasks into high-level subtasks; next, during Retrieval-augmented Action Generation (RaA), it traverses the trajectory obtained with RaD to iteratively synthesize actions based on dynamically retrieved exemplars. We compare RaDA with strong baselines covering a broad space of design choices, using both GPT-3.5 and GPT-4 as backbones; and we find consistent improvements over previous SOTA in two challenging benchmarks, CompWoB and Mind2Web, covering settings with different complexities. We show the contributions of RaDA via ablation studies and qualitative analysis; and we discuss the structural benefits of our more compositional design.}
}

@misc{kimSelfGeneratedInContextLearning2022,
  title = {Self-{{Generated In-Context Learning}}: {{Leveraging Auto-regressive Language Models}} as a {{Demonstration Generator}}},
  shorttitle = {Self-{{Generated In-Context Learning}}},
  author = {Kim, Hyuhng Joon and Cho, Hyunsoo and Kim, Junyeob and Kim, Taeuk and Yoo, Kang Min and Lee, Sang-goo},
  year = {2022},
  month = jun,
  number = {arXiv:2206.08082},
  eprint = {2206.08082},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2206.08082},
  urldate = {2023-11-14},
  abstract = {Large-scale pre-trained language models (PLMs) are well-known for being capable of solving a task simply by conditioning a few input-label pairs dubbed demonstrations on a prompt without being explicitly tuned for the desired downstream task. Such a process (i.e., in-context learning), however, naturally leads to high reliance on the demonstrations which are usually selected from external datasets. In this paper, we propose self-generated in-context learning (SG-ICL), which generates demonstrations for in-context learning from PLM itself to minimize the reliance on the external demonstration. We conduct experiments on four different text classification tasks and show SG-ICL significantly outperforms zero-shot learning and is generally worth approximately 0.6 gold training samples. Moreover, our generated demonstrations show more consistent performance with low variance compared to randomly selected demonstrations from the training dataset.},
  archiveprefix = {arXiv}
}

@book{kirwanGuideTaskAnalysis2014,
  title = {A {{Guide To Task Analysis}}: {{The Task Analysis Working Group}}},
  shorttitle = {A {{Guide To Task Analysis}}},
  editor = {Kirwan, B. and Ainsworth, L. K.},
  year = {2014},
  month = may,
  publisher = {CRC Press},
  address = {London},
  doi = {10.1201/b16826},
  abstract = {This work shows readers how to target task analysis TA resources effectively over the life cycle of a project from conceptual design Through To Systems Operation, Noting The Role Of TA In Safety And Quality assurance, minimizing operator error,},
  isbn = {978-0-429-17358-5}
}

@inproceedings{knearemExploringFutureDesign2023,
  title = {Exploring the Future of Design Tooling: {{The}} Role of Artificial Intelligence in Tools for User Experience Professionals},
  shorttitle = {Exploring the Future of Design Tooling},
  booktitle = {Extended {{Abstracts}} of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Knearem, Tiffany and Khwaja, Mohammed and Gao, Yuling and Bentley, Frank and {Kliman-Silver}, Clara E},
  year = {2023},
  month = apr,
  series = {{{CHI EA}} '23},
  pages = {1--6},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3544549.3573874},
  urldate = {2024-09-13},
  abstract = {Recently, artificial intelligence (AI) has been introduced into a variety of consumer applications for creative work. Although AI-driven features in design tooling are nascent, there is growing interest in utilizing AI to support user experience (UX) workflows. In this case study, we surveyed industry UX professionals to understand how they perceive AI-driven assists in their tools, their concerns about accepting AI in design tools and which design-related workflows could be promising for future research. Our results suggest that UX professionals are overall positive about AI-driven features in design tools; looking to AI as a creative partner to iterate with and as an assistant with mundane tasks. We offer practical directions for the future of AI in UX tooling, but caution against developing tools that do not sufficiently address UX professionals' concerns around bias and trust.},
  isbn = {978-1-4503-9422-2}
}

@article{kociskyNarrativeQAReadingComprehension2017,
  title = {The {{NarrativeQA Reading Comprehension Challenge}}},
  author = {Ko{\v c}isk{\'y}, Tom{\'a}{\v s} and Schwarz, Jonathan and Blunsom, Phil and Dyer, Chris and Hermann, Karl Moritz and Melis, G{\'a}bor and Grefenstette, Edward},
  year = {2017},
  month = dec,
  eprint = {1712.07040},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1712.07040},
  urldate = {2023-01-20},
  abstract = {Reading comprehension (RC)---in contrast to information retrieval---requires integrating information and reasoning about events, entities, and their relations across a full document. Question answering is conventionally used to assess RC ability, in both artificial agents and children learning to read. However, existing RC datasets and tasks are dominated by questions that can be solved by selecting answers using superficial information (e.g., local context similarity or global term frequency); they thus fail to test for the essential integrative aspect of RC. To encourage progress on deeper comprehension of language, we present a new dataset and set of tasks in which the reader must answer questions about stories by reading entire books or movie scripts. These tasks are designed so that successfully answering their questions requires understanding the underlying narrative rather than relying on shallow pattern matching or salience. We show that although humans solve the tasks easily, standard RC models struggle on the tasks presented here. We provide an analysis of the dataset and the challenges it presents.},
  archiveprefix = {arXiv}
}

@inproceedings{kohVisualWebArenaEvaluatingMultimodal2024,
  title = {{{VisualWebArena}}: {{Evaluating Multimodal Agents}} on {{Realistic Visual Web Tasks}}},
  shorttitle = {{{VisualWebArena}}},
  booktitle = {Proceedings of the 62nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Koh, Jing Yu and Lo, Robert and Jang, Lawrence and Duvvur, Vikram and Lim, Ming and Huang, Po-Yu and Neubig, Graham and Zhou, Shuyan and Salakhutdinov, Russ and Fried, Daniel},
  editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
  year = {2024},
  month = aug,
  pages = {881--905},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.50},
  urldate = {2024-12-11},
  abstract = {Autonomous agents capable of planning, reasoning, and executing actions on the web offer a promising avenue for automating computer tasks. However, the majority of existing benchmarks primarily focus on text-based agents, neglecting many natural tasks that require visual information to effectively solve. Given that most computer interfaces cater to human perception, visual information often augments textual data in ways that text-only models struggle to harness effectively. To bridge this gap, we introduce VisualWebArena, a benchmark designed to assess the performance of multimodal web agents on *realistic visually grounded tasks*. VisualWebArena comprises of a set of diverse and complex web-based tasks that evaluate various capabilities of autonomous multimodal agents. To perform on this benchmark, agents need to accurately process image-text inputs, interpret natural language instructions, and execute actions on websites to accomplish user-defined objectives. We conduct an extensive evaluation of state-of-the-art LLM-based autonomous agents, including several multimodal models. Through extensive quantitative and qualitative analysis, we identify several limitations of text-only LLM agents, and reveal gaps in the capabilities of state-of-the-art multimodal language agents. VisualWebArena provides a framework for evaluating multimodal autonomous language agents, and offers insights towards building stronger autonomous agents for the web.}
}

@article{kojimaLargeLanguageModels2022,
  title = {Large {{Language Models}} Are {{Zero-Shot Reasoners}}},
  author = {Kojima, Takeshi and Gu, Shixiang (Shane) and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  year = {2022},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {22199--22213},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/8bb0d291acd4acf06ef112099c16f326-Abstract-Conference.html},
  urldate = {2023-11-15},
  langid = {english}
}

@inproceedings{kongCalibratedLanguageModel2020,
  title = {Calibrated {{Language Model Fine-Tuning}} for {{In-}} and {{Out-of-Distribution Data}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Kong, Lingkai and Jiang, Haoming and Zhuang, Yuchen and Lyu, Jie and Zhao, Tuo and Zhang, Chao},
  year = {2020},
  pages = {1326--1340},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.emnlp-main.102},
  urldate = {2024-01-08},
  abstract = {Fine-tuned pre-trained language models can suffer from severe miscalibration for both in-distribution and out-of-distribution (OOD) data due to over-parameterization. To mitigate this issue, we propose a regularized fine-tuning method. Our method introduces two types of regularization for better calibration: (1) On-manifold regularization, which generates pseudo on-manifold samples through interpolation within the data manifold. Augmented training with these pseudo samples imposes a smoothness regularization to improve in-distribution calibration. (2) Off-manifold regularization, which encourages the model to output uniform distributions for pseudo off-manifold samples to address the over-confidence issue for OOD data. Our experiments demonstrate that the proposed method outperforms existing calibration methods for text classification in terms of expectation calibration error, misclassification detection, and OOD detection on six datasets. Our code can be found at https://github.com/Lingkai-Kong/ Calibrated-BERT-Fine-Tuning.},
  langid = {english}
}

@inproceedings{koreedaContractNLIDatasetDocumentlevel2021,
  title = {{{ContractNLI}}: {{A Dataset}} for {{Document-level Natural Language Inference}} for {{Contracts}}},
  shorttitle = {{{ContractNLI}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2021},
  author = {Koreeda, Yuta and Manning, Christopher},
  year = {2021},
  month = nov,
  pages = {1907--1919},
  publisher = {Association for Computational Linguistics},
  address = {Punta Cana, Dominican Republic},
  doi = {10.18653/v1/2021.findings-emnlp.164},
  urldate = {2023-10-12},
  abstract = {Reviewing contracts is a time-consuming procedure that incurs large expenses to companies and social inequality to those who cannot afford it. In this work, we propose ``document-level natural language inference (NLI) for contracts'', a novel, real-world application of NLI that addresses such problems. In this task, a system is given a set of hypotheses (such as ``Some obligations of Agreement may survive termination.'') and a contract, and it is asked to classify whether each hypothesis is ``entailed by'', ``contradicting to'' or ``not mentioned by'' (neutral to) the contract as well as identifying ``evidence'' for the decision as spans in the contract. We annotated and release the largest corpus to date consisting of 607 annotated contracts. We then show that existing models fail badly on our task and introduce a strong baseline, which (a) models evidence identification as multi-label classification over spans instead of trying to predict start and end tokens, and (b) employs more sophisticated context segmentation for dealing with long documents. We also show that linguistic characteristics of contracts, such as negations by exceptions, are contributing to the difficulty of this task and that there is much room for improvement.}
}

@inproceedings{krishnaReformulatingUnsupervisedStyle2020,
  title = {Reformulating {{Unsupervised Style Transfer}} as {{Paraphrase Generation}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Krishna, Kalpesh and Wieting, John and Iyyer, Mohit},
  year = {2020},
  month = nov,
  pages = {737--762},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.emnlp-main.55},
  urldate = {2023-01-17},
  abstract = {Modern NLP defines the task of style transfer as modifying the style of a given sentence without appreciably changing its semantics, which implies that the outputs of style transfer systems should be paraphrases of their inputs. However, many existing systems purportedly designed for style transfer inherently warp the input's meaning through attribute transfer, which changes semantic properties such as sentiment. In this paper, we reformulate unsupervised style transfer as a paraphrase generation problem, and present a simple methodology based on fine-tuning pretrained language models on automatically generated paraphrase data. Despite its simplicity, our method significantly outperforms state-of-the-art style transfer systems on both human and automatic evaluations. We also survey 23 style transfer papers and discover that existing automatic metrics can be easily gamed and propose fixed variants. Finally, we pivot to a more real-world style transfer setting by collecting a large dataset of 15M sentences in 11 diverse styles, which we use for an in-depth analysis of our system.}
}

@inproceedings{krizhevskyImageNetClassificationDeep2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  year = {2012},
  volume = {25},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
  urldate = {2023-09-26},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.}
}

@inproceedings{kuangCollaborationConversationalAI2023,
  title = {Collaboration with {{Conversational AI Assistants}} for {{UX Evaluation}}: {{Questions}} and {{How}} to {{Ask}} Them ({{Voice}} vs. {{Text}})},
  shorttitle = {Collaboration with {{Conversational AI Assistants}} for {{UX Evaluation}}},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Kuang, Emily and Jahangirzadeh Soure, Ehsan and Fan, Mingming and Zhao, Jian and Shinohara, Kristen},
  year = {2023},
  month = apr,
  series = {{{CHI}} '23},
  pages = {1--15},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3544548.3581247},
  urldate = {2024-08-27},
  abstract = {AI is promising in assisting UX evaluators with analyzing usability tests, but its judgments are typically presented as non-interactive visualizations. Evaluators may have questions about test recordings, but have no way of asking them. Interactive conversational assistants provide a Q\&amp;A dynamic that may improve analysis efficiency and evaluator autonomy. To understand the full range of analysis-related questions, we conducted a Wizard-of-Oz design probe study with 20 participants who interacted with simulated AI assistants via text or voice. We found that participants asked for five categories of information: user actions, user mental model, help from the AI assistant, product and task information, and user demographics. Those who used the text assistant asked more questions, but the question lengths were similar. The text assistant was perceived as significantly more efficient, but both were rated equally in satisfaction and trust. We also provide design considerations for future conversational AI assistants for UX evaluation.},
  isbn = {978-1-4503-9421-5}
}

@inproceedings{kuangEnhancingUXEvaluation2024,
  title = {Enhancing {{UX Evaluation Through Collaboration}} with {{Conversational AI Assistants}}: {{Effects}} of {{Proactive Dialogue}} and {{Timing}}},
  shorttitle = {Enhancing {{UX Evaluation Through Collaboration}} with {{Conversational AI Assistants}}},
  booktitle = {Proceedings of the {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Kuang, Emily and Li, Minghao and Fan, Mingming and Shinohara, Kristen},
  year = {2024},
  month = may,
  series = {{{CHI}} '24},
  pages = {1--16},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3613904.3642168},
  urldate = {2024-08-27},
  abstract = {Usability testing is vital for enhancing the user experience (UX) of interactive systems. However, analyzing test videos is complex and resource-intensive. Recent AI advancements have spurred exploration into human-AI collaboration for UX analysis, particularly through natural language. Unlike user-initiated dialogue, our study investigated the potential of proactive conversational assistants to aid UX evaluators through automatic suggestions at three distinct times: before, in sync with, and after potential usability problems. We conducted a hybrid Wizard-of-Oz study involving 24 UX evaluators, using ChatGPT to generate automatic problem suggestions and a human actor to respond to impromptu questions. While timing did not significantly impact analytic performance, suggestions appearing after potential problems were preferred, enhancing trust and efficiency. Participants found the automatic suggestions useful, but they collectively identified more than twice as many problems, underscoring the irreplaceable role of human expertise. Our findings also offer insights into future human-AI collaborative tools for UX evaluation.},
  isbn = {979-8-4007-0330-0}
}

@inproceedings{kuangMergingResultsNo2022,
  title = {``{{Merging Results Is No Easy Task}}'': {{An International Survey Study}} of {{Collaborative Data Analysis Practices Among UX Practitioners}}},
  shorttitle = {``{{Merging Results Is No Easy Task}}''},
  booktitle = {Proceedings of the 2022 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Kuang, Emily and Jin, Xiaofu and Fan, Mingming},
  year = {2022},
  month = apr,
  series = {{{CHI}} '22},
  pages = {1--16},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3491102.3517647},
  urldate = {2024-09-10},
  abstract = {Analysis is a key part of usability testing where UX practitioners seek to identify usability problems and generate redesign suggestions. Although previous research reported how analysis was conducted, the findings were typically focused on individual analysis or based on a small number of professionals in specific geographic regions. We conducted an online international survey of 279 UX practitioners on their practices and challenges while collaborating during data analysis. We found that UX practitioners were often under time pressure to conduct analysis and adopted three modes of collaboration: independently analyze different portions of the data and then collaborate, collaboratively analyze the session with little or no independent analysis, and independently analyze the same set of data and then collaborate. Moreover, most encountered challenges related to lack of resources, disagreements with colleagues regarding usability problems, and difficulty merging analysis from multiple practitioners. We discuss design implications to better support collaborative data analysis.},
  isbn = {978-1-4503-9157-3}
}

@misc{kuzmanChatGPTBeginningEnd2023,
  title = {{{ChatGPT}}: {{Beginning}} of an {{End}} of {{Manual Linguistic Data Annotation}}? {{Use Case}} of {{Automatic Genre Identification}}},
  shorttitle = {{{ChatGPT}}},
  author = {Kuzman, Taja and Mozeti{\v c}, Igor and Ljube{\v s}i{\'c}, Nikola},
  year = {2023},
  month = mar,
  number = {arXiv:2303.03953},
  eprint = {2303.03953},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2303.03953},
  urldate = {2023-11-13},
  abstract = {ChatGPT has shown strong capabilities in natural language generation tasks, which naturally leads researchers to explore where its abilities end. In this paper, we examine whether ChatGPT can be used for zero-shot text classification, more specifically, automatic genre identification. We compare ChatGPT with a multilingual XLM-RoBERTa language model that was fine-tuned on datasets, manually annotated with genres. The models are compared on test sets in two languages: English and Slovenian. Results show that ChatGPT outperforms the fine-tuned model when applied to the dataset which was not seen before by either of the models. Even when applied on Slovenian language as an under-resourced language, ChatGPT's performance is no worse than when applied to English. However, if the model is fully prompted in Slovenian, the performance drops significantly, showing the current limitations of ChatGPT usage on smaller languages. The presented results lead us to questioning whether this is the beginning of an end of laborious manual annotation campaigns even for smaller languages, such as Slovenian.},
  archiveprefix = {arXiv}
}

@misc{laiAutoWebGLMBootstrapReinforce2024,
  title = {{{AutoWebGLM}}: {{Bootstrap And Reinforce A Large Language Model-based Web Navigating Agent}}},
  shorttitle = {{{AutoWebGLM}}},
  author = {Lai, Hanyu and Liu, Xiao and Iong, Iat Long and Yao, Shuntian and Chen, Yuxuan and Shen, Pengbo and Yu, Hao and Zhang, Hanchen and Zhang, Xiaohan and Dong, Yuxiao and Tang, Jie},
  year = {2024},
  month = apr,
  number = {arXiv:2404.03648},
  eprint = {2404.03648},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.03648},
  urldate = {2024-08-08},
  abstract = {Large language models (LLMs) have fueled many intelligent agent tasks, such as web navigation -- but most existing agents perform far from satisfying in real-world webpages due to three factors: (1) the versatility of actions on webpages, (2) HTML text exceeding model processing capacity, and (3) the complexity of decision-making due to the open-domain nature of web. In light of the challenge, we develop AutoWebGLM, a GPT-4-outperforming automated web navigation agent built upon ChatGLM3-6B. Inspired by human browsing patterns, we design an HTML simplification algorithm to represent webpages, preserving vital information succinctly. We employ a hybrid human-AI method to build web browsing data for curriculum training. Then, we bootstrap the model by reinforcement learning and rejection sampling to further facilitate webpage comprehension, browser operations, and efficient task decomposition by itself. For testing, we establish a bilingual benchmark -- AutoWebBench -- for real-world web browsing tasks. We evaluate AutoWebGLM across diverse web navigation benchmarks, revealing its improvements but also underlying challenges to tackle real environments. Related code, model, and data will be released at {\textbackslash}url\{https://github.com/THUDM/AutoWebGLM\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@inproceedings{laiEmpoweringUnsupervisedDomain2024,
  title = {Empowering {{Unsupervised Domain Adaptation With Large-Scale Pre-Trained Vision-Language Models}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Winter Conference}} on {{Applications}} of {{Computer Vision}}},
  author = {Lai, Zhengfeng and Bai, Haoping and Zhang, Haotian and Du, Xianzhi and Shan, Jiulong and Yang, Yinfei and Chuah, Chen-Nee and Cao, Meng},
  year = {2024},
  pages = {2691--2701},
  url = {https://openaccess.thecvf.com/content/WACV2024/html/Lai_Empowering_Unsupervised_Domain_Adaptation_With_Large-Scale_Pre-Trained_Vision-Language_Models_WACV_2024_paper.html},
  urldate = {2024-06-16},
  langid = {english}
}

@inproceedings{lanALBERTLiteBERT2019,
  title = {{{ALBERT}}: {{A Lite BERT}} for {{Self-supervised Learning}} of {{Language Representations}}},
  shorttitle = {{{ALBERT}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  year = {2019},
  month = sep,
  url = {https://openreview.net/forum?id=H1eA7AEtvS},
  urldate = {2022-04-12},
  abstract = {A new pretraining method that establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large.},
  langid = {english}
}

@misc{lanhamMeasuringFaithfulnessChainofThought2023,
  title = {Measuring {{Faithfulness}} in {{Chain-of-Thought Reasoning}}},
  author = {Lanham, Tamera and Chen, Anna and Radhakrishnan, Ansh and Steiner, Benoit and Denison, Carson and Hernandez, Danny and Li, Dustin and Durmus, Esin and Hubinger, Evan and Kernion, Jackson and Luko{\v s}i{\=u}t{\.e}, Kamil{\.e} and Nguyen, Karina and Cheng, Newton and Joseph, Nicholas and Schiefer, Nicholas and Rausch, Oliver and Larson, Robin and McCandlish, Sam and Kundu, Sandipan and Kadavath, Saurav and Yang, Shannon and Henighan, Thomas and Maxwell, Timothy and {Telleen-Lawton}, Timothy and Hume, Tristan and {Hatfield-Dodds}, Zac and Kaplan, Jared and Brauner, Jan and Bowman, Samuel R. and Perez, Ethan},
  year = {2023},
  month = jul,
  number = {arXiv:2307.13702},
  eprint = {2307.13702},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.13702},
  urldate = {2023-10-11},
  abstract = {Large language models (LLMs) perform better when they produce step-by-step, "Chain-of-Thought" (CoT) reasoning before answering a question, but it is unclear if the stated reasoning is a faithful explanation of the model's actual reasoning (i.e., its process for answering the question). We investigate hypotheses for how CoT reasoning may be unfaithful, by examining how the model predictions change when we intervene on the CoT (e.g., by adding mistakes or paraphrasing it). Models show large variation across tasks in how strongly they condition on the CoT when predicting their answer, sometimes relying heavily on the CoT and other times primarily ignoring it. CoT's performance boost does not seem to come from CoT's added test-time compute alone or from information encoded via the particular phrasing of the CoT. As models become larger and more capable, they produce less faithful reasoning on most tasks we study. Overall, our results suggest that CoT can be faithful if the circumstances such as the model size and task are carefully chosen.},
  archiveprefix = {arXiv}
}

@inproceedings{lattnerLLVMCompilationFramework2004,
  title = {{{LLVM}}: A Compilation Framework for Lifelong Program Analysis \& Transformation},
  shorttitle = {{{LLVM}}},
  booktitle = {International {{Symposium}} on {{Code Generation}} and {{Optimization}}, 2004. {{CGO}} 2004.},
  author = {Lattner, C. and Adve, V.},
  year = {2004},
  month = mar,
  pages = {75--86},
  doi = {10.1109/CGO.2004.1281665},
  abstract = {We describe LLVM (low level virtual machine), a compiler framework designed to support transparent, lifelong program analysis and transformation for arbitrary programs, by providing high-level information to compiler transformations at compile-time, link-time, run-time, and in idle time between runs. LLVM defines a common, low-level code representation in static single assignment (SSA) form, with several novel features: a simple, language-independent type-system that exposes the primitives commonly used to implement high-level language features; an instruction for typed address arithmetic; and a simple mechanism that can be used to implement the exception handling features of high-level languages (and setjmp/longjmp in C) uniformly and efficiently. The LLVM compiler framework and code representation together provide a combination of key capabilities that are important for practical, lifelong analysis and transformation of programs. To our knowledge, no existing compilation approach provides all these capabilities. We describe the design of the LLVM representation and compiler framework, and evaluate the design in three ways: (a) the size and effectiveness of the representation, including the type information it provides; (b) compiler performance for several interprocedural problems; and (c) illustrative examples of the benefits LLVM provides for several challenging compiler problems.}
}

@inproceedings{ledoEvaluationStrategiesHCI2018,
  title = {Evaluation {{Strategies}} for {{HCI Toolkit Research}}},
  booktitle = {Proceedings of the 2018 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Ledo, David and Houben, Steven and Vermeulen, Jo and Marquardt, Nicolai and Oehlberg, Lora and Greenberg, Saul},
  year = {2018},
  month = apr,
  series = {{{CHI}} '18},
  pages = {1--17},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3173574.3173610},
  urldate = {2024-09-13},
  abstract = {Toolkit research plays an important role in the field of HCI, as it can heavily influence both the design and implementation of interactive systems. For publication, the HCI community typically expects toolkit research to include an evaluation component. The problem is that toolkit evaluation is challenging, as it is often unclear what 'evaluating' a toolkit means and what methods are appropriate. To address this problem, we analyzed 68 published toolkit papers. From our analysis, we provide an overview of, reflection on, and discussion of evaluation methods for toolkit contributions. We identify and discuss the value of four toolkit evaluation strategies, including the associated techniques that each employs. We offer a categorization of evaluation strategies for toolkit researchers, along with a discussion of the value, potential limitations, and trade-offs associated with each strategy.},
  isbn = {978-1-4503-5620-6}
}

@inproceedings{leeAbstractModelingEvaluating2020,
  title = {Abstract: {{Modeling}} and {{Evaluating Intervention Options}} and {{Strategies}} for {{COVID-19 Containment}}: {{A Biological-Behavioral-Logistics Computation Decision Framework}}},
  shorttitle = {Abstract},
  booktitle = {2020 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  author = {Lee, Eva},
  year = {2020},
  pages = {1--2},
  doi = {10.1109/BIBM49941.2020.9313325},
  abstract = {SARs, bird flu, H1N1, Ebola crisis in W. Africa, Zika and current SARS-CoV-2 underscore the critical importance of emergency response and medical preparedness. Such needs are wide-spread as globalization and air transportation facilitate rapid disease spread across the world. Computational modeling of infectious disease outbreaks and epidemics offer insights in propagation patterns and facilitate policy makers to synthesize potential interventions. Current models include inclined decay with an exponential adjustment, SEIR (susceptible, exposed, infectious, recovered) compartmental model, discrete time stochastic processes, and transmission tree. While many of these models incorporate contact tracing to predict spread pattern, key elements on optimal usage of scarce resources and effective and efficient process performance (e.g., diagnostics and screening, non-pharmaceutical interventions, trained personnel/robots for treatment, decontamination) have not been included. This is particularly critical in the fight of COVID-19 containment due to lack of testing kits and the prevalence of asymptomatic transmission, and the long period of hospitalization required by severely sick patients.This work focuses on designing a system computational decision modeling framework that simultaneously i) captures disease spread characteristics, ii) incorporates day-to-day hospital and home care processes and resource usage, iii) explores non-pharmaceutical intervention, social and human behavior and iv) allows for system optimization to minimize infection and mortality under time and labor constraints.},
  annotation = {00000}
}

@article{leeApplicationsGPTPolitical,
  title = {Applications of {{GPT}} in {{Political Science Research}}},
  author = {Lee, Kyuwon and Paci, Simone and Park, Jeongmin and You, Hye Young and Zheng, Sylvan},
  abstract = {This paper explores the transformative role of GPT in political science research, demonstrating its potential to streamline data collection and analysis processes. By automating the extraction of information from diverse data sources---such as historical documents, meeting minutes, news articles, and unstructured digital content---GPT significantly reduces the time and financial resources traditionally required for data management. We explore how GPT's capabilities complement the work of human research assistants, combining automated efficiency with human oversight to enhance both the reliability and depth of research outputs. The integration of GPT not only makes comprehensive data collection and analysis accessible to researchers with limited resources, it also enhances the overall efficiency and scope of research in political science. This article underscores the increasing importance of artificial intelligence tools in advancing empirical research within the field.},
  langid = {english},
  keywords = {No DOI found}
}

@article{leeBioBERTPretrainedBiomedical2019,
  title = {{{BioBERT}}: A Pre-Trained Biomedical Language Representation Model for Biomedical Text Mining},
  shorttitle = {{{BioBERT}}},
  author = {Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
  editor = {Wren, Jonathan},
  year = {2019},
  month = sep,
  journal = {Bioinformatics},
  pages = {btz682},
  issn = {1367-4803, 1460-2059},
  doi = {10/ggh5qq},
  urldate = {2021-11-30},
  abstract = {Motivation: Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora.},
  langid = {english}
}

@inproceedings{leeDREAMDomainInvariant2022,
  title = {{{DREAM}}: {{Domain Invariant}} and {{Contrastive Representation}} for {{Sleep Dynamics}}},
  shorttitle = {{{DREAM}}},
  booktitle = {2022 {{IEEE International Conference}} on {{Data Mining}} ({{ICDM}})},
  author = {Lee, Seungyeon and Pham, Thai-Hoang and Zhang, Ping},
  year = {2022},
  month = nov,
  pages = {1029--1034},
  issn = {2374-8486},
  doi = {10.1109/ICDM54844.2022.00126},
  urldate = {2024-04-05},
  abstract = {Sleep staging is a key challenge in diagnosing and treating sleep-related diseases due to its labor-intensive, time-consuming, costly, and error-prone. With the availability of large-scale sleep signal data, many deep learning methods are proposed for automatic sleep staging. However, these existing methods face several challenges including the heterogeneity of patients' underlying health conditions and the difficulty modeling complex interactions between sleep stages. In this paper, we propose a neural network architecture named DREAM to tackle these issues for automatic sleep staging. DREAM consists of (i) a feature representation network that generates robust representations for sleep signals via the variational auto-encoder framework and contrastive learning and (ii) a sleep stage classification network that explicitly models the interactions between sleep stages in the sequential context at both feature representation and label classification levels via Transformer and conditional random field architectures. Our experimental results indicate that DREAM significantly outperforms existing methods for automatic sleep staging on three sleep signal datasets.}
}

@inproceedings{leEffectiveInterpretablePersonJob2019,
  title = {Towards {{Effective}} and {{Interpretable Person-Job Fitting}}},
  booktitle = {Proceedings of the 28th {{ACM International Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Le, Ran and Hu, Wenpeng and Song, Yang and Zhang, Tao and Zhao, Dongyan and Yan, Rui},
  year = {2019},
  month = nov,
  series = {{{CIKM}} '19},
  pages = {1883--1892},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3357384.3357949},
  urldate = {2022-12-25},
  abstract = {The diversity of job requirements and the complexity of job seekers' abilities put forward higher requirements for the accuracy and interpretability of Person-Job Fit system. Interpretable Person-Job Fit system can show reasons for giving recommendations or not recommending specific jobs to some people, and vice versa. Such reasons help us understand according to what the final decision is made by the system and guarantee a high recommending accuracy. Existing studies on Person-Job Fit have focused on 1) one perspective, without considering the variances of role and psychological motivation between interviewer and job seeker; 2) modeling the matching degree between resume and job requirements directly through a deep neural network without interaction matching modules, which leads to shortage on interpretation. To this end, we propose an Interpretable Person-Job Fit (IPJF) model, which 1) models the Person-Job Fit problem from the perspectives/intentions of employer and job seeker in a multi-tasks optimization fashion to interpretively formulate the Person-Job Fit process; 2) leverages deep interactive representation learning to automatically learn the interdependence between a resume and job requirements without relying on a clear list of job seeker's abilities, and deploys the optimizing problem as a learning to rank problem. Experiments on large real dataset show that the proposed IPJF model outperforms state-of-the-art baselines and also gives promising interpretable recommending reasons.},
  isbn = {978-1-4503-6976-3}
}

@misc{leeLEANLIFELabelEfficientAnnotation2020,
  title = {{{LEAN-LIFE}}: {{A Label-Efficient Annotation Framework Towards Learning}} from {{Explanation}}},
  shorttitle = {{{LEAN-LIFE}}},
  author = {Lee, Dong-Ho and Khanna, Rahul and Lin, Bill Yuchen and Chen, Jamin and Lee, Seyeon and Ye, Qinyuan and Boschee, Elizabeth and Neves, Leonardo and Ren, Xiang},
  year = {2020},
  month = apr,
  number = {arXiv:2004.07499},
  eprint = {2004.07499},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2004.07499},
  urldate = {2023-08-02},
  abstract = {Successfully training a deep neural network demands a huge corpus of labeled data. However, each label only provides limited information to learn from and collecting the requisite number of labels involves massive human effort. In this work, we introduce LEAN-LIFE, a web-based, Label-Efficient AnnotatioN framework for sequence labeling and classification tasks, with an easy-to-use UI that not only allows an annotator to provide the needed labels for a task, but also enables LearnIng From Explanations for each labeling decision. Such explanations enable us to generate useful additional labeled data from unlabeled instances, bolstering the pool of available training data. On three popular NLP tasks (named entity recognition, relation extraction, sentiment analysis), we find that using this enhanced supervision allows our models to surpass competitive baseline F1 scores by more than 5-10 percentage points, while using 2X times fewer labeled instances. Our framework is the first to utilize this enhanced supervision technique and does so for three important tasks -- thus providing improved annotation recommendations to users and an ability to build datasets of (data, label, explanation) triples instead of the regular (data, label) pair.},
  archiveprefix = {arXiv}
}

@inproceedings{leeLLMCXRInstructionFinetunedLLM2023,
  title = {{{LLM-CXR}}: {{Instruction-Finetuned LLM}} for {{CXR Image Understanding}} and {{Generation}}},
  shorttitle = {{{LLM-CXR}}},
  booktitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  author = {Lee, Suhyeon and Kim, Won Jun and Chang, Jinho and Ye, Jong Chul},
  year = {2023},
  month = oct,
  url = {https://openreview.net/forum?id=BqHaLnans2},
  urldate = {2024-03-18},
  abstract = {Following the impressive development of LLMs, vision-language alignment in LLMs is actively being researched to enable multimodal reasoning and visual input/output. This direction of research is particularly relevant to medical imaging because accurate medical image analysis and generation consist of a combination of reasoning based on visual features and prior knowledge. Many recent works have focused on training adapter networks that serve as an information bridge between image processing (encoding or generating) networks and LLMs; but presumably, in order to achieve maximum reasoning potential of LLMs on visual information as well, visual and language features should be allowed to interact more freely. This is especially important in the medical domain because understanding and generating medical images such as chest X-rays (CXR) require not only accurate visual and language-based reasoning but also a more intimate mapping between the two modalities. Thus, taking inspiration from previous work on the transformer and VQ-GAN combination for bidirectional image and text generation, we build upon this approach and develop a method for instruction-tuning an LLM pre-trained only on text to gain vision-language capabilities for medical images. Specifically, we leverage a pretrained LLM's existing question-answering and instruction-following abilities to teach it to understand visual inputs by instructing it to answer questions about image inputs and, symmetrically, output both text and image responses appropriate to a given query by tuning the LLM with diverse tasks that encompass image-based text-generation and text-based image-generation. We show that our LLM-CXR trained in this approach shows better image-text alignment in both CXR understanding and generation tasks while being smaller in size compared to previously developed models that perform a narrower range of tasks.},
  langid = {english}
}

@inproceedings{lesterPowerScaleParameterEfficient2021,
  title = {The {{Power}} of {{Scale}} for {{Parameter-Efficient Prompt Tuning}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Lester, Brian and {Al-Rfou}, Rami and Constant, Noah},
  year = {2021},
  month = nov,
  pages = {3045--3059},
  publisher = {Association for Computational Linguistics},
  address = {Online and Punta Cana, Dominican Republic},
  doi = {10.18653/v1/2021.emnlp-main.243},
  urldate = {2022-08-02},
  abstract = {In this work, we explore ``prompt tuning,'' a simple yet effective mechanism for learning ``soft prompts'' to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method ``closes the gap'' and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed ``prefix tuning'' of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient ``prompt ensembling.'' We release code and model checkpoints to reproduce our experiments.}
}

@article{lewis2012usability,
  title = {Usability Testing},
  author = {Lewis, James R},
  year = {2012},
  journal = {Handbook of human factors and ergonomics},
  pages = {1267--1312},
  publisher = {Wiley Online Library},
  doi = {10.1002/9781118131350.ch46}
}

@misc{lewisBARTDenoisingSequencetoSequence2019,
  title = {{{BART}}: {{Denoising Sequence-to-Sequence Pre-training}} for {{Natural Language Generation}}, {{Translation}}, and {{Comprehension}}},
  shorttitle = {{{BART}}},
  author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  year = {2019},
  month = oct,
  number = {arXiv:1910.13461},
  eprint = {1910.13461},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1910.13461},
  urldate = {2022-08-16},
  abstract = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.},
  archiveprefix = {arXiv}
}

@misc{lewisPAQ65Million2021,
  title = {{{PAQ}}: 65 {{Million Probably-Asked Questions}} and {{What You Can Do With Them}}},
  shorttitle = {{{PAQ}}},
  author = {Lewis, Patrick and Wu, Yuxiang and Liu, Linqing and Minervini, Pasquale and K{\"u}ttler, Heinrich and Piktus, Aleksandra and Stenetorp, Pontus and Riedel, Sebastian},
  year = {2021},
  month = feb,
  number = {arXiv:2102.07033},
  eprint = {2102.07033},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2102.07033},
  urldate = {2022-06-29},
  abstract = {Open-domain Question Answering models which directly leverage question-answer (QA) pairs, such as closed-book QA (CBQA) models and QA-pair retrievers, show promise in terms of speed and memory compared to conventional models which retrieve and read from text corpora. QA-pair retrievers also offer interpretable answers, a high degree of control, and are trivial to update at test time with new knowledge. However, these models lack the accuracy of retrieve-and-read systems, as substantially less knowledge is covered by the available QA-pairs relative to text corpora like Wikipedia. To facilitate improved QA-pair models, we introduce Probably Asked Questions (PAQ), a very large resource of 65M automatically-generated QA-pairs. We introduce a new QA-pair retriever, RePAQ, to complement PAQ. We find that PAQ preempts and caches test questions, enabling RePAQ to match the accuracy of recent retrieve-and-read models, whilst being significantly faster. Using PAQ, we train CBQA models which outperform comparable baselines by 5\%, but trail RePAQ by over 15\%, indicating the effectiveness of explicit retrieval. RePAQ can be configured for size (under 500MB) or speed (over 1K questions per second) whilst retaining high accuracy. Lastly, we demonstrate RePAQ's strength at selective QA, abstaining from answering when it is likely to be incorrect. This enables RePAQ to ``back-off" to a more expensive state-of-the-art model, leading to a combined system which is both more accurate and 2x faster than the state-of-the-art model alone.},
  archiveprefix = {arXiv}
}

@misc{liAgentHospitalSimulacrum2024,
  title = {Agent {{Hospital}}: {{A Simulacrum}} of {{Hospital}} with {{Evolvable Medical Agents}}},
  shorttitle = {Agent {{Hospital}}},
  author = {Li, Junkai and Wang, Siyu and Zhang, Meng and Li, Weitao and Lai, Yunghwei and Kang, Xinhui and Ma, Weizhi and Liu, Yang},
  year = {2024},
  month = may,
  number = {arXiv:2405.02957},
  eprint = {2405.02957},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.02957},
  urldate = {2024-07-05},
  abstract = {In this paper, we introduce a simulacrum of hospital called Agent Hospital that simulates the entire process of treating illness. All patients, nurses, and doctors are autonomous agents powered by large language models (LLMs). Our central goal is to enable a doctor agent to learn how to treat illness within the simulacrum. To do so, we propose a method called MedAgent-Zero. As the simulacrum can simulate disease onset and progression based on knowledge bases and LLMs, doctor agents can keep accumulating experience from both successful and unsuccessful cases. Simulation experiments show that the treatment performance of doctor agents consistently improves on various tasks. More interestingly, the knowledge the doctor agents have acquired in Agent Hospital is applicable to real-world medicare benchmarks. After treating around ten thousand patients (real-world doctors may take over two years), the evolved doctor agent achieves a state-of-the-art accuracy of 93.06\% on a subset of the MedQA dataset that covers major respiratory diseases. This work paves the way for advancing the applications of LLM-powered agent techniques in medical scenarios.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence}
}

@inproceedings{liAliMeKGDomainKnowledge2020,
  title = {{{AliMeKG}}: {{Domain Knowledge Graph Construction}} and {{Application}} in {{E-commerce}}},
  shorttitle = {{{AliMeKG}}},
  booktitle = {Proceedings of the 29th {{ACM International Conference}} on {{Information}} \& {{Knowledge Management}}},
  author = {Li, Feng-Lin and Chen, Hehong and Xu, Guohai and Qiu, Tian and Ji, Feng and Zhang, Ji and Chen, Haiqing},
  year = {2020},
  month = oct,
  series = {{{CIKM}} '20},
  pages = {2581--2588},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3340531.3412685},
  urldate = {2022-07-19},
  abstract = {Pre-\-sales customer service is of importance to E\--commerce plat\-forms as it contributes to optimizing customers? buying process. To better serve users, we propose AliMe KG, a domain knowledge graph in E\--commerce that captures user problems, points of inter\-est (POI), item information and relations thereof. It helps to under\- stand user needs, answer pre\--sales questions and generate explana\-tion texts. We applied AliMe KG to several online business scenar\-ios such as shopping guide, question answering over properties and selling point generation, and gained positive and beneficial business results. In the paper, we systematically introduce how we construct domain knowledge graph from free text, and demonstrate its busi\-ness value with several applications. Our experience shows that min\- ing structured knowledge from free text in vertical domain is prac\-ticable, and can be of substantial value in industrial settings.},
  isbn = {978-1-4503-6859-9}
}

@misc{liaoMARFTMultiAgentReinforcement2025,
  title = {{{MARFT}}: {{Multi-Agent Reinforcement Fine-Tuning}}},
  shorttitle = {{{MARFT}}},
  author = {Liao, Junwei and Wen, Muning and Wang, Jun and Zhang, Weinan},
  year = {2025},
  month = may,
  number = {arXiv:2504.16129},
  eprint = {2504.16129},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.16129},
  urldate = {2025-06-04},
  abstract = {LLM-based Multi-Agent Systems have demonstrated remarkable capabilities in addressing complex, agentic tasks, from generating high-quality presentation slides to even conducting sophisticated scientific research. Meanwhile, RL has been widely recognized for its effectiveness in enhancing agent intelligence, but limited research has investigated the fine-tuning of LaMAS using foundational RL techniques. Moreover, the direct application of MARL methods to LaMAS introduces significant challenges, stemming from the unique characteristics and mechanisms inherent to LaMAS. To address these challenges, this article presents a comprehensive study of LLM-based MARL and proposes a novel paradigm termed Multi-Agent Reinforcement Fine-Tuning (MARFT). We introduce a brand-new POMDP called Flex-POMDP, which aligns with the LaMAS optimization in real-world applications and a universal algorithmic framework tailored specifically for LaMAS, outlining the conceptual foundations, key distinctions, and practical implementation strategies. We review the evolution from RL to RFT, setting the stage for a parallel analysis in the multi-agent domain. In the context of LaMAS, we elucidate critical differences between MARL and MARFT. These differences motivate a transition toward a LaMAS-oriented formulation of RFT. Central to this work is a robust and scalable MARFT framework. We detail the core algorithm and provide a complete, open-source implementation to facilitate adoption and further research. The latter sections of the paper explore real-world application perspectives and opening challenges in MARFT. By bridging theoretical underpinnings with practical methodologies, this work serves as a roadmap for researchers seeking to advance MARFT toward resilient and adaptive solutions in agentic systems. Our implementation of the proposed framework is publicly available at: https://github.com/jwliao-ai/MARFT.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Computer Science - Robotics}
}

@book{liArtificialIntelligenceHuman2021,
  title = {Artificial {{Intelligence}} for {{Human Computer Interaction}}: {{A Modern Approach}}},
  shorttitle = {Artificial {{Intelligence}} for {{Human Computer Interaction}}},
  editor = {Li, Yang and Hilliges, Otmar},
  year = {2021},
  series = {Human--{{Computer Interaction Series}}},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-82681-9},
  urldate = {2024-09-13},
  copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
  isbn = {978-3-030-82680-2 978-3-030-82681-9},
  langid = {english},
  keywords = {Artificial Intelligence,Data-driven Methods,Deep Learning Methods,Human Computer Interaction,Human Performance Modeling}
}

@inproceedings{liBEHAVIOR1KBenchmarkEmbodied2023,
  title = {{{BEHAVIOR-1K}}: {{A Benchmark}} for {{Embodied AI}} with 1,000 {{Everyday Activities}} and {{Realistic Simulation}}},
  shorttitle = {{{BEHAVIOR-1K}}},
  booktitle = {Proceedings of {{The}} 6th {{Conference}} on {{Robot Learning}}},
  author = {Li, Chengshu and Zhang, Ruohan and Wong, Josiah and Gokmen, Cem and Srivastava, Sanjana and {Mart{\'i}n-Mart{\'i}n}, Roberto and Wang, Chen and Levine, Gabrael and Lingelbach, Michael and Sun, Jiankai and Anvari, Mona and Hwang, Minjune and Sharma, Manasi and Aydin, Arman and Bansal, Dhruva and Hunter, Samuel and Kim, Kyu-Young and Lou, Alan and Matthews, Caleb R. and {Villa-Renteria}, Ivan and Tang, Jerry Huayang and Tang, Claire and Xia, Fei and Savarese, Silvio and Gweon, Hyowon and Liu, Karen and Wu, Jiajun and {Fei-Fei}, Li},
  year = {2023},
  month = mar,
  pages = {80--93},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v205/li23a.html},
  urldate = {2024-02-20},
  abstract = {We present BEHAVIOR-1K, a comprehensive simulation benchmark for human-centered robotics. BEHAVIOR-1K includes two components, guided and motivated by the results of an extensive survey on "what do you want robots to do for you?". The first is the definition of 1,000 everyday activities, grounded in 50 scenes (houses, gardens, restaurants, offices, etc.) with more than 5,000 objects annotated with rich physical and semantic properties. The second is OmniGibson, a novel simulation environment that supports these activities via realistic physics simulation and rendering of rigid bodies, deformable bodies, and liquids. Our experiments indicate that the activities in BEHAVIOR-1K are long-horizon and dependent on complex manipulation skills, both of which remain a challenge for even state-of-the-art robot learning solutions. To calibrate the simulation-to-reality gap of BEHAVIOR-1K, we provide an initial study on transferring solutions learned with a mobile manipulator in a simulated apartment to its real-world counterpart. We hope that BEHAVIOR-1K's human-grounded nature, diversity, and realism make it valuable for embodied AI and robot learning research. Project website: https://behavior.stanford.edu.},
  langid = {english}
}

@article{liDIGMNDynamicIntent,
  title = {{{DIGMN}}: {{Dynamic Intent Guided Meta Network}} for {{Differentiated User Engagement Forecasting}} in {{Online Professional Social Platforms}}},
  author = {Li, Feifan and Du, Lun and Fu, Qiang and Han, Shi and Du, Yushu and Lu, Guangming and Li, Zi},
  pages = {10},
  abstract = {User engagement prediction plays a critical role for designing interaction strategies to grow user engagement and increase revenue in online social platforms. Through the in-depth analysis of the real-world data from the world's largest professional social platforms, i.e., LinkedIn, we find that users expose diverse engagement patterns, and a major reason for the differences in user engagement patterns is that users have different intents. That is, people have different intents when using LinkedIn, e.g., applying for jobs, building connections, or checking notifications, which shows quite different engagement patterns. Meanwhile, user intents and the corresponding engagement patterns may change over time. Although such pattern differences and dynamics are essential for user engagement prediction, differentiating user engagement patterns based on user dynamic intents for better user engagement forecasting has not received enough attention in previous works. In this paper, we proposed a Dynamic Intent Guided Meta Network (DIGMN), which can explicitly model user intent varying with time and perform differentiated user engagement forecasting. Specifically, we derive some interpretable basic user intents as prior knowledge from data mining and introduce prior intents in explicitly modeling dynamic user intent. Furthermore, based on the dynamic user intent representations, we propose a meta predictor to perform differentiated user engagement forecasting. Through a comprehensive evaluation on LinkedIn anonymous user data, our method outperforms stateof-the-art baselines significantly, i.e., 2.96\% and 3.48\% absolute error reduction, on coarse-grained and fine-grained user engagement prediction tasks, respectively, demonstrating the effectiveness of our method.},
  langid = {english}
}

@misc{liGuidingLargeLanguage2023,
  title = {Guiding {{Large Language Models}} via {{Directional Stimulus Prompting}}},
  author = {Li, Zekun and Peng, Baolin and He, Pengcheng and Galley, Michel and Gao, Jianfeng and Yan, Xifeng},
  year = {2023},
  month = feb,
  number = {arXiv:2302.11520},
  eprint = {2302.11520},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.11520},
  urldate = {2023-03-13},
  abstract = {We introduce a new framework, Directional Stimulus Prompting, that uses a tuneable language model (LM) to provide guidance for the black-box frozen large language model (LLM) on downstream tasks. Unlike prior work that manually or automatically finds the optimal prompt for each task, we train a policy LM to generate discrete tokens as ``directional stimulus'' of each input, which is a hint/cue such as keywords of an article for summarization. The directional stimulus is then combined with the original input and fed into the LLM to guide its generation toward the desired target. The policy LM can be trained through 1) supervised learning from annotated data and 2) reinforcement learning from offline and online rewards to explore directional stimulus that better aligns LLMs with human preferences. This framework is flexibly applicable to various LMs and tasks. To verify its effectiveness, we apply our framework to summarization and dialogue response generation tasks. Experimental results demonstrate that it can significantly improve LLMs' performance with a small collection of training data: a T5 (780M) trained with 2,000 samples from the CNN/Daily Mail dataset improves Codex (175B)'s performance by 7.2\% in ROUGE-Avg scores; 500 dialogues boost the combined score by 52.5\%, achieving comparable or even better performance than fully trained models on the MultiWOZ dataset.},
  archiveprefix = {arXiv}
}

@misc{liHumanCenteredPrivacyResearch2024,
  title = {Human-{{Centered Privacy Research}} in the {{Age}} of {{Large Language Models}}},
  author = {Li, Tianshi and Das, Sauvik and Lee, Hao-Ping and Wang, Dakuo and Yao, Bingsheng and Zhang, Zhiping},
  year = {2024},
  month = feb,
  number = {arXiv:2402.01994},
  eprint = {2402.01994},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.01994},
  urldate = {2024-09-12},
  abstract = {The emergence of large language models (LLMs), and their increased use in user-facing systems, has led to substantial privacy concerns. To date, research on these privacy concerns has been model-centered: exploring how LLMs lead to privacy risks like memorization, or can be used to infer personal characteristics about people from their content. We argue that there is a need for more research focusing on the human aspect of these privacy issues: e.g., research on how design paradigms for LLMs affect users' disclosure behaviors, users' mental models and preferences for privacy controls, and the design of tools, systems, and artifacts that empower end-users to reclaim ownership over their personal data. To build usable, efficient, and privacy-friendly systems powered by these models with imperfect privacy properties, our goal is to initiate discussions to outline an agenda for conducting human-centered research on privacy issues in LLM-powered systems. This Special Interest Group (SIG) aims to bring together researchers with backgrounds in usable security and privacy, human-AI collaboration, NLP, or any other related domains to share their perspectives and experiences on this problem, to help our community establish a collective understanding of the challenges, research opportunities, research methods, and strategies to collaborate with researchers outside of HCI.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security,Computer Science - Human-Computer Interaction}
}

@article{liLearningKnowledgeGraph2021,
  title = {Learning {{Knowledge Graph Embedding With Heterogeneous Relation Attention Networks}}},
  author = {Li, Zhifei and Liu, Hai and Zhang, Zhaoli and Liu, Tingting and Xiong, Neal N.},
  year = {2021},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  pages = {1--13},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2021.3055147},
  abstract = {Knowledge graph (KG) embedding aims to study the embedding representation to retain the inherent structure of KGs. Graph neural networks (GNNs), as an effective graph representation technique, have shown impressive performance in learning graph embedding. However, KGs have an intrinsic property of heterogeneity, which contains various types of entities and relations. How to address complex graph data and aggregate multiple types of semantic information simultaneously is a critical issue. In this article, a novel heterogeneous GNNs framework based on attention mechanism is proposed. Specifically, the neighbor features of an entity are first aggregated under each relation-path. Then the importance of different relation-paths is learned through the relation features. Finally, each relation-path-based features with the learned weight values are aggregated to generate the embedding representation. Thus, the proposed method not only aggregates entity features from different semantic aspects but also allocates appropriate weights to them. This method can capture various types of semantic information and selectively aggregate informative features. The experiment results on three real-world KGs demonstrate superior performance when compared with several state-of-the-art methods.}
}

@article{limaCardiotoxicityCancerPatients2022,
  title = {Cardiotoxicity in Cancer Patients Treated with Chemotherapy: {{A}} Systematic Review},
  shorttitle = {Cardiotoxicity in Cancer Patients Treated with Chemotherapy},
  author = {Lima, Maria Adriely Cunha and Brito, Henrique Rodrigues de Almeida and Mitidieri, Gabriel Guimar{\~a}es and {de Souza}, Eduardo Paulo and Sobral, Ana Caroline Gois and Melo, Hemmely Hevelyn Maria Araujo and Vasconcelos, Guilherme Barreto and {de Almeida}, Berila Beatriz Dias and Figueiredo, Thain{\'a} de Ara{\'u}jo Diniz and Filho, Marcello Augusto Anchieta Santos and Santos, Douglas Silva Rosendo and {de Carvalho}, Renan Fontes and Oliveira, Halley Ferraro},
  year = {2022},
  journal = {International Journal of Health Sciences},
  volume = {16},
  number = {6},
  pages = {39--46},
  issn = {1658-3639},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9682875/},
  urldate = {2024-03-28},
  abstract = {Objective: The objective of the study was to assess the incidence of chemotherapy cardiotoxicity. Methods: This is a systematic review carried out through the PubMed, VHL and Scientific Electronic Library Online databases, using the descriptors ``Cardiotoxicity'' and ``Chemotherapy'' associated with the Boolean operator ``AND.'' Initially, 15,090 articles were found between 2015 and 2021. After applying the defined inclusion and exclusion criteria, 80 studies remained, of which 27 underwent complete reading, after which all were included in the study. Results: In total, 32,009 cancer patients were analyzed, of which 27,270 (85.2\%) were female. Breast cancer was the most frequent neoplasm, with 11,145 (34.8\%) cases. Regarding the type of chemotherapy, anthracycline was the most prevalent, analyzed in 18 (66.7\%) studies, followed by trastuzumab, in 9 (33.3\%) studies. Of the studies evaluated, five did not present any case of cardiotoxicity, a total of 2255 (8.3\%) cases were recorded, in addition other outcomes mentioned in patients after chemotherapy were arrhythmia (n = 522), acute coronary syndrome (n = 185), diastolic dysfunction (n = 184), cardiomyopathy (n = 161), and arterial hypertension (n = 89). Conclusion: Post-chemotherapeutic cardiotoxicity was mentioned in most studies, being present in a relevant percentage of the sample. Furthermore, these patients may develop other cardiovascular events.},
  pmcid = {PMC9682875},
  pmid = {36475028}
}

@misc{liMultitaskPretrainingLanguage2022,
  title = {Multi-Task {{Pre-training Language Model}} for {{Semantic Network Completion}}},
  author = {Li, Da and Yang, Sen and Xu, Kele and Yi, Ming and He, Yukai and Wang, Huaimin},
  year = {2022},
  month = apr,
  number = {arXiv:2201.04843},
  eprint = {2201.04843},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2201.04843},
  urldate = {2022-06-29},
  abstract = {Semantic networks, such as the knowledge graph, can represent the knowledge leveraging the graph structure. Although the knowledge graph shows promising values in natural language processing, it suffers from incompleteness. This paper focuses on knowledge graph completion by predicting linkage between entities, which is a fundamental yet critical task. Semantic matching is a potential solution as it can deal with unseen entities, which the translational distance based methods struggle with. However, to achieve competitive performance as translational distance based methods, semantic matching based methods require large-scale datasets for the training purpose, which are typically unavailable in practical settings. Therefore, we employ the language model and introduce a novel knowledge graph architecture named LP-BERT, which contains two main stages: multi-task pre-training and knowledge graph fine-tuning. In the pre-training phase, three tasks are taken to drive the model to learn the relationship from triples by predicting either entities or relations. While in the fine-tuning phase, inspired by contrastive learning, we design a triple-style negative sampling in a batch, which greatly increases the proportion of negative sampling while keeping the training time almost unchanged. Furthermore, we propose a new data augmentation method utilizing the inverse relationship of triples to improve the performance and robustness of the model. To demonstrate the effectiveness of our method, we conduct extensive experiments on three widely-used datasets, WN18RR, FB15k-237, and UMLS. The experimental results demonstrate the superiority of our methods, and our approach achieves state-of-the-art results on WN18RR and FB15k-237 datasets. Significantly, Hits@10 indicator is improved by 5\% from previous state-of-the-art result on the WN18RR dataset while reaching 100\% on the UMLS dataset.},
  archiveprefix = {arXiv}
}

@inproceedings{linAlpacaTagActiveLearningbased2019a,
  title = {{{AlpacaTag}}: {{An Active Learning-based Crowd Annotation Framework}} for {{Sequence Tagging}}},
  shorttitle = {{{AlpacaTag}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{System Demonstrations}}},
  author = {Lin, Bill Yuchen and Lee, Dong-Ho and Xu, Frank F. and Lan, Ouyu and Ren, Xiang},
  year = {2019},
  pages = {58--63},
  publisher = {Association for Computational Linguistics},
  address = {Florence, Italy},
  doi = {10.18653/v1/P19-3010},
  urldate = {2023-08-02},
  langid = {english}
}

@article{lingAutomaticQuestionanswerPairs2024,
  title = {Automatic Question-Answer Pairs Generation Using Pre-Trained Large Language Models in Higher Education},
  author = {Ling, Jintao and Afzaal, Muhammad},
  year = {2024},
  month = jun,
  journal = {Computers and Education: Artificial Intelligence},
  volume = {6},
  pages = {100252},
  issn = {2666-920X},
  doi = {10.1016/j.caeai.2024.100252},
  urldate = {2024-09-26},
  abstract = {The process of manually generating question and answer (QA) pairs for assessments is known to be a time-consuming and energy-intensive task for teachers, specifically in higher education. Several studies have proposed various methods utilising pre-trained large language models for the generation of QA pairs. However, it is worth noting that these methods have primarily been evaluated on datasets that are not specifically educational in nature. Furthermore, the evaluation metrics and strategies employed in these studies differ significantly from those typically used in educational contexts. The present discourse fails to present a compelling case regarding the efficacy and practicality of stated methods within the context of higher education. This study aimed to examine multiple QA pairs generation approaches in relation to their performance and the efficacy and constraints within the context of higher education. The various approaches encompassed in this study comprise pipeline, joint, multi-task approach. The performance of these approaches under consideration was assessed on three datasets related to distinct courses. The evaluation integrates three automated methods, teacher assessments, and real-world educational evaluations to provide a comprehensive analysis. The comparison of various approaches was conducted by directly assessing their performance using the average scores of different automatic metrics on three datasets. The results of the teachers and real educational evaluation indicate that the assessments generated were beneficial in enhancing the understanding of concepts and overall performance of students. The implications of the findings from this study hold significant importance in enhancing the efficacy of QA pair generation tools within the context of higher education.},
  keywords = {Automatic evaluation,Higher education,Pre-trained language model,Question-answer pairs generation,Real-educational evaluation}
}

@inproceedings{linLearningEntityRelation2015,
  title = {Learning {{Entity}} and {{Relation Embeddings}} for {{Knowledge Graph Completion}}},
  booktitle = {Twenty-{{Ninth AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Lin, Yankai and Liu, Zhiyuan and Sun, Maosong and Liu, Yang and Zhu, Xuan},
  year = {2015},
  month = feb,
  url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9571},
  urldate = {2021-12-07},
  abstract = {Knowledge graph completion aims to perform link prediction between entities. In this paper, we consider the approach of knowledge graph embeddings. Recently, models such as TransE and TransH build entity and relation embeddings by regarding a relation as translation from head entity to tail entity. We note that these models simply put both entities and relations within the same semantic space. In fact, an entity may have multiple aspects and various relations may focus on different aspects of entities, which makes a common space insufficient for modeling. In this paper, we propose TransR to build entity and relation embeddings in separate entity space and relation spaces. Afterwards, we learn embeddings by first projecting entities from entity space to corresponding relation space and then building translations between projected entities. In experiments, we evaluate our models on three tasks including link prediction, triple classification and relational fact extraction. Experimental results show significant and consistent improvements compared to state-of-the-art baselines including TransE and TransH.},
  copyright = {Authors who publish a paper in this conference agree to the following terms:   Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.  The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.  The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys' fees incurred therein.  Author(s) retain all proprietary rights other than copyright (such as patent rights).  Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.  Author(s) may reproduce, or have reproduced, their article/paper for the author's personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author's employer, and then only on the author's or the employer's own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author's or the employer's creation (including tables of contents with links to other papers) without AAAI's written permission.  Author(s) may make limited distribution of all or portions of their article/paper prior to publication.  In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.  In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
  langid = {english}
}

@inproceedings{linModelingRelationPaths2015,
  title = {Modeling {{Relation Paths}} for {{Representation Learning}} of {{Knowledge Bases}}},
  booktitle = {Proceedings of the 2015 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Lin, Yankai and Liu, Zhiyuan and Luan, Huanbo and Sun, Maosong and Rao, Siwei and Liu, Song},
  year = {2015},
  pages = {705--714},
  publisher = {Association for Computational Linguistics},
  address = {Lisbon, Portugal},
  doi = {10.18653/v1/D15-1082},
  urldate = {2021-12-09},
  abstract = {Representation learning of knowledge bases aims to embed both entities and relations into a low-dimensional space. Most existing methods only consider direct relations in representation learning. We argue that multiple-step relation paths also contain rich inference patterns between entities, and propose a path-based representation learning model. This model considers relation paths as translations between entities for representation learning, and addresses two key challenges: (1) Since not all relation paths are reliable, we design a path-constraint resource allocation algorithm to measure the reliability of relation paths. (2) We represent relation paths via semantic composition of relation embeddings. Experimental results on real-world datasets show that, as compared with baselines, our model achieves significant and consistent improvements on knowledge base completion and relation extraction from text. The source code of this paper can be obtained from https://github.com/mrlyk423/ relation\_extraction.},
  langid = {english}
}

@misc{linRecR1BridgingGenerative2025,
  title = {Rec-{{R1}}: {{Bridging Generative Large Language Models}} and {{User-Centric Recommendation Systems}} via {{Reinforcement Learning}}},
  shorttitle = {Rec-{{R1}}},
  author = {Lin, Jiacheng and Wang, Tian and Qian, Kun},
  year = {2025},
  month = may,
  number = {arXiv:2503.24289},
  eprint = {2503.24289},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.24289},
  urldate = {2025-06-04},
  abstract = {We propose Rec-R1, a general reinforcement learning framework that bridges large language models (LLMs) with recommendation systems through closed-loop optimization. Unlike prompting and supervised fine-tuning (SFT), Rec-R1 directly optimizes LLM generation using feedback from a fixed black-box recommendation model, without relying on synthetic SFT data from proprietary models such as GPT-4o. This avoids the substantial cost and effort required for data distillation. To verify the effectiveness of Rec-R1, we evaluate it on two representative tasks: product search and sequential recommendation. Experimental results demonstrate that Rec-R1 not only consistently outperforms prompting- and SFT-based methods, but also achieves significant gains over strong discriminative baselines, even when used with simple retrievers such as BM25. Moreover, Rec-R1 preserves the general-purpose capabilities of the LLM, unlike SFT, which often impairs instruction-following and reasoning. These findings suggest Rec-R1 as a promising foundation for continual task-specific adaptation without catastrophic forgetting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval}
}

@misc{linUnlockingSpellBase2023,
  title = {The {{Unlocking Spell}} on {{Base LLMs}}: {{Rethinking Alignment}} via {{In-Context Learning}}},
  shorttitle = {The {{Unlocking Spell}} on {{Base LLMs}}},
  author = {Lin, Bill Yuchen and Ravichander, Abhilasha and Lu, Ximing and Dziri, Nouha and Sclar, Melanie and Chandu, Khyathi and Bhagavatula, Chandra and Choi, Yejin},
  year = {2023},
  month = dec,
  number = {arXiv:2312.01552},
  eprint = {2312.01552},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2312.01552},
  urldate = {2023-12-13},
  abstract = {The alignment tuning process of large language models (LLMs) typically involves instruction learning through supervised fine-tuning (SFT) and preference tuning via reinforcement learning from human feedback (RLHF). A recent study, LIMA (Zhou et al. 2023), shows that using merely 1K examples for SFT can achieve significant alignment performance as well, suggesting that the effect of alignment tuning might be "superficial." This raises questions about how exactly the alignment tuning transforms a base LLM. We analyze the effect of alignment tuning by examining the token distribution shift between base LLMs and their aligned counterpart. Our findings reveal that base LLMs and their alignment-tuned versions perform nearly identically in decoding on the majority of token positions. Most distribution shifts occur with stylistic tokens. These direct evidence strongly supports the Superficial Alignment Hypothesis suggested by LIMA. Based on these findings, we rethink the alignment of LLMs by posing the research question: how effectively can we align base LLMs without SFT or RLHF? To address this, we introduce a simple, tuning-free alignment method, URIAL. URIAL achieves effective alignment purely through in-context learning (ICL) with base LLMs, requiring as few as three constant stylistic examples and a system prompt. We conduct a fine-grained and interpretable evaluation on a diverse set of examples, named JUST-EVAL-INSTRUCT. Results demonstrate that base LLMs with URIAL can match or even surpass the performance of LLMs aligned with SFT or SFT+RLHF. We show that the gap between tuning-free and tuning-based alignment methods can be significantly reduced through strategic prompting and ICL. Our findings on the superficial nature of alignment tuning and results with URIAL suggest that deeper analysis and theoretical understanding of alignment is crucial to future LLM research.},
  archiveprefix = {arXiv}
}

@article{lippiCLAUDETTEAutomatedDetector2019,
  title = {{{CLAUDETTE}}: An {{Automated Detector}} of {{Potentially Unfair Clauses}} in {{Online Terms}} of {{Service}}},
  shorttitle = {{{CLAUDETTE}}},
  author = {Lippi, Marco and Palka, Przemyslaw and Contissa, Giuseppe and Lagioia, Francesca and Micklitz, Hans-Wolfgang and Sartor, Giovanni and Torroni, Paolo},
  year = {2019},
  month = jun,
  journal = {Artificial Intelligence and Law},
  volume = {27},
  number = {2},
  eprint = {1805.01217},
  primaryclass = {cs},
  pages = {117--139},
  issn = {0924-8463, 1572-8382},
  doi = {10.1007/s10506-019-09243-2},
  urldate = {2023-11-01},
  abstract = {Terms of service of on-line platforms too often contain clauses that are potentially unfair to the consumer. We present an experimental study where machine learning is employed to automatically detect such potentially unfair clauses. Results show that the proposed system could provide a valuable tool for lawyers and consumers alike.},
  archiveprefix = {arXiv}
}

@misc{liuAutonomousAgentsCollaborative2024,
  title = {Autonomous {{Agents}} for {{Collaborative Task}} under {{Information Asymmetry}}},
  author = {Liu, Wei and Wang, Chenxi and Wang, Yifei and Xie, Zihao and Qiu, Rennai and Dang, Yufan and Du, Zhuoyun and Chen, Weize and Yang, Cheng and Qian, Chen},
  year = {2024},
  month = jun,
  number = {arXiv:2406.14928},
  eprint = {2406.14928},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.14928},
  urldate = {2024-08-08},
  abstract = {Large Language Model Multi-Agent Systems (LLM-MAS) have achieved great progress in solving complex tasks. It performs communication among agents within the system to collaboratively solve tasks, under the premise of shared information. However, when agents' communication is leveraged to enhance human cooperation, a new challenge arises due to information asymmetry, since each agent can only access the information of its human user. Previous MAS struggle to complete tasks under this condition. To address this, we propose a new MAS paradigm termed iAgents, which denotes Informative Multi-Agent Systems. In iAgents, the human social network is mirrored in the agent network, where agents proactively exchange human information necessary for task resolution, thereby overcoming information asymmetry. iAgents employs a novel agent reasoning mechanism, InfoNav, to navigate agents' communication towards effective information exchange. Together with InfoNav, iAgents organizes human information in a mixed memory to provide agents with accurate and comprehensive information for exchange. Additionally, we introduce InformativeBench, the first benchmark tailored for evaluating LLM agents' task-solving ability under information asymmetry. Experimental results show that iAgents can collaborate within a social network of 140 individuals and 588 relationships, autonomously communicate over 30 turns, and retrieve information from nearly 70,000 messages to complete tasks within 3 minutes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,Computer Science - Multiagent Systems,Computer Science - Social and Information Networks}
}

@misc{liuCOLosSALBenchmarkColdstart2023,
  title = {{{COLosSAL}}: {{A Benchmark}} for {{Cold-start Active Learning}} for {{3D Medical Image Segmentation}}},
  shorttitle = {{{COLosSAL}}},
  author = {Liu, Han and Li, Hao and Yao, Xing and Fan, Yubo and Hu, Dewei and Dawant, Benoit and Nath, Vishwesh and Xu, Zhoubing and Oguz, Ipek},
  year = {2023},
  month = jul,
  number = {arXiv:2307.12004},
  eprint = {2307.12004},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2307.12004},
  urldate = {2024-04-25},
  abstract = {Medical image segmentation is a critical task in medical image analysis. In recent years, deep learning based approaches have shown exceptional performance when trained on a fully-annotated dataset. However, data annotation is often a significant bottleneck, especially for 3D medical images. Active learning (AL) is a promising solution for efficient annotation but requires an initial set of labeled samples to start active selection. When the entire data pool is unlabeled, how do we select the samples to annotate as our initial set? This is also known as the cold-start AL, which permits only one chance to request annotations from experts without access to previously annotated data. Coldstart AL is highly relevant in many practical scenarios but has been under-explored, especially for 3D medical segmentation tasks requiring substantial annotation effort. In this paper, we present a benchmark named COLosSAL by evaluating six cold-start AL strategies on five 3D medical image segmentation tasks from the public Medical Segmentation Decathlon collection. We perform a thorough performance analysis and explore important open questions for cold-start AL, such as the impact of budget on different strategies. Our results show that cold-start AL is still an unsolved problem for 3D segmentation tasks but some important trends have been observed. The code repository, data partitions, and baseline results for the complete benchmark are publicly available at https://github.com/MedICL-VU/COLosSAL.},
  archiveprefix = {arXiv},
  langid = {american},
  keywords = {Cold Start Active Learning}
}

@inproceedings{liuContrastiveAttentionAutomatic2021,
  title = {Contrastive {{Attention}} for {{Automatic Chest X-ray Report Generation}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL-IJCNLP}} 2021},
  author = {Liu, Fenglin and Yin, Changchang and Wu, Xian and Ge, Shen and Zhang, Ping and Sun, Xu},
  editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
  year = {2021},
  month = aug,
  pages = {269--280},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.findings-acl.23},
  urldate = {2024-03-18}
}

@misc{liuGEvalNLGEvaluation2023,
  title = {G-{{Eval}}: {{NLG Evaluation}} Using {{GPT-4}} with {{Better Human Alignment}}},
  shorttitle = {G-{{Eval}}},
  author = {Liu, Yang and Iter, Dan and Xu, Yichong and Wang, Shuohang and Xu, Ruochen and Zhu, Chenguang},
  year = {2023},
  month = may,
  number = {arXiv:2303.16634},
  eprint = {2303.16634},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.16634},
  urldate = {2023-11-13},
  abstract = {The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose preliminary analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluators having a bias towards the LLM-generated texts. The code is at https://github.com/nlpyang/geval},
  archiveprefix = {arXiv}
}

@misc{liuInfiGUIR1AdvancingMultimodal2025,
  title = {{{InfiGUI-R1}}: {{Advancing Multimodal GUI Agents}} from {{Reactive Actors}} to {{Deliberative Reasoners}}},
  shorttitle = {{{InfiGUI-R1}}},
  author = {Liu, Yuhang and Li, Pengxiang and Xie, Congkai and Hu, Xavier and Han, Xiaotian and Zhang, Shengyu and Yang, Hongxia and Wu, Fei},
  year = {2025},
  month = apr,
  number = {arXiv:2504.14239},
  eprint = {2504.14239},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.14239},
  urldate = {2025-06-04},
  abstract = {Multimodal Large Language Models (MLLMs) have powered Graphical User Interface (GUI) Agents, showing promise in automating tasks on computing devices. Recent works have begun exploring reasoning in GUI tasks with encouraging results. However, many current approaches rely on manually designed reasoning templates, which may result in reasoning that is not sufficiently robust and adaptive for complex GUI environments. Meanwhile, some existing agents continue to operate as Reactive Actors, relying primarily on implicit reasoning that may lack sufficient depth for GUI tasks demanding planning and error recovery. We argue that advancing these agents requires a shift from reactive acting towards acting based on deliberate reasoning. To facilitate this transformation, we introduce InfiGUI-R1, an MLLM-based GUI agent developed through our Actor2Reasoner framework, a reasoning-centric, two-stage training approach designed to progressively evolve agents from Reactive Actors to Deliberative Reasoners. The first stage, Reasoning Injection, focuses on establishing a basic reasoner. We employ Spatial Reasoning Distillation to transfer cross-modal spatial reasoning capabilities from teacher models to MLLMs through trajectories with explicit reasoning steps, enabling models to integrate GUI visual-spatial information with logical reasoning before action generation. The second stage, Deliberation Enhancement, refines the basic reasoner into a deliberative one using Reinforcement Learning. This stage introduces two approaches: Sub-goal Guidance, which rewards models for generating accurate intermediate sub-goals, and Error Recovery Scenario Construction, which creates failure-and-recovery training scenarios from identified prone-to-error steps. Experimental results show InfiGUI-R1 achieves strong performance in GUI grounding and trajectory tasks. Resources at https://github.com/Reallm-Labs/InfiGUI-R1.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@article{LiuJiYuBiLSTMDeShuXueZhuGuanTiZiDongYueJuanFangFa2018,
  title = {{Bi-LSTM}},
  author = {,  and ,  and ,  and , },
  year = {2018},
  journal = {},
  number = {02},
  pages = {109--113},
  issn = {1674-2877},
  url = {https://kns.cnki.net/kcms/detail/frame/list.aspx?dbcode=CJFD&filename=glkw201802046&dbname=CJFDLAST2018&RefType=1&vl=z1ZZ6_Hn98HW9JZgxp45CcyQ5kAhBfGukyc86STLgdtE0IJzj-DaXMBQuvh5JoS-},
  urldate = {2022-02-11},
  abstract = {TF-IDF,,,,83.17\%},
  langid = {chinese},
  annotation = {00000}
}

@inproceedings{liuKeywordawareAbstractiveSummarization2021,
  title = {Keyword-Aware {{Abstractive Summarization}} by {{Extracting Set-level Intermediate Summaries}}},
  booktitle = {Proceedings of the {{Web Conference}} 2021},
  author = {Liu, Yizhu and Jia, Qi and Zhu, Kenny},
  year = {2021},
  month = apr,
  series = {{{WWW}} '21},
  pages = {3042--3054},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3442381.3449906},
  urldate = {2022-07-27},
  abstract = {Abstractive summarization is useful in providing a summary or a digest of news or other web texts and enhancing users reading experience, especially when they are reading on small displays such as mobile phones. However, existing encoder-decoder summarization models have difficulty learning the latent alignment between source documents and summaries because of their vast disparity in length. In this paper, we propose a extractor-abstractor framework in which the keyword-based extractor selects a few sets of salient sentences from the input document and then the abstractor paraphrases these sets of sentences in parallel, which are more aligned to the summary, to generate the final summary. The new extractor and abstractor are pretrained from a set of ``pseudo summaries'' extracted by specially designed heuristics, and then further trained together in a reinforcement learning framework. The results show that the proposed model generates high-quality summaries with faster training speed and less training memory footprint, and outperforms the state-of-the-art models on CNN/Daily Mail, Webis-TLDR-17, Webis-Snippet-20, WikiHow and DUC-2002 datasets.},
  isbn = {978-1-4503-8312-7}
}

@misc{liuLargeLanguageModels2023,
  title = {Large {{Language Models}} Are {{Few-Shot Health Learners}}},
  author = {Liu, Xin and McDuff, Daniel and Kovacs, Geza and {Galatzer-Levy}, Isaac and Sunshine, Jacob and Zhan, Jiening and Poh, Ming-Zher and Liao, Shun and Di Achille, Paolo and Patel, Shwetak},
  year = {2023},
  month = may,
  number = {arXiv:2305.15525},
  eprint = {2305.15525},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.15525},
  urldate = {2024-01-16},
  abstract = {Large language models (LLMs) can capture rich representations of concepts that are useful for real-world tasks. However, language alone is limited. While existing LLMs excel at text-based inferences, health applications require that models be grounded in numerical data (e.g., vital signs, laboratory values in clinical domains; steps, movement in the wellness domain) that is not easily or readily expressed as text in existing training corpus. We demonstrate that with only few-shot tuning, a large language model is capable of grounding various physiological and behavioral time-series data and making meaningful inferences on numerous health tasks for both clinical and wellness contexts. Using data from wearable and medical sensor recordings, we evaluate these capabilities on the tasks of cardiac signal analysis, physical activity recognition, metabolic calculation (e.g., calories burned), and estimation of stress reports and mental health screeners.},
  archiveprefix = {arXiv}
}

@inproceedings{liuLearningMultiGraphNeural2021,
  title = {Learning {{Multi-Graph Neural Network}} for {{Data-Driven Job Skill Prediction}}},
  booktitle = {2021 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Liu, Liting and Zhang, Wenzheng and Liu, Jie and Shi, Wenxuan and Huang, Yalou},
  year = {2021},
  month = jul,
  pages = {1--8},
  issn = {2161-4407},
  doi = {10.1109/IJCNN52387.2021.9533402},
  abstract = {Specifying an appropriate skill set for a job position is critical for talent recruitment. However, it is often more difficult than people think since it needs a great understanding of the role of the position, related technologies, and even the global situation of the job market. To this end, we propose to learn the mapping between the job position description and its required skills in a data-driven manner. This task is challenging due to the complex mapping relationships between job descriptions and skills, which is caused by complex influencing factors. In this paper, we propose a novel Multi-Graph Neural Network based Skill Prediction model (MGNSP) to make skill prediction by learning effective deep semantics matching of job positions and skills. Specifically, to capture the complex heterogeneous relations among the job positions, skills, and meta information, we devise a joint learning approach of graph neural networks for multiple information networks, which are J-Net, S-Net and JS-Net, respectively. After jointly learning complementary semantics of job positions and skills with three multi-layer graph neural networks from these information networks, the skills are predicted by learning to match their representations. Extensive experimental results on a real-world dataset validate the effectiveness of our model.}
}

@misc{liuLLMPoweredHierarchicalLanguage2024,
  title = {{{LLM-Powered Hierarchical Language Agent}} for {{Real-time Human-AI Coordination}}},
  author = {Liu, Jijia and Yu, Chao and Gao, Jiaxuan and Xie, Yuqing and Liao, Qingmin and Wu, Yi and Wang, Yu},
  year = {2024},
  month = jan,
  number = {arXiv:2312.15224},
  eprint = {2312.15224},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2312.15224},
  urldate = {2024-09-11},
  abstract = {AI agents powered by Large Language Models (LLMs) have made significant advances, enabling them to assist humans in diverse complex tasks and leading to a revolution in human-AI coordination. LLM-powered agents typically require invoking LLM APIs and employing artificially designed complex prompts, which results in high inference latency. While this paradigm works well in scenarios with minimal interactive demands, such as code generation, it is unsuitable for highly interactive and real-time applications, such as gaming. Traditional gaming AI often employs small models or reactive policies, enabling fast inference but offering limited task completion and interaction abilities. In this work, we consider Overcooked as our testbed where players could communicate with natural language and cooperate to serve orders. We propose a Hierarchical Language Agent (HLA) for human-AI coordination that provides both strong reasoning abilities while keeping real-time execution. In particular, HLA adopts a hierarchical framework and comprises three modules: a proficient LLM, referred to as Slow Mind, for intention reasoning and language interaction, a lightweight LLM, referred to as Fast Mind, for generating macro actions, and a reactive policy, referred to as Executor, for transforming macro actions into atomic actions. Human studies show that HLA outperforms other baseline agents, including slow-mind-only agents and fast-mind-only agents, with stronger cooperation abilities, faster responses, and more consistent language communications.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction}
}

@inproceedings{liUnifiedDemonstrationRetriever2023,
  title = {Unified {{Demonstration Retriever}} for {{In-Context Learning}}},
  booktitle = {Proceedings of the 61st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Li, Xiaonan and Lv, Kai and Yan, Hang and Lin, Tianyang and Zhu, Wei and Ni, Yuan and Xie, Guotong and Wang, Xiaoling and Qiu, Xipeng},
  editor = {Rogers, Anna and {Boyd-Graber}, Jordan and Okazaki, Naoaki},
  year = {2023},
  month = jul,
  pages = {4644--4668},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  doi = {10.18653/v1/2023.acl-long.256},
  urldate = {2023-11-14},
  abstract = {In-context learning is a new learning paradigm where a language model conditions on a few input-output pairs (demonstrations) and a test input, and directly outputs the prediction. It has been shown sensitive to the provided demonstrations and thus promotes the research of demonstration retrieval: given a test input, relevant examples are retrieved from the training set to serve as informative demonstrations for in-context learning. While previous works train task-specific retrievers for several tasks separately, these methods are hard to transfer and scale on various tasks, and separately trained retrievers will cause a lot of parameter storage and deployment cost. In this paper, we propose Unified Demonstration Retriever (UDR), a single model to retrieve demonstrations for a wide range of tasks. To train UDR, we cast various tasks' training signals into a unified list-wise ranking formulation by language model's feedback. Then we propose a multi-task list-wise ranking training framework with an iterative mining strategy to find high-quality candidates, which can help UDR fully incorporate various tasks' signals. Experiments on 30+ tasks across 13 task families and multiple data domains show that UDR significantly outperforms baselines. Further analyses show the effectiveness of each proposed component and UDR's strong ability in various scenarios including different LMs (1.3B 175B), unseen datasets, varying demonstration quantities, etc. We will release the code and model checkpoint after review.}
}

@misc{liuPretrainPromptPredict2021,
  title = {Pre-Train, {{Prompt}}, and {{Predict}}: {{A Systematic Survey}} of {{Prompting Methods}} in {{Natural Language Processing}}},
  shorttitle = {Pre-Train, {{Prompt}}, and {{Predict}}},
  author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  year = {2021},
  month = jul,
  number = {arXiv:2107.13586},
  eprint = {2107.13586},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2107.13586},
  urldate = {2022-08-02},
  abstract = {This paper surveys and organizes research works in a new paradigm in natural language processing, which we dub "prompt-based learning". Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y{\textbar}x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x' that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: it allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this paper we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.the choice of pre-trained models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts, but also release other resources, e.g., a website http://pretrain.nlpedia.ai/ including constantly-updated survey, and paperlist.},
  archiveprefix = {arXiv}
}

@misc{liuRadiologyGPTLargeLanguage2023,
  title = {Radiology-{{GPT}}: {{A Large Language Model}} for {{Radiology}}},
  shorttitle = {Radiology-{{GPT}}},
  author = {Liu, Zhengliang and Zhong, Aoxiao and Li, Yiwei and Yang, Longtao and Ju, Chao and Wu, Zihao and Ma, Chong and Shu, Peng and Chen, Cheng and Kim, Sekeun and Dai, Haixing and Zhao, Lin and Zhu, Dajiang and Liu, Jun and Liu, Wei and Shen, Dinggang and Li, Xiang and Li, Quanzheng and Liu, Tianming},
  year = {2023},
  month = jun,
  number = {arXiv:2306.08666},
  eprint = {2306.08666},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2306.08666},
  urldate = {2024-01-20},
  abstract = {We introduce Radiology-GPT, a large language model for radiology. Using an instruction tuning approach on an extensive dataset of radiology domain knowledge, Radiology-GPT demonstrates superior performance compared to general language models such as StableLM, Dolly and LLaMA. It exhibits significant versatility in radiological diagnosis, research, and communication. This work serves as a catalyst for future developments in clinical NLP. The successful implementation of Radiology-GPT is indicative of the potential of localizing generative large language models, specifically tailored for distinctive medical specialties, while ensuring adherence to privacy standards such as HIPAA. The prospect of developing individualized, large-scale language models that cater to specific needs of various hospitals presents a promising direction. The fusion of conversational competence and domain-specific knowledge in these models is set to foster future development in healthcare AI. A demo of Radiology-GPT is available at https://huggingface.co/spaces/allen-eric/radiology-gpt.},
  archiveprefix = {arXiv}
}

@article{liuREINFORCEMENTLEARNINGWEB2018,
  title = {{{REINFORCEMENT LEARNING ON WEB INTERFACES USING WORKFLOW-GUIDED EXPLORATION}}},
  author = {Liu, Evan Zheran and Guu, Kelvin and Pasupat, Panupong and Shi, Tianlin and Liang, Percy},
  year = {2018},
  abstract = {Reinforcement learning (RL) agents improve through trial-and-error, but when reward is sparse and the agent cannot discover successful action sequences, learning stagnates. This has been a notable problem in training deep RL agents to perform web-based tasks, such as booking flights or replying to emails, where a single mistake can ruin the entire sequence of actions. A common remedy is to ``warmstart'' the agent by pre-training it to mimic expert demonstrations, but this is prone to overfitting. Instead, we propose to constrain exploration using demonstrations. From each demonstration, we induce high-level ``workflows'' which constrain the allowable actions at each time step to be similar to those in the demonstration (e.g., ``Step 1: click on a textbox; Step 2: enter some text''). Our exploration policy then learns to identify successful workflows and samples actions that satisfy these workflows. Workflows prune out bad exploration directions and accelerate the agent's ability to discover rewards. We use our approach to train a novel neural policy designed to handle the semi-structured nature of websites, and evaluate on a suite of web tasks, including the recent World of Bits benchmark. We achieve new state-of-the-art results, and show that workflow-guided exploration improves sample efficiency over behavioral cloning by more than 100x.},
  langid = {english}
}

@inproceedings{liuRetrieveReasonRefine2022,
  title = {Retrieve, {{Reason}}, and {{Refine}}: {{Generating Accurate}} and {{Faithful Patient Instructions}}},
  shorttitle = {Retrieve, {{Reason}}, and {{Refine}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Liu, Fenglin and Yang, Bang and You, Chenyu and Wu, Xian and Ge, Shen and Liu, Zhangdaihong and Sun, Xu and Yang, Yang and Clifton, David A.},
  year = {2022},
  month = oct,
  url = {https://openreview.net/forum?id=dp0zWsdOV1h},
  urldate = {2024-03-18},
  abstract = {The "Patient Instruction" (PI), which contains critical instructional information provided both to carers and to the patient at the time of discharge, is essential for the patient to manage their condition outside hospital. An accurate and easy-to-follow PI can improve the self-management of patients which can in turn reduce hospital readmission rates. However, writing an appropriate PI can be extremely time consuming for physicians, and is subject to being incomplete or error-prone for (potentially overworked) physicians. Therefore, we propose a new task that can provide an objective means of avoiding incompleteness, while reducing clinical workload: the automatic generation of the PI, which is imagined as being a document that the clinician can review, modify, and approve as necessary (rather than taking the human "out of the loop"). We build a benchmark clinical dataset and propose the Re\${\textasciicircum}3\$Writer, which imitates the working patterns of physicians to first retrieve related working experience from historical PIs written by physicians, then reason related medical knowledge. Finally, it refines the retrieved working experience and reasoned medical knowledge to extract useful information, which is used to generate the PI for previously-unseen patient according to their health records during hospitalization. Our experiments show that, using our method, the performance of 6 different models can be substantially boosted across all metrics, with up to 20\%, 11\%, and 19\% relative improvements in BLEU-4, ROUGE-L, and METEOR, respectively. Meanwhile, we show results from human evaluations to measure the effectiveness in terms of its usefulness for clinical practice. The code is available at https://github.com/AI-in-Health/Patient-Instructions.},
  langid = {english}
}

@article{liuRoBERTaRobustlyOptimized2019,
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  month = jul,
  journal = {arXiv:1907.11692 [cs]},
  eprint = {1907.11692},
  primaryclass = {cs},
  url = {https://openreview.net/forum?id=SyxS0T4tvS},
  urldate = {2022-02-11},
  abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  archiveprefix = {arXiv},
  langid = {english},
  annotation = {02721}
}

@misc{liuScissorhandsExploitingPersistence2023,
  title = {Scissorhands: {{Exploiting}} the {{Persistence}} of {{Importance Hypothesis}} for {{LLM KV Cache Compression}} at {{Test Time}}},
  shorttitle = {Scissorhands},
  author = {Liu, Zichang and Desai, Aditya and Liao, Fangshuo and Wang, Weitao and Xie, Victor and Xu, Zhaozhuo and Kyrillidis, Anastasios and Shrivastava, Anshumali},
  year = {2023},
  month = may,
  number = {arXiv:2305.17118},
  eprint = {2305.17118},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2305.17118},
  urldate = {2023-06-23},
  abstract = {Large language models(LLMs) have sparked a new wave of exciting AI applications. Hosting these models at scale requires significant memory resources. One crucial memory bottleneck for the deployment stems from the context window. It is commonly recognized that model weights are memory hungry; however, the size of key-value embedding stored during the generation process (KV cache) can easily surpass the model size. The enormous size of the KV cache puts constraints on the inference batch size, which is crucial for high throughput inference workload. Inspired by an interesting observation of the attention scores, we hypothesize the persistence of importance: only pivotal tokens, which had a substantial influence at one step, will significantly influence future generations. Based on our empirical verification and theoretical analysis around this hypothesis, we propose Scissorhands, a system that maintains the memory usage of the KV cache at a fixed budget without finetuning the model. In essence, Scissorhands manages the KV cache by storing the pivotal tokens with a higher probability. We validate that Scissorhands reduces the inference memory usage of the KV cache by up to 5X without compromising model quality. We further demonstrate that Scissorhands can be combined with 4-bit quantization, traditionally used to compress model weights, to achieve up to 20X compression.},
  archiveprefix = {arXiv}
}

@misc{liuUncertaintyEstimationQuantification2024,
  title = {Uncertainty {{Estimation}} and {{Quantification}} for {{LLMs}}: {{A Simple Supervised Approach}}},
  shorttitle = {Uncertainty {{Estimation}} and {{Quantification}} for {{LLMs}}},
  author = {Liu, Linyu and Pan, Yu and Li, Xiaocheng and Chen, Guanting},
  year = {2024},
  month = jun,
  number = {arXiv:2404.15993},
  eprint = {2404.15993},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.15993},
  urldate = {2024-09-16},
  abstract = {In this paper, we study the problem of uncertainty estimation and calibration for LLMs. We first formulate the uncertainty estimation problem for LLMs and then propose a supervised approach that takes advantage of the labeled datasets and estimates the uncertainty of the LLMs' responses. Based on the formulation, we illustrate the difference between the uncertainty estimation for LLMs and that for standard ML models and explain why the hidden neurons of the LLMs may contain uncertainty information. Our designed approach demonstrates the benefits of utilizing hidden activations to enhance uncertainty estimation across various tasks and shows robust transferability in out-of-distribution settings. We distinguish the uncertainty estimation task from the uncertainty calibration task and show that a better uncertainty estimation mode leads to a better calibration performance. Furthermore, our method is easy to implement and adaptable to different levels of model accessibility including black box, grey box, and white box.},
  archiveprefix = {arXiv},
  keywords = {68T07 68T50,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{liuVisualAgentBenchLargeMultimodal2024,
  title = {{{VisualAgentBench}}: {{Towards Large Multimodal Models}} as {{Visual Foundation Agents}}},
  shorttitle = {{{VisualAgentBench}}},
  author = {Liu, Xiao and Zhang, Tianjie and Gu, Yu and Iong, Iat Long and Xu, Yifan and Song, Xixuan and Zhang, Shudan and Lai, Hanyu and Liu, Xinyi and Zhao, Hanlin and Sun, Jiadai and Yang, Xinyue and Yang, Yu and Qi, Zehan and Yao, Shuntian and Sun, Xueqiao and Cheng, Siyi and Zheng, Qinkai and Yu, Hao and Zhang, Hanchen and Hong, Wenyi and Ding, Ming and Pan, Lihang and Gu, Xiaotao and Zeng, Aohan and Du, Zhengxiao and Song, Chan Hee and Su, Yu and Dong, Yuxiao and Tang, Jie},
  year = {2024},
  month = aug,
  number = {arXiv:2408.06327},
  eprint = {2408.06327},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.06327},
  urldate = {2024-11-01},
  abstract = {Large Multimodal Models (LMMs) have ushered in a new era in artificial intelligence, merging capabilities in both language and vision to form highly capable Visual Foundation Agents. These agents are postulated to excel across a myriad of tasks, potentially approaching general artificial intelligence. However, existing benchmarks fail to sufficiently challenge or showcase the full potential of LMMs in complex, real-world environments. To address this gap, we introduce VisualAgentBench (VAB), a comprehensive and pioneering benchmark specifically designed to train and evaluate LMMs as visual foundation agents across diverse scenarios, including Embodied, Graphical User Interface, and Visual Design, with tasks formulated to probe the depth of LMMs' understanding and interaction capabilities. Through rigorous testing across nine proprietary LMM APIs and eight open models, we demonstrate the considerable yet still developing agent capabilities of these models. Additionally, VAB constructs a trajectory training set constructed through hybrid methods including Program-based Solvers, LMM Agent Bootstrapping, and Human Demonstrations, promoting substantial performance improvements in LMMs through behavior cloning. Our work not only aims to benchmark existing models but also provides a solid foundation for future development into visual foundation agents. Code, train {\textbackslash}\& test data, and part of fine-tuned open LMMs are available at {\textbackslash}url\{https://github.com/THUDM/VisualAgentBench\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition}
}

@misc{liuVisualWebBenchHowFar2024,
  title = {{{VisualWebBench}}: {{How Far Have Multimodal LLMs Evolved}} in {{Web Page Understanding}} and {{Grounding}}?},
  shorttitle = {{{VisualWebBench}}},
  author = {Liu, Junpeng and Song, Yifan and Lin, Bill Yuchen and Lam, Wai and Neubig, Graham and Li, Yuanzhi and Yue, Xiang},
  year = {2024},
  month = apr,
  number = {arXiv:2404.05955},
  eprint = {2404.05955},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.05955},
  urldate = {2024-07-10},
  abstract = {Multimodal Large Language models (MLLMs) have shown promise in web-related tasks, but evaluating their performance in the web domain remains a challenge due to the lack of comprehensive benchmarks. Existing benchmarks are either designed for general multimodal tasks, failing to capture the unique characteristics of web pages, or focus on end-to-end web agent tasks, unable to measure fine-grained abilities such as OCR, understanding, and grounding. In this paper, we introduce {\textbackslash}bench\{\}, a multimodal benchmark designed to assess the capabilities of MLLMs across a variety of web tasks. {\textbackslash}bench\{\} consists of seven tasks, and comprises 1.5K human-curated instances from 139 real websites, covering 87 sub-domains. We evaluate 14 open-source MLLMs, Gemini Pro, Claude-3 series, and GPT-4V(ision) on {\textbackslash}bench\{\}, revealing significant challenges and performance gaps. Further analysis highlights the limitations of current MLLMs, including inadequate grounding in text-rich environments and subpar performance with low-resolution image inputs. We believe {\textbackslash}bench\{\} will serve as a valuable resource for the research community and contribute to the creation of more powerful and versatile MLLMs for web-related applications.},
  archiveprefix = {arXiv},
  keywords = {CoLM2024,Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@inproceedings{liuWhatMakesGood2022,
  title = {What {{Makes Good In-Context Examples}} for {{GPT-3}}?},
  booktitle = {Proceedings of {{Deep Learning Inside Out}} ({{DeeLIO}} 2022): {{The}} 3rd {{Workshop}} on {{Knowledge Extraction}} and {{Integration}} for {{Deep Learning Architectures}}},
  author = {Liu, Jiachang and Shen, Dinghan and Zhang, Yizhe and Dolan, Bill and Carin, Lawrence and Chen, Weizhu},
  editor = {Agirre, Eneko and Apidianaki, Marianna and Vuli{\'c}, Ivan},
  year = {2022},
  month = may,
  pages = {100--114},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland and Online},
  doi = {10.18653/v1/2022.deelio-1.10},
  urldate = {2023-11-14},
  abstract = {GPT-3 has attracted lots of attention due to its superior performance across a wide range of NLP tasks, especially with its in-context learning abilities. Despite its success, we found that the empirical results of GPT-3 depend heavily on the choice of in-context examples. In this work, we investigate whether there are more effective strategies for judiciously selecting in-context examples (relative to random sampling) that better leverage GPT-3's in-context learning capabilities. Inspired by the recent success of leveraging a retrieval module to augment neural networks, we propose to retrieve examples that are semantically-similar to a test query sample to formulate its corresponding prompt. Intuitively, the examples selected with such a strategy may serve as more informative inputs to unleash GPT-3's power of text generation. We evaluate the proposed approach on several natural language understanding and generation benchmarks, where the retrieval-based prompt selection approach consistently outperforms the random selection baseline. Moreover, it is observed that the sentence encoders fine-tuned on task-related datasets yield even more helpful retrieval results. Notably, significant gains are observed on tasks such as table-to-text generation (44.3\% on the ToTTo dataset) and open-domain question answering (45.5\% on the NQ dataset).}
}

@article{LiuZhiYuanSunMaoSongLinYanKaiXieRuoBingZhiShiBiaoShiXueXiYanJiuJinZhan2016,
  title = {{}},
  author = { and Liu Zhiyuan, Sun Maosong},
  year = {2016},
  month = feb,
  journal = {},
  volume = {53},
  number = {2},
  pages = {247},
  issn = {1000-1239},
  doi = {10.7544/issn1000-1239.2016.20160020},
  urldate = {2021-12-07},
  abstract = {...},
  langid = {chinese}
}

@article{luccioniEstimatingCarbonFootprint2023,
  title = {Estimating the {{Carbon Footprint}} of {{BLOOM}}, a {{176B Parameter Language Model}}},
  author = {Luccioni, Alexandra Sasha and Viguier, Sylvain and Ligozat, Anne-Laure},
  year = {2023},
  journal = {Journal of Machine Learning Research},
  volume = {24},
  number = {253},
  pages = {1--15},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v24/23-0069.html},
  urldate = {2024-02-13},
  abstract = {Progress in machine learning (ML) comes with a cost to the environment, given that training ML models requires computational resources, energy and materials. In the present article, we aim to quantify the carbon footprint of BLOOM, a 176-billion parameter language model, across its life cycle. We estimate that BLOOM's final training emitted approximately 24.7 tonnes of CO2eq if we consider only the dynamic power consumption, and 50.5 tonnes if we account for all processes ranging from equipment manufacturing to energy-based operational consumption. We also carry out an empirical study to measure the energy requirements and carbon emissions of its deployment for inference via an API endpoint receiving user queries in real-time. We conclude with a discussion regarding the difficulty of precisely estimating the carbon footprint of ML models and future research directions that can contribute towards improving carbon emissions reporting.}
}

@inproceedings{luContextualEmbeddingModel2022,
  title = {Contextual Embedding and Model Weighting by Fusing Domain Knowledge on Biomedical Question Answering},
  booktitle = {Proceedings of the 13th {{ACM International Conference}} on {{Bioinformatics}}, {{Computational Biology}} and {{Health Informatics}}},
  author = {Lu, Yuxuan and Yan, Jingya and Qi, Zhixuan and Ge, Zhongzheng and Du, Yongping},
  year = {2022},
  month = aug,
  series = {{{BCB}} '22},
  pages = {1--4},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3535508.3545508},
  urldate = {2023-12-15},
  abstract = {Biomedical Question Answering aims to obtain an answer to the given question from the biomedical domain. Due to its high requirement of biomedical domain knowledge, it is difficult for the model to learn domain knowledge from limited training data. We propose a contextual embedding method that combines open-domain QA model AoA Reader and BioBERT model pre-trained on biomedical domain data. We adopt unsupervised pre-training on large biomedical corpus and supervised fine-tuning on biomedical question answering dataset. Additionally, we adopt an MLP-based model weighting layer to automatically exploit the advantages of two models to provide the correct answer. The public dataset biomrc constructed from PubMed corpus is used to evaluate our method. Experimental results show that our model outperforms state-of-the-art system by a large margin.},
  isbn = {978-1-4503-9386-7}
}

@misc{luDeepSeekVLRealWorldVisionLanguage2024,
  title = {{{DeepSeek-VL}}: {{Towards Real-World Vision-Language Understanding}}},
  shorttitle = {{{DeepSeek-VL}}},
  author = {Lu, Haoyu and Liu, Wen and Zhang, Bo and Wang, Bingxuan and Dong, Kai and Liu, Bo and Sun, Jingxiang and Ren, Tongzheng and Li, Zhuoshu and Yang, Hao and Sun, Yaofeng and Deng, Chengqi and Xu, Hanwei and Xie, Zhenda and Ruan, Chong},
  year = {2024},
  month = mar,
  number = {arXiv:2403.05525},
  eprint = {2403.05525},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.05525},
  urldate = {2024-03-14},
  abstract = {We present DeepSeek-VL, an open-source Vision-Language (VL) Model designed for real-world vision and language understanding applications. Our approach is structured around three key dimensions: We strive to ensure our data is diverse, scalable, and extensively covers real-world scenarios including web screenshots, PDFs, OCR, charts, and knowledge-based content, aiming for a comprehensive representation of practical contexts. Further, we create a use case taxonomy from real user scenarios and construct an instruction tuning dataset accordingly. The fine-tuning with this dataset substantially improves the model's user experience in practical applications. Considering efficiency and the demands of most real-world scenarios, DeepSeek-VL incorporates a hybrid vision encoder that efficiently processes high-resolution images (1024 x 1024), while maintaining a relatively low computational overhead. This design choice ensures the model's ability to capture critical semantic and detailed information across various visual tasks. We posit that a proficient Vision-Language Model should, foremost, possess strong language abilities. To ensure the preservation of LLM capabilities during pretraining, we investigate an effective VL pretraining strategy by integrating LLM training from the beginning and carefully managing the competitive dynamics observed between vision and language modalities. The DeepSeek-VL family (both 1.3B and 7B models) showcases superior user experiences as a vision-language chatbot in real-world applications, achieving state-of-the-art or competitive performance across a wide range of visual-language benchmarks at the same model size while maintaining robust performance on language-centric benchmarks. We have made both 1.3B and 7B models publicly accessible to foster innovations based on this foundation model.},
  archiveprefix = {arXiv}
}

@inproceedings{luFantasticallyOrderedPrompts2022,
  title = {Fantastically {{Ordered Prompts}} and {{Where}} to {{Find Them}}: {{Overcoming Few-Shot Prompt Order Sensitivity}}},
  shorttitle = {Fantastically {{Ordered Prompts}} and {{Where}} to {{Find Them}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Lu, Yao and Bartolo, Max and Moore, Alastair and Riedel, Sebastian and Stenetorp, Pontus},
  editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
  year = {2022},
  month = may,
  pages = {8086--8098},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-long.556},
  urldate = {2023-11-14},
  abstract = {When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, fine-tuned, large, pretrained language models. We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance: essentially some permutations are ``fantastic'' and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the true few-shot setting as it requires additional annotated data. Instead, we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, we identify performant prompts. Our method yields a 13\% relative improvement for GPT-family models across eleven different established text classification tasks.}
}

@misc{luHumanStillWins2023,
  title = {Human {{Still Wins}} over {{LLM}}: {{An Empirical Study}} of {{Active Learning}} on {{Domain-Specific Annotation Tasks}}},
  shorttitle = {Human {{Still Wins}} over {{LLM}}},
  author = {Lu, Yuxuan and Yao, Bingsheng and Zhang, Shao and Wang, Yun and Zhang, Peng and Lu, Tun and Li, Toby Jia-Jun and Wang, Dakuo},
  year = {2023},
  month = nov,
  number = {arXiv:2311.09825},
  eprint = {2311.09825},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.09825},
  urldate = {2024-02-01},
  abstract = {Large Language Models (LLMs) have demonstrated considerable advances, and several claims have been made about their exceeding human performance. However, in real-world tasks, domain knowledge is often required. Low-resource learning methods like Active Learning (AL) have been proposed to tackle the cost of domain expert annotation, raising this question: Can LLMs surpass compact models trained with expert annotations in domain-specific tasks? In this work, we conduct an empirical experiment on four datasets from three different domains comparing SOTA LLMs with small models trained on expert annotations with AL. We found that small models can outperform GPT-3.5 with a few hundreds of labeled data, and they achieve higher or similar performance with GPT-4 despite that they are hundreds time smaller. Based on these findings, we posit that LLM predictions can be used as a warmup method in real-world applications and human experts remain indispensable in tasks involving data annotation driven by domain-specific knowledge.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@article{LuJiYuBLEUDeShuXueZhuGuanTiZiDongYueJuanDeFangFa2019,
  title = {{BLEU}},
  author = {,  and , },
  year = {2019},
  journal = {},
  number = {03},
  pages = {121--124},
  issn = {1674-2877},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&dbname=CJFDLAST2019&filename=GLKW201903051&uniplatform=NZKPT&v=ix6i2z872Y_kL_nlW-jNIIiz6C2k692i0uIrHQioQSLul4GSRgL1jT1MRXeUvO9-},
  urldate = {2022-02-11},
  abstract = {,,BLEU,,88.11\%},
  langid = {chinese},
  annotation = {00000}
}

@misc{luLLaMAReviewerAdvancingCode2023,
  title = {{{LLaMA-Reviewer}}: {{Advancing Code Review Automation}} with {{Large Language Models}} through {{Parameter-Efficient Fine-Tuning}}},
  shorttitle = {{{LLaMA-Reviewer}}},
  author = {Lu, Junyi and Yu, Lei and Li, Xiaojia and Yang, Li and Zuo, Chun},
  year = {2023},
  month = sep,
  number = {arXiv:2308.11148},
  eprint = {2308.11148},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2308.11148},
  urldate = {2024-01-20},
  abstract = {The automation of code review activities, a long-standing pursuit in software engineering, has been primarily addressed by numerous domain-specific pre-trained models. Despite their success, these models frequently demand extensive resources for pre-training from scratch. In contrast, Large Language Models (LLMs) provide an intriguing alternative, given their remarkable capabilities when supplemented with domain-specific knowledge. However, their potential for automating code review tasks remains largely unexplored. In response to this research gap, we present LLaMA-Reviewer, an innovative framework that leverages the capabilities of LLaMA, a popular LLM, in the realm of code review. Mindful of resource constraints, this framework employs parameter-efficient fine-tuning (PEFT) methods, delivering high performance while using less than 1\% of trainable parameters. An extensive evaluation of LLaMA-Reviewer is conducted on two diverse, publicly available datasets. Notably, even with the smallest LLaMA base model consisting of 6.7B parameters and a limited number of tuning epochs, LLaMA-Reviewer equals the performance of existing code-review-focused models. The ablation experiments provide insights into the influence of various fine-tuning process components, including input representation, instruction tuning, and different PEFT methods. To foster continuous progress in this field, the code and all PEFT-weight plugins have been made open-source.},
  archiveprefix = {arXiv}
}

@misc{luLLMAgentsThat2025,
  title = {{{LLM Agents That Act Like Us}}: {{Accurate Human Behavior Simulation}} with {{Real-World Data}}},
  shorttitle = {{{LLM Agents That Act Like Us}}},
  author = {Lu, Yuxuan and Huang, Jing and Han, Yan and Bei, Bennet and Xie, Yaochen and Wang, Dakuo and Wang, Jessie and He, Qi},
  year = {2025},
  month = apr,
  number = {arXiv:2503.20749},
  eprint = {2503.20749},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.20749},
  urldate = {2025-04-30},
  abstract = {Recent research shows that LLMs can simulate ``believable'' human behaviors to power LLM agents via prompt-only methods. In this work, we focus on evaluating and improving LLM's objective ``accuracy'' rather than the subjective ``believability'' in the web action generation task, leveraging a large-scale, real-world dataset collected from online shopping human actions. We present the first comprehensive quantitative evaluation of state-of-the-art LLMs (e.g., DeepSeek-R1, Llama, and Claude) on the task of web action generation. Our results show that fine-tuning LLMs on real-world behavioral data substantially improves their ability to generate actions compared to prompt-only methods. Furthermore, incorporating synthesized reasoning traces into model training leads to additional performance gains, demonstrating the value of explicit rationale in behavior modeling. This work establishes a new benchmark for evaluating LLMs in behavior simulation and offers actionable insights into how real-world action data and reasoning augmentation can enhance the fidelity of LLM agents.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@inproceedings{luoAliCoCo2CommonsenseKnowledge2021,
  title = {{{AliCoCo2}}: {{Commonsense Knowledge Extraction}}, {{Representation}} and {{Application}} in {{E-commerce}}},
  shorttitle = {{{AliCoCo2}}},
  booktitle = {Proceedings of the 27th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Luo, Xusheng and Bo, Le and Wu, Jinhang and Li, Lin and Luo, Zhiy and Yang, Yonghua and Yang, Keping},
  year = {2021},
  month = aug,
  series = {{{KDD}} '21},
  pages = {3385--3393},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3447548.3467203},
  urldate = {2022-07-14},
  abstract = {Commonsense knowledge used by humans while doing online shopping is valuable but difficult to be captured by existing systems running on e-commerce platforms. While construction of common- sense knowledge graphs in e-commerce is non-trivial, representation learning upon such graphs poses unique challenge compared to well-studied open-domain knowledge graphs (e.g., Freebase). By leveraging the commonsense knowledge and representation techniques, various applications in e-commerce can be benefited. Based on AliCoCo, the large-scale e-commerce concept net assisting a series of core businesses in Alibaba, we further enrich it with more commonsense relations and present AliCoCo2, the first commonsense knowledge graph constructed for e-commerce use. We propose a multi-task encoder-decoder framework to provide effective representations for nodes and edges from AliCoCo2. To explore the possibility of improving e-commerce businesses with commonsense knowledge, we apply newly mined commonsense relations and learned embeddings to e-commerce search engine and recommendation system in different ways. Experimental results demonstrate that our proposed representation learning method achieves state-of-the-art performance on the task of knowledge graph completion (KGC), and applications on search and recommendation indicate great potential value of the construction and use of commonsense knowledge graph in e-commerce. Besides, we propose an e-commerce QA task with a new benchmark during the construction of AliCoCo2, for testing machine common sense in e-commerce, which can benefit research community in exploring commonsense reasoning.},
  isbn = {978-1-4503-8332-5}
}

@inproceedings{luoAliCoCoAlibabaEcommerce2020,
  title = {{{AliCoCo}}: {{Alibaba E-commerce Cognitive Concept Net}}},
  shorttitle = {{{AliCoCo}}},
  booktitle = {Proceedings of the 2020 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author = {Luo, Xusheng and Liu, Luxin and Yang, Yonghua and Bo, Le and Cao, Yuanpeng and Wu, Jinghang and Li, Qiang and Yang, Keping and Zhu, Kenny Q.},
  year = {2020},
  month = jun,
  series = {{{SIGMOD}} '20},
  pages = {313--327},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3318464.3386132},
  urldate = {2022-07-12},
  abstract = {One of the ultimate goals of e-commerce platforms is to satisfy various shopping needs for their customers. Much efforts are devoted to creating taxonomies or ontologies in e-commerce towards this goal. However, user needs in e-commerce are still not well defined, and none of the existing ontologies has the enough depth and breadth for universal user needs understanding. The semantic gap in-between prevents shopping experience from being more intelligent. In this paper, we propose to construct a large-scale E-commerce Cognitive Concept Net named "AliCoCo", which is practiced in Alibaba, the largest Chinese e-commerce platform in the world. We formally define user needs in e-commerce, then conceptualize them as nodes in the net. We present details on how AliCoCo is constructed semi-automatically and its successful, ongoing and potential applications in e-commerce.},
  isbn = {978-1-4503-6735-6}
}

@misc{luoRepoAgentLLMPoweredOpenSource2024,
  title = {{{RepoAgent}}: {{An LLM-Powered Open-Source Framework}} for {{Repository-level Code Documentation Generation}}},
  shorttitle = {{{RepoAgent}}},
  author = {Luo, Qinyu and Ye, Yining and Liang, Shihao and Zhang, Zhong and Qin, Yujia and Lu, Yaxi and Wu, Yesai and Cong, Xin and Lin, Yankai and Zhang, Yingli and Che, Xiaoyin and Liu, Zhiyuan and Sun, Maosong},
  year = {2024},
  month = feb,
  number = {arXiv:2402.16667},
  eprint = {2402.16667},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.16667},
  urldate = {2024-07-10},
  abstract = {Generative models have demonstrated considerable potential in software engineering, particularly in tasks such as code generation and debugging. However, their utilization in the domain of code documentation generation remains underexplored. To this end, we introduce RepoAgent, a large language model powered open-source framework aimed at proactively generating, maintaining, and updating code documentation. Through both qualitative and quantitative evaluations, we have validated the effectiveness of our approach, showing that RepoAgent excels in generating high-quality repository-level documentation. The code and results are publicly accessible at https://github.com/OpenBMB/RepoAgent.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,F.2.2,I.2.7}
}

@misc{LuoYuGuaYongTeSiLaShouJiCheZaiMenBianZhiJiaCiXiKuaiChongShouJiZhiJiamodel3YYaPeiJiantmallcomTianMao,
  title = {model3/Y-Tmall.Com},
  url = {https://detail.tmall.com/item.htm?id=897221041435&skuId=5749183484466},
  urldate = {2025-05-20}
}

@misc{lutzWILBURAdaptiveInContext2024,
  title = {{{WILBUR}}: {{Adaptive In-Context Learning}} for {{Robust}} and {{Accurate Web Agents}}},
  shorttitle = {{{WILBUR}}},
  author = {Lutz, Michael and Bohra, Arth and Saroyan, Manvel and Harutyunyan, Artem and Campagna, Giovanni},
  year = {2024},
  month = apr,
  number = {arXiv:2404.05902},
  eprint = {2404.05902},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.05902},
  urldate = {2024-07-12},
  abstract = {In the realm of web agent research, achieving both generalization and accuracy remains a challenging problem. Due to high variance in website structure, existing approaches often fail. Moreover, existing fine-tuning and in-context learning techniques fail to generalize across multiple websites. We introduce Wilbur, an approach that uses a differentiable ranking model and a novel instruction synthesis technique to optimally populate a black-box large language model's prompt with task demonstrations from previous runs. To maximize end-to-end success rates, we also propose an intelligent backtracking mechanism that learns and recovers from its mistakes. Finally, we show that our ranking model can be trained on data from a generative auto-curriculum which samples representative goals from an LLM, runs the agent, and automatically evaluates it, with no manual annotation. Wilbur achieves state-of-the-art results on the WebVoyager benchmark, beating text-only models by 8\% overall, and up to 36\% on certain websites. On the same benchmark, Wilbur is within 5\% of a strong multi-modal model despite only receiving textual inputs, and further analysis reveals a substantial number of failures are due to engineering challenges of operating the web.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@misc{luUILayoutGeneration2023,
  title = {{{UI Layout Generation}} with {{LLMs Guided}} by {{UI Grammar}}},
  author = {Lu, Yuwen and Tong, Ziang and Zhao, Qinyi and Zhang, Chengzhi and Li, Toby Jia-Jun},
  year = {2023},
  month = oct,
  number = {arXiv:2310.15455},
  eprint = {2310.15455},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2310.15455},
  urldate = {2024-04-09},
  abstract = {The recent advances in Large Language Models (LLMs) have stimulated interest among researchers and industry professionals, particularly in their application to tasks concerning mobile user interfaces (UIs). This position paper investigates the use of LLMs for UI layout generation. Central to our exploration is the introduction of UI grammar -- a novel approach we proposed to represent the hierarchical structure inherent in UI screens. The aim of this approach is to guide the generative capacities of LLMs more effectively and improve the explainability and controllability of the process. Initial experiments conducted with GPT-4 showed the promising capability of LLMs to produce high-quality user interfaces via in-context learning. Furthermore, our preliminary comparative study suggested the potential of the grammar-based approach in improving the quality of generative results in specific aspects.},
  archiveprefix = {arXiv},
  langid = {american}
}

@misc{luUXAgentLLMAgentBased2025,
  title = {{{UXAgent}}: {{An LLM Agent-Based Usability Testing Framework}} for {{Web Design}}},
  shorttitle = {{{UXAgent}}},
  author = {Lu, Yuxuan and Yao, Bingsheng and Gu, Hansu and Huang, Jing and Wang, Jessie and Li, Laurence and Gesi, Jiri and He, Qi and Li, Toby Jia-Jun and Wang, Dakuo},
  year = {2025},
  month = feb,
  eprint = {2502.12561},
  primaryclass = {cs},
  doi = {10.1145/3706599.3719729},
  urldate = {2025-03-16},
  abstract = {Usability testing is a fundamental yet challenging (e.g., inflexible to iterate the study design flaws and hard to recruit study participants) research method for user experience (UX) researchers to evaluate a web design. Recent advances in Large Language Model-simulated Agent (LLM-Agent) research inspired us to design UXAgent to support UX researchers in evaluating and reiterating their usability testing study design before they conduct the real human subject study. Our system features an LLM-Agent module and a universal browser connector module so that UX researchers can automatically generate thousands of simulated users to test the target website. The results are shown in qualitative (e.g., interviewing how an agent thinks ), quantitative (e.g., \# of actions), and video recording formats for UX researchers to analyze. Through a heuristic user evaluation with five UX researchers, participants praised the innovation of our system but also expressed concerns about the future of LLM Agent-assisted UX study.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Human-Computer Interaction}
}

@inproceedings{luUXAgentLLMAgentBased2025a,
  title = {{{UXAgent}}: {{An LLM Agent-Based Usability Testing Framework}} for {{Web Design}}},
  shorttitle = {{{UXAgent}}},
  booktitle = {Proceedings of the {{Extended Abstracts}} of the {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Lu, Yuxuan and Yao, Bingsheng and Gu, Hansu and Huang, Jing and Wang, Zheshen Jessie and Li, Yang and Gesi, Jiri and He, Qi and Li, Toby Jia-Jun and Wang, Dakuo},
  year = {2025},
  month = apr,
  series = {{{CHI EA}} '25},
  pages = {1--12},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3706599.3719729},
  urldate = {2025-04-30},
  abstract = {Usability testing is a fundamental yet challenging research method for user experience (UX) researchers to evaluate a web design. Recent advances in Large Language Model-simulated Agent (LLM Agent) research inspired us to design UXAgent to support UX researchers in evaluating and reiterating their usability testing study design before they conduct the real human-subject study. Our system features an LLM Agent module and a universal browser connector module so that UX researchers can automatically generate thousands of simulated users to test the target website. The system can generate UX study results in qualitative (e.g., interviewing how an agent thinks), quantitative (e.g., \# of actions), and video recording formats for UX researchers to analyze. Through a heuristic user evaluation with five UX researchers, participants praised the innovation of our system but also expressed concerns about the future of UX study with LLM Agents1.},
  isbn = {979-8-4007-1395-8}
}

@misc{luUXAgentSystemSimulating2025,
  title = {{{UXAgent}}: {{A System}} for {{Simulating Usability Testing}} of {{Web Design}} with {{LLM Agents}}},
  shorttitle = {{{UXAgent}}},
  author = {Lu, Yuxuan and Yao, Bingsheng and Gu, Hansu and Huang, Jing and Wang, Jessie and Li, Yang and Gesi, Jiri and He, Qi and Li, Toby Jia-Jun and Wang, Dakuo},
  year = {2025},
  month = apr,
  number = {arXiv:2504.09407},
  eprint = {2504.09407},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.09407},
  urldate = {2025-04-30},
  abstract = {Usability testing is a fundamental research method that user experience (UX) researchers use to evaluate and iterate a web design, but{\textbackslash}textbf\{ how to evaluate and iterate the usability testing study design \} itself? Recent advances in Large Language Model-simulated Agent ({\textbackslash}textbf\{LLM Agent\}) research inspired us to design {\textbackslash}textbf\{UXAgent\} to support UX researchers in evaluating and reiterating their usability testing study design before they conduct the real human-subject study. Our system features a Persona Generator module, an LLM Agent module, and a Universal Browser Connector module to automatically generate thousands of simulated users to interactively test the target website. The system also provides an Agent Interview Interface and a Video Replay Interface so that the UX researchers can easily review and analyze the generated qualitative and quantitative log data. Through a heuristic evaluation, five UX researcher participants praised the innovation of our system but also expressed concerns about the future of LLM Agent usage in UX studies.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Human-Computer Interaction}
}

@misc{luWebLINXRealWorldWebsite2024,
  title = {{{WebLINX}}: {{Real-World Website Navigation}} with {{Multi-Turn Dialogue}}},
  shorttitle = {{{WebLINX}}},
  author = {L{\`u}, Xing Han and Kasner, Zden{\v e}k and Reddy, Siva},
  year = {2024},
  month = feb,
  number = {arXiv:2402.05930},
  eprint = {2402.05930},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.05930},
  urldate = {2024-08-08},
  abstract = {We propose the problem of conversational web navigation, where a digital agent controls a web browser and follows user instructions to solve real-world tasks in a multi-turn dialogue fashion. To support this problem, we introduce WEBLINX - a large-scale benchmark of 100K interactions across 2300 expert demonstrations of conversational web navigation. Our benchmark covers a broad range of patterns on over 150 real-world websites and can be used to train and evaluate agents in diverse scenarios. Due to the magnitude of information present, Large Language Models (LLMs) cannot process entire web pages in real-time. To solve this bottleneck, we design a retrieval-inspired model that efficiently prunes HTML pages by ranking relevant elements. We use the selected elements, along with screenshots and action history, to assess a variety of models for their ability to replicate human behavior when navigating the web. Our experiments span from small text-only to proprietary multimodal LLMs. We find that smaller finetuned decoders surpass the best zero-shot LLMs (including GPT-4V), but also larger finetuned multimodal models which were explicitly pretrained on screenshots. However, all finetuned models struggle to generalize to unseen websites. Our findings highlight the need for large multimodal models that can generalize to novel settings. Our code, data and models are available for research: https://mcgill-nlp.github.io/weblinx},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@inproceedings{macalAgentbasedModelingSimulation2009,
  title = {Agent-Based Modeling and Simulation},
  booktitle = {Proceedings of the 2009 {{Winter Simulation Conference}} ({{WSC}})},
  author = {Macal, Charles M. and North, Michael J.},
  year = {2009},
  month = dec,
  pages = {86--98},
  issn = {1558-4305},
  doi = {10.1109/WSC.2009.5429318},
  urldate = {2024-10-10},
  abstract = {Agent-based modeling and simulation (ABMS) is a new approach to modeling systems comprised of autonomous, interacting agents. Computational advances have made possible a growing number of agent-based models across a variety of application domains. Applications range from modeling agent behavior in the stock market, supply chains, and consumer markets, to predicting the spread of epidemics, mitigating the threat of bio-warfare, and understanding the factors that may be responsible for the fall of ancient civilizations. Such progress suggests the potential of ABMS to have far-reaching effects on the way that businesses use computers to support decision-making and researchers use agent-based models as electronic laboratories. Some contend that ABMS {\textquestiondown}is a third way of doing science{\textquestiondown} and could augment traditional deductive and inductive reasoning as discovery methods. This brief tutorial introduces agent-based modeling by describing the foundations of ABMS, discussing some illustrative applications, and addressing toolkits and methods for developing agent-based models.},
  keywords = {Adaptive systems,Application software,Computational modeling,Environmental economics,Laboratories,Power generation economics,Power system economics,Power system modeling,Predictive models,Stock markets}
}

@inproceedings{macalTutorialAgentbasedModeling2005,
  title = {Tutorial on Agent-Based Modeling and Simulation},
  booktitle = {Proceedings of the {{Winter Simulation Conference}}, 2005.},
  author = {Macal, C.M. and North, M.J.},
  year = {2005},
  month = dec,
  pages = {14 pp.-},
  issn = {1558-4305},
  doi = {10.1109/WSC.2005.1574234},
  urldate = {2024-10-10},
  abstract = {Agent-based modeling and simulation (ABMS) is a new approach to modeling systems comprised of autonomous, interacting agents. ABMS promises to have far reaching effects on the way that businesses use computers to support decision making and researchers use electronic laboratories to support their research. Some have gone so far as to contend that ABMS is a third way of doing science besides deductive and inductive reasoning. Computational advances have made possible a growing number of agent-based applications in a variety of fields. Applications range from modeling agent behavior in the stock market and supply chains, to predicting the spread of epidemics and the threat of biowarfare, from modeling consumer behavior to understanding the fall of ancient civilizations, to name a few. This tutorial describes the theoretical and practical foundations of ABMS, identifies toolkits and methods for developing ABMS models, and provides some thoughts on the relationship between ABMS and traditional modeling techniques.},
  keywords = {Application software,Computational modeling,Consumer behavior,Consumer electronics,Decision making,Laboratories,Predictive models,Stock markets,Supply chains,Tutorial}
}

@inproceedings{maccartneyModelingSemanticContainment2008,
  title = {Modeling {{Semantic Containment}} and {{Exclusion}} in {{Natural Language Inference}}},
  booktitle = {Proceedings of the 22nd {{International Conference}} on {{Computational Linguistics}} ({{Coling}} 2008)},
  author = {MacCartney, Bill and Manning, Christopher D.},
  editor = {Scott, Donia and Uszkoreit, Hans},
  year = {2008},
  month = aug,
  pages = {521--528},
  publisher = {Coling 2008 Organizing Committee},
  address = {Manchester, UK},
  url = {https://aclanthology.org/C08-1066},
  urldate = {2023-12-15}
}

@misc{maLASERLLMAgent2024,
  title = {{{LASER}}: {{LLM Agent}} with {{State-Space Exploration}} for {{Web Navigation}}},
  shorttitle = {{{LASER}}},
  author = {Ma, Kaixin and Zhang, Hongming and Wang, Hongwei and Pan, Xiaoman and Yu, Wenhao and Yu, Dong},
  year = {2024},
  month = feb,
  number = {arXiv:2309.08172},
  eprint = {2309.08172},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.08172},
  urldate = {2024-07-05},
  abstract = {Large language models (LLMs) have been successfully adapted for interactive decision-making tasks like web navigation. While achieving decent performance, previous methods implicitly assume a forward-only execution mode for the model, where they only provide oracle trajectories as in-context examples to guide the model on how to reason in the environment. Consequently, the model could not handle more challenging scenarios not covered in the in-context examples, e.g., mistakes, leading to sub-optimal performance. To address this issue, we propose to model the interactive task as state space exploration, where the LLM agent transitions among a pre-defined set of states by performing actions to complete the task. This formulation enables flexible backtracking, allowing the model to recover from errors easily. We evaluate our proposed LLM Agent with State-Space ExploRation (LASER) on both the WebShop task and amazon.com. Experimental results show that LASER significantly outperforms previous methods and closes the gap with human performance on the web navigation task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@inproceedings{mallariLookCriminalExamining2020,
  title = {Do {{I Look Like}} a {{Criminal}}? {{Examining}} How {{Race Presentation Impacts Human Judgement}} of {{Recidivism}}},
  shorttitle = {Do {{I Look Like}} a {{Criminal}}?},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Mallari, Keri and Inkpen, Kori and Johns, Paul and Tan, Sarah and Ramesh, Divya and Kamar, Ece},
  year = {2020},
  month = apr,
  series = {{{CHI}} '20},
  pages = {1--13},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3313831.3376257},
  urldate = {2024-02-05},
  abstract = {Understanding how racial information impacts human decision making in online systems is critical in today's world. Prior work revealed that race information of criminal defendants, when presented as a text field, had no significant impact on users' judgements of recidivism. We replicated and extended this work to explore how and when race information influences users' judgements, with respect to the saliency of presentation. Our results showed that adding photos to the race labels had a significant impact on recidivism predictions for users who identified as female, but not for those who identified as male. The race of the defendant also impacted these results, with black defendants being less likely to be predicted to recidivate compared to white defendants. These results have strong implications for how system-designers choose to display race information, and cautions researchers to be aware of gender and race effects when using Amazon Mechanical Turk workers.},
  isbn = {978-1-4503-6708-0}
}

@inproceedings{marinelliSyntheticInterviewsArt1998,
  title = {Synthetic Interviews: The Art of Creating a ``Dyad'' between Humans and Machine-Based Characters},
  shorttitle = {Synthetic Interviews},
  booktitle = {Proceedings of the Sixth {{ACM}} International Conference on {{Multimedia}}: {{Technologies}} for Interactive Movies},
  author = {Marinelli, Donald and Stevens, Scott},
  year = {1998},
  month = sep,
  pages = {11--16},
  publisher = {ACM},
  address = {Bristol United Kingdom},
  doi = {10.1145/306774.306780},
  urldate = {2024-09-05},
  isbn = {978-1-58113-162-8},
  langid = {english}
}

@article{markusLeveragingResearcherDomain2023,
  title = {Leveraging {{Researcher Domain Expertise}} to {{Annotate Concepts Within Imbalanced Data}}},
  author = {Markus, Dror K. and {Mor-Lan}, Guy and Sheafer, Tamir and Shenhav, Shaul R.},
  year = {2023},
  month = jul,
  journal = {Communication Methods and Measures},
  volume = {17},
  number = {3},
  pages = {250--271},
  publisher = {Routledge},
  issn = {1931-2458},
  doi = {10.1080/19312458.2023.2182278},
  urldate = {2024-03-29},
  abstract = {As more computational communication researchers turn to supervised machine learning methods for text classification, we note the challenge in implementing such techniques within an imbalanced dataset. Such issues are critical in our domain, where, in many cases, researchers attempt to identify and study theoretically interesting categories that can be rare in a target corpus. Specifically, imbalanced distributions, with a skewed distribution of texts among the categories, can lead to a lengthy and expensive annotation stage, forcing practitioners to sample and label large numbers of texts to train a classification model. In this paper, we provide an overview of the issue, and describe existing strategies for mitigating such challenges. Noting the pitfalls of previous solutions, we then provide a semi-supervised method -- Expert Initiated Latent Space Sampling -- that complements researcher domain expertise with a systematic, unsupervised exploration of the latent semantic space to overcome such limitations. Utilizing simulations to systematically evaluate our method and compare it to existing approaches, we show that our procedure offers significant advantages in terms of efficiency and accuracy in many classification tasks.}
}

@article{masicMedicalDecisionMaking2022,
  title = {Medical {{Decision Making}} - an {{Overview}}},
  author = {Masic, Izet},
  year = {2022},
  month = sep,
  journal = {Acta Informatica Medica},
  volume = {30},
  number = {3},
  pages = {230--235},
  issn = {0353-8109},
  doi = {10.5455/aim.2022.30.230-235},
  urldate = {2024-01-30},
  abstract = {Background: Medical professionals (doctors and other medical staff) in the field of healthcare everyday must make calculated decisions which have important consequences, impacting patients on the individual level, local (community), national or global level. Healthcare professionals must at times make these choices with limited information, resources, and knowledge, and yet is is expected that these decisions are highly calculated and accurate. It is important to familiarise oneself with the exact definitions regarding medical decision making. Objective: The aim of this study was to describe application of the most important rules to help decision makers to be good or excellent decision makers in medical practice at every level of health care system. Methods: The author used descriptive method of explanation teoretical and practical issues regarding application of od decision making processes in the praxis, based on searchied scientific literature about this topic deposited in online databases. Results and Discussion: The author of this paper discussed about important topics: a) the importance of medical decision in emergency situations; b) the varies of decision making with solving problems by medical professionals; c) the limitations when it comes to medical decison making; and d) what doctors need to follow regarding decision making in the praxis. Two factors that have influenced to the decision process: a) degree of uncertainty about future events; b) usefulness of outcomes in any particular case. The clinical decision problem analysis process demands: a) explicit formalization of a decision making problem or the description of the medical problem decision with a registration of all possible actions which have to be undertaken and registration of all the possible so determined outcomes. b) construction of the decision tree which presents all described actions and outcomes with predictions of the probabilities and the choice of the most optimal action based on the probability outcome and its use. Doing this allows us to delve deeper into more intricate options present within medical decision making. Simple put, a decision is a choice between two options. The person or entity conducting that decision is the decision maker. The exact definition is ``Under the decision should imply some specific action which is selected from several variables or which satisfies the expectation that is previously set''.Many different factors and individuals may be involved in medical decision making, with varying consequences, according to different players and settings. Conclusion: A vital component of medical decision making is evaluation. Decision makers must concisely evaluate situations, in order to make better choices. For example, when examining a health care system, their decisions should consider the following questions, such as, what is the health status of the given population? What economic resources are at the disposal of our patients, and government? How effective is the current healthcare model that is already in place? Does the existing social system pay enough attention to the healthcare protection? Does the organisation structure of the healthcare system satisfy? Are the existing practice and the healthcare technologies secure, effective, and suitable? Are the planning, programming, determination and the choice of priority the adequate to the needs of people? How are the monitoring and evaluation of healthcare system quality organised? These are a few examples of evaluation in medical decision making.},
  pmcid = {PMC9560052},
  pmid = {36311160}
}

@inproceedings{maStateoftheartChineseWord2018,
  title = {State-of-the-Art {{Chinese Word Segmentation}} with {{Bi-LSTMs}}},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Ma, Ji and Ganchev, Kuzman and Weiss, David},
  year = {2018},
  month = oct,
  pages = {4902--4908},
  publisher = {Association for Computational Linguistics},
  address = {Brussels, Belgium},
  doi = {10.18653/v1/D18-1529},
  urldate = {2022-06-09},
  abstract = {A wide variety of neural-network architectures have been proposed for the task of Chinese word segmentation. Surprisingly, we find that a bidirectional LSTM model, when combined with standard deep learning techniques and best practices, can achieve better accuracy on many of the popular datasets as compared to models based on more complex neuralnetwork architectures. Furthermore, our error analysis shows that out-of-vocabulary words remain challenging for neural-network models, and many of the remaining errors are unlikely to be fixed through architecture changes. Instead, more effort should be made on exploring resources for further improvement.}
}

@inproceedings{maxwellAgentsSimulatedUsers2016,
  title = {Agents, {{Simulated Users}} and {{Humans}}: {{An Analysis}} of {{Performance}} and {{Behaviour}}},
  shorttitle = {Agents, {{Simulated Users}} and {{Humans}}},
  booktitle = {Proceedings of the 25th {{ACM International}} on {{Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Maxwell, David and Azzopardi, Leif},
  year = {2016},
  month = oct,
  series = {{{CIKM}} '16},
  pages = {731--740},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2983323.2983805},
  urldate = {2024-08-27},
  abstract = {Most of the current models that are used to simulate users in Interactive Information Retrieval (IIR) lack realism and agency. Such models generally make decisions in a stochastic manner, without recourse to the actual information encountered or the underlying information need. In this paper, we develop a more sophisticated model of the user that includes their cognitive state within the simulation. The cognitive state maintains data about what the simulated user knows, has done and has seen, along with representations of what it considers attractive and relevant. Decisions to inspect or judge are then made based upon the simulated user's current state, rather than stochastically. In the context of ad-hoc topic retrieval, we evaluate the quality of the simulated users and agents by comparing their behaviour and performance against 48 human subjects under the same conditions, topics, time constraints, costs and search engine. Our findings show that while naive configurations of simulated users and agents substantially outperform our human subjects, their search behaviour is notably different from actual searchers. However, more sophisticated search agents can be tuned to act more like actual searchers providing greater realism. This innovation advances the state of the art in simulation, from simulated users towards autonomous agents. It provides a much needed step forward enabling the creation of more realistic simulations, while also motivating the development of more advanced cognitive agents and tools to help support and augment human searchers. Future work will focus not only on the pragmatics of tuning and training such agents for topic retrieval, but will also look at developing agents for other tasks and contexts such as collaborative search and slow search.},
  isbn = {978-1-4503-4073-1}
}

@inproceedings{mcdonnellEasierHarderDepending2023,
  title = {``{{Easier}} or {{Harder}}, {{Depending}} on {{Who}} the {{Hearing Person Is}}'': {{Codesigning Videoconferencing Tools}} for {{Small Groups}} with {{Mixed Hearing Status}}},
  shorttitle = {``{{Easier}} or {{Harder}}, {{Depending}} on {{Who}} the {{Hearing Person Is}}''},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {McDonnell, Emma J and Moon, Soo Hyun and Jiang, Lucy and Goodman, Steven M. and Kushalnagar, Raja and Froehlich, Jon E. and Findlater, Leah},
  year = {2023},
  month = apr,
  series = {{{CHI}} '23},
  pages = {1--15},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3544548.3580809},
  urldate = {2024-02-08},
  abstract = {With improvements in automated speech recognition and increased use of videoconferencing, real-time captioning has changed significantly. This shift toward broadly available but less accurate captioning invites exploration of the role hearing conversation partners play in shaping the accessibility of a conversation to d/Deaf and hard of hearing (DHH) captioning users. While recent work has explored DHH individuals' videoconferencing experiences with captioning, we focus on established groups' current practices and priorities for future tools to support more accessible online conversations. Our study consists of three codesign sessions, conducted with four groups (17 participants total, 10 DHH, 7 hearing). We found that established groups crafted social accessibility norms that met their relational contexts. We also identify promising directions for future captioning design, including the need to standardize speaker identification and customization, opportunities to provide behavioral feedback during a conversation, and ways that videoconferencing platforms could enable groups to set and share norms.},
  isbn = {978-1-4503-9421-5}
}

@misc{MeiQiaLunTanZaiMeiPiaoBoDeZhongGuoRenDeJiaYuan,
  title = {{ - }},
  journal = {},
  url = {https://www.uscardforum.com/},
  urldate = {2024-10-06},
  abstract = {///////},
  langid = {chinese}
}

@misc{menShortGPTLayersLarge2024,
  title = {{{ShortGPT}}: {{Layers}} in {{Large Language Models}} Are {{More Redundant Than You Expect}}},
  shorttitle = {{{ShortGPT}}},
  author = {Men, Xin and Xu, Mingyu and Zhang, Qingyu and Wang, Bingning and Lin, Hongyu and Lu, Yaojie and Han, Xianpei and Chen, Weipeng},
  year = {2024},
  month = mar,
  number = {arXiv:2403.03853},
  eprint = {2403.03853},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2403.03853},
  urldate = {2024-03-07},
  abstract = {As Large Language Models (LLMs) continue to advance in performance, their size has escalated significantly, with current LLMs containing billions or even trillions of parameters. However, in this study, we discovered that many layers of LLMs exhibit high similarity, and some layers play a negligible role in network functionality. Based on this observation, we define a metric called Block Influence (BI) to gauge the significance of each layer in LLMs. We then propose a straightforward pruning approach: layer removal, in which we directly delete the redundant layers in LLMs based on their BI scores. Experiments demonstrate that our method, which we call ShortGPT, significantly outperforms previous state-of-the-art (SOTA) methods in model pruning. Moreover, ShortGPT is orthogonal to quantization-like methods, enabling further reduction in parameters and computation. The ability to achieve better results through simple layer removal, as opposed to more complex pruning techniques, suggests a high degree of redundancy in the model architecture.},
  archiveprefix = {arXiv}
}

@inproceedings{micikeviciusMixedPrecisionTraining2018,
  title = {Mixed {{Precision Training}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and Wu, Hao},
  year = {2018},
  month = feb,
  url = {https://openreview.net/forum?id=r1gs9JgRZ},
  urldate = {2022-01-17},
  abstract = {Increasing the size of a neural network typically improves accuracy but also increases the memory and compute requirements for training the model. We introduce methodology for training deep neural...},
  langid = {english},
  annotation = {00710}
}

@article{minarSwarmSimulationSystem,
  title = {The {{Swarm Simulation System}}: {{A Toolkit}} for {{Building Multi-agent Simulations}}},
  author = {Minar, Nelson and Burkhart, Roger and Langton, Chris and Askenazi, Manor},
  langid = {english},
  keywords = {No DOI found}
}

@article{minMetaICLLearningLearn2021,
  title = {{{MetaICL}}: {{Learning}} to {{Learn In Context}}},
  shorttitle = {{{MetaICL}}},
  author = {Min, Sewon and Lewis, Mike and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
  year = {2021},
  journal = {arXiv preprint arXiv:2110.15943},
  eprint = {2110.15943},
  archiveprefix = {arXiv}
}

@misc{mishraTemplateControllableKeywordstotext2020,
  title = {Template {{Controllable}} Keywords-to-Text {{Generation}}},
  author = {Mishra, Abhijit and Chowdhury, Md Faisal Mahbub and Manohar, Sagar and Gutfreund, Dan and Sankaranarayanan, Karthik},
  year = {2020},
  month = nov,
  number = {arXiv:2011.03722},
  eprint = {2011.03722},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2011.03722},
  urldate = {2022-09-07},
  abstract = {This paper proposes a novel neural model for the understudied task of generating text from keywords. The model takes as input a set of un-ordered keywords, and part-of-speech (POS) based template instructions. This makes it ideal for surface realization in any NLG setup. The framework is based on the encode-attend-decode paradigm, where keywords and templates are encoded first, and the decoder judiciously attends over the contexts derived from the encoded keywords and templates to generate the sentences. Training exploits weak supervision, as the model trains on a large amount of labeled data with keywords and POS based templates prepared through completely automatic means. Qualitative and quantitative performance analyses on publicly available test-data in various domains reveal our system's superiority over baselines, built using state-of-the-art neural machine translation and controllable transfer techniques. Our approach is indifferent to the order of input keywords.},
  archiveprefix = {arXiv}
}

@inproceedings{mitsopoulosPsychologicallyValidGenerativeAgents2024,
  title = {Psychologically-{{Valid Generative Agents}}: {{A Novel Approach}} to {{Agent-Based Modeling}} in {{Social Sciences}}},
  shorttitle = {Psychologically-{{Valid Generative Agents}}},
  booktitle = {Proceedings of the {{AAAI Symposium Series}}},
  author = {Mitsopoulos, Konstantinos and Bose, Ritwik and Mather, Brodie and Bhatia, Archna and Gluck, Kevin and Dorr, Bonnie and Lebiere, Christian and Pirolli, Peter},
  year = {2024},
  month = jan,
  volume = {2},
  pages = {340--348},
  doi = {10.1609/aaaiss.v2i1.27698},
  urldate = {2024-07-06},
  abstract = {Incorporating dynamic realistic human behaviors in population-scale computational models has been challenging.  While some efforts have leveraged behavioral theories from social science, validated theories specifically applicable to  Agent-based modeling remain limited. Existing approaches lack a comprehensive framework to model the situated, adaptive nature of human cognition and choice. To address these challenges, this paper proposes a novel framework, Psychologically-Valid Generative Agents. These agents consist of a Cognitive Architecture that provides data-driven and cognitively-constrained decision-making functionality, and a Large Language Model that generates human-like linguistic data. In addition, our framework benefits from Stance Detection, a Natural Language Processing technique, that allows highly personalize initialization of the agents, based on real-world data, within Agent-based modeling simulations. This combination provides a flexible yet structured approach to endogenously represent how people perceive, deliberate, and respond to social or other types of complex decision-making dynamics. Previous work has demonstrated promising results by using a subset of the components of our proposed architecture. Our approach has the potential to exhibit highly-realistic human behavior and can be used across a variety of domains (e.g., public health, group dynamics, social and psychological sciences, and financial markets).}
}

@article{ModelingRelationalData,
  title = {Modeling {{Relational Data}} with {{Graph Convolutional Networks}} {\textbar} {{SpringerLink}}},
  doi = {10.1007/978-3-319-93417-4\_38},
  urldate = {2021-12-07}
}

@misc{moonAnyMALEfficientScalable2023,
  title = {{{AnyMAL}}: {{An Efficient}} and {{Scalable Any-Modality Augmented Language Model}}},
  shorttitle = {{{AnyMAL}}},
  author = {Moon, Seungwhan and Madotto, Andrea and Lin, Zhaojiang and Nagarajan, Tushar and Smith, Matt and Jain, Shashank and Yeh, Chun-Fu and Murugesan, Prakash and Heidari, Peyman and Liu, Yue and Srinet, Kavya and Damavandi, Babak and Kumar, Anuj},
  year = {2023},
  month = sep,
  number = {arXiv:2309.16058},
  eprint = {2309.16058},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2309.16058},
  urldate = {2024-01-08},
  abstract = {We present Any-Modality Augmented Language Model (AnyMAL), a unified model that reasons over diverse input modality signals (i.e. text, image, video, audio, IMU motion sensor), and generates textual responses. AnyMAL inherits the powerful text-based reasoning abilities of the state-of-the-art LLMs including LLaMA-2 (70B), and converts modality-specific signals to the joint textual space through a pre-trained aligner module. To further strengthen the multimodal LLM's capabilities, we fine-tune the model with a multimodal instruction set manually collected to cover diverse topics and tasks beyond simple QAs. We conduct comprehensive empirical analysis comprising both human and automatic evaluations, and demonstrate state-of-the-art performance on various multimodal tasks.},
  archiveprefix = {arXiv}
}

@article{mouNarrativeQuestionAnswering2021,
  title = {Narrative {{Question Answering}} with {{Cutting-Edge Open-Domain QA Techniques}}: {{A Comprehensive Study}}},
  shorttitle = {Narrative {{Question Answering}} with {{Cutting-Edge Open-Domain QA Techniques}}},
  author = {Mou, Xiangyang and Yang, Chenghao and Yu, Mo and Yao, Bingsheng and Guo, Xiaoxiao and Potdar, Saloni and Su, Hui},
  editor = {Roark, Brian and Nenkova, Ani},
  year = {2021},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {9},
  pages = {1032--1046},
  publisher = {MIT Press},
  address = {Cambridge, MA},
  doi = {10.1162/tacl\_a\_00411},
  urldate = {2023-12-15},
  abstract = {Recent advancements in open-domain question answering (ODQA), that is, finding answers from large open-domain corpus like Wikipedia, have led to human-level performance on many datasets. However, progress in QA over book stories (Book QA) lags despite its similar task formulation to ODQA. This work provides a comprehensive and quantitative analysis about the difficulty of Book QA: (1) We benchmark the research on the NarrativeQA dataset with extensive experiments with cutting-edge ODQA techniques. This quantifies the challenges Book QA poses, as well as advances the published state-of-the-art with a {$\sim$}7\% absolute improvement on ROUGE-L. (2) We further analyze the detailed challenges in Book QA through human studies.1 Our findings indicate that the event-centric questions dominate this task, which exemplifies the inability of existing QA models to handle event-oriented scenarios.}
}

@article{moussiadesPDetectClusteringApproach2005,
  title = {{{PDetect}}: {{A}} Clustering Approach for Detecting Plagiarism in Source Code Datasets},
  shorttitle = {{{PDetect}}},
  author = {Moussiades, Lefteris and Vakali, Athena},
  year = {2005},
  journal = {The computer journal},
  volume = {48},
  number = {6},
  pages = {651--661},
  publisher = {Oxford University Press},
  doi = {10.1093/comjnl/bxh119}
}

@misc{muennighoffOctoPackInstructionTuning2023,
  title = {{{OctoPack}}: {{Instruction Tuning Code Large Language Models}}},
  shorttitle = {{{OctoPack}}},
  author = {Muennighoff, Niklas and Liu, Qian and Zebaze, Armel and Zheng, Qinkai and Hui, Binyuan and Zhuo, Terry Yue and Singh, Swayam and Tang, Xiangru and {von Werra}, Leandro and Longpre, Shayne},
  year = {2023},
  month = aug,
  number = {arXiv:2308.07124},
  eprint = {2308.07124},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.07124},
  urldate = {2024-01-23},
  abstract = {Finetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile CommitPack: 4 terabytes of Git commits across 350 programming languages. We benchmark CommitPack against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2\% pass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among all permissive models, demonstrating CommitPack's benefits in generalizing to a wider set of languages and natural coding tasks. Code, models and data are freely available at https://github.com/bigcode-project/octopack.},
  archiveprefix = {arXiv}
}

@inproceedings{murnaneDesigningAmbientNarrativeBased2020,
  title = {Designing {{Ambient Narrative-Based Interfaces}} to {{Reflect}} and {{Motivate Physical Activity}}},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Murnane, Elizabeth L. and Jiang, Xin and Kong, Anna and Park, Michelle and Shi, Weili and Soohoo, Connor and Vink, Luke and Xia, Iris and Yu, Xin and {Yang-Sammataro}, John and Young, Grace and Zhi, Jenny and Moya, Paula and Landay, James A.},
  year = {2020},
  month = apr,
  series = {{{CHI}} '20},
  pages = {1--14},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3313831.3376478},
  urldate = {2023-08-31},
  abstract = {Numerous technologies now exist for promoting more active lifestyles. However, while quantitative data representations (e.g., charts, graphs, and statistical reports) typify most health tools, growing evidence suggests such feedback can not only fail to motivate behavior but may also harm self-integrity and fuel negative mindsets about exercise. Our research seeks to devise alternative, more qualitative schemes for encoding personal information. In particular, this paper explores the design of data-driven narratives, given the intuitive and persuasive power of stories. We present WhoIsZuki, a smartphone application that visualizes physical activities and goals as components of a multi-chapter quest, where the main character's progress is tied to the user's. We report on our design process involving online surveys, in-lab studies, and in-the-wild deployments, aimed at refining the interface and the narrative and gaining a deep understanding of people's experiences with this type of feedback. From these insights, we contribute recommendations to guide future development of narrative-based applications for motivating healthy behavior.},
  isbn = {978-1-4503-6708-0}
}

@misc{nakanoWebGPTBrowserassistedQuestionanswering2022,
  title = {{{WebGPT}}: {{Browser-assisted}} Question-Answering with Human Feedback},
  shorttitle = {{{WebGPT}}},
  author = {Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and Jiang, Xu and Cobbe, Karl and Eloundou, Tyna and Krueger, Gretchen and Button, Kevin and Knight, Matthew and Chess, Benjamin and Schulman, John},
  year = {2022},
  month = jun,
  number = {arXiv:2112.09332},
  eprint = {2112.09332},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.09332},
  urldate = {2022-08-15},
  abstract = {We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56\% of the time to those of our human demonstrators, and 69\% of the time to the highest-voted answer from Reddit.},
  archiveprefix = {arXiv}
}

@inproceedings{nallapatiAbstractiveTextSummarization2016,
  title = {Abstractive {{Text Summarization}} Using {{Sequence-to-sequence RNNs}} and {{Beyond}}},
  booktitle = {Proceedings of {{The}} 20th {{SIGNLL Conference}} on {{Computational Natural Language Learning}}},
  author = {Nallapati, Ramesh and Zhou, Bowen and {dos Santos}, Cicero and G{\.u}l{\c c}ehre, {\c C}a{\u g}lar and Xiang, Bing},
  year = {2016},
  month = aug,
  pages = {280--290},
  publisher = {Association for Computational Linguistics},
  address = {Berlin, Germany},
  doi = {10.18653/v1/K16-1028},
  urldate = {2021-12-25},
  langid = {english},
  annotation = {01587}
}

@misc{narasimhanTextStyleTransfer2022,
  title = {On {{Text Style Transfer}} via {{Style Masked Language Models}}},
  author = {Narasimhan, Sharan and Shekar, Pooja and Dey, Suvodip and Desarkar, Maunendra Sankar},
  year = {2022},
  month = oct,
  number = {arXiv:2210.06394},
  eprint = {2210.06394},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2210.06394},
  urldate = {2023-01-30},
  abstract = {Text Style Transfer (TST) is performable through approaches such as latent space disentanglement, cycle-consistency losses, prototype editing etc. The prototype editing approach, which is known to be quite successful in TST, involves two key phases a) Masking of source style-associated tokens and b) Reconstruction of this source-style masked sentence conditioned with the target style. We follow a similar transduction method, in which we transpose the more difficult direct source to target TST task to a simpler Style-Masked Language Model (SMLM) Task, wherein, similar to BERT {\textbackslash}cite\{bert\}, the goal of our model is now to reconstruct the source sentence from its style-masked version. We arrive at the SMLM mechanism naturally by formulating prototype editing/ transduction methods in a probabilistic framework, where TST resolves into estimating a hypothetical parallel dataset from a partially observed parallel dataset, wherein each domain is assumed to have a common latent style-masked prior. To generate this style-masked prior, we use "Explainable Attention" as our choice of attribution for a more precise style-masking step and also introduce a cost-effective and accurate "Attribution-Surplus" method of determining the position of masks from any arbitrary attribution model in O(1) time. We empirically show that this non-generational approach well suites the "content preserving" criteria for a task like TST, even for a complex style like Discourse Manipulation. Our model, the Style MLM, outperforms strong TST baselines and is on par with state-of-the-art TST models, which use complex architectures and orders of more parameters.},
  archiveprefix = {arXiv}
}

@article{narayananEfficientLargeScaleLanguage2021,
  title = {Efficient {{Large-Scale Language Model Training}} on {{GPU Clusters Using Megatron-LM}}},
  author = {Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay Anand and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and Phanishayee, Amar and Zaharia, Matei},
  year = {2021},
  month = aug,
  journal = {arXiv:2104.04473 [cs]},
  eprint = {2104.04473},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2104.04473},
  urldate = {2022-01-22},
  abstract = {Large language models have led to state-of-the-art accuracies across a range of tasks. However, training these models efficiently is challenging for two reasons: a) GPU memory capacity is limited, making it impossible to fit large models on even a multi-GPU server, and b) the number of compute operations required to train these models can result in unrealistically long training times. Consequently, new methods of model parallelism such as tensor and pipeline parallelism have been proposed. Unfortunately, naive usage of these methods leads to fundamental scaling issues at thousands of GPUs, e.g., due to expensive cross-node communication or devices spending significant time waiting on other devices to make progress. In this paper, we show how different types of parallelism methods (tensor, pipeline, and data parallelism) can be composed to scale to thousands of GPUs and models with trillions of parameters. We survey techniques for pipeline parallelism and propose a novel interleaved pipeline parallelism schedule that can improve throughput by 10+\% with memory footprint comparable to existing approaches. We quantitatively study the trade-offs between tensor, pipeline, and data parallelism, and provide intuition as to how to configure distributed training of a large model. Our approach allows us to perform training iterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs with achieved per-GPU throughput of 52\% of theoretical peak. Our code is open sourced at https://github.com/nvidia/megatron-lm.},
  archiveprefix = {arXiv},
  annotation = {00000}
}

@article{neangOrganizingOceanographicInfrastructure2023,
  title = {Organizing {{Oceanographic Infrastructure}}: {{The Work}} of {{Making}} a {{Software Pipeline Repurposable}}},
  shorttitle = {Organizing {{Oceanographic Infrastructure}}},
  author = {Neang, Andrew B. and Sutherland, Will and Ribes, David and Lee, Charlotte P.},
  year = {2023},
  month = apr,
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  volume = {7},
  number = {CSCW1},
  pages = {1--18},
  issn = {2573-0142},
  doi = {10.1145/3579512},
  urldate = {2023-12-29},
  abstract = {Drawing from a longitudinal case study, we inspect the activities of an expanding team of scientists and their collaborators as they sought to develop a novel software pipeline that worked both for themselves and for their wider community. We argue that these two tasks - making the software work for themselves and also for their wider scientific community - could not be differentiated from each other at the beginning of the software development process. Rather, this division of labor and software capacities emerged, articulated by the actors themselves as they went about their tasks. The activities of making the novel software "work" at all, and the "extra work" of making that software repurposable or reusable could not be distinguished until near the end of the development process - rather than defined or structured in advance. We discuss implications for the trajectory of software development, and the practical work of making software repurposable.},
  langid = {english}
}

@misc{nemaDiversityDrivenAttention2018,
  title = {Diversity Driven {{Attention Model}} for {{Query-based Abstractive Summarization}}},
  author = {Nema, Preksha and Khapra, Mitesh and Laha, Anirban and Ravindran, Balaraman},
  year = {2018},
  month = jul,
  number = {arXiv:1704.08300},
  eprint = {1704.08300},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1704.08300},
  urldate = {2022-08-17},
  abstract = {Abstractive summarization aims to generate a shorter version of the document covering all the salient points in a compact and coherent fashion. On the other hand, query-based summarization highlights those points that are relevant in the context of a given query. The encode-attend-decode paradigm has achieved notable success in machine translation, extractive summarization, dialog systems, etc. But it suffers from the drawback of generation of repeated phrases. In this work we propose a model for the query-based summarization task based on the encode-attend-decode paradigm with two key additions (i) a query attention model (in addition to document attention model) which learns to focus on different portions of the query at different time steps (instead of using a static representation for the query) and (ii) a new diversity based attention model which aims to alleviate the problem of repeating phrases in the summary. In order to enable the testing of this model we introduce a new query-based summarization dataset building on debatepedia. Our experiments show that with these two additions the proposed model clearly outperforms vanilla encode-attend-decode models with a gain of 28\% (absolute) in ROUGE-L scores.},
  archiveprefix = {arXiv}
}

@inproceedings{nentidisOverviewBioASQ8a2020,
  title = {Overview of {{BioASQ}} 8a and 8b: {{Results}} of the {{Eighth Edition}} of the {{BioASQ Tasks}} a and b.},
  shorttitle = {Overview of {{BioASQ}} 8a and 8b},
  booktitle = {{{CLEF}} ({{Working Notes}})},
  author = {Nentidis, Anastasios and Krithara, Anastasia and Bougiatiotis, Konstantinos and Paliouras, Georgios},
  year = {2020}
}

@inproceedings{nguyenFrameworkLearningRequest2022,
  title = {A {{Framework}} for {{Learning}} to {{Request Rich}} and {{Contextually Useful Information}} from {{Humans}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Nguyen, Khanh X. and Bisk, Yonatan and Iii, Hal Daum{\'e}},
  year = {2022},
  month = jun,
  pages = {16553--16568},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v162/nguyen22a.html},
  urldate = {2025-06-04},
  abstract = {When deployed, AI agents will encounter problems that are beyond their autonomous problem-solving capabilities. Leveraging human assistance can help agents overcome their inherent limitations and robustly cope with unfamiliar situations. We present a general interactive framework that enables an agent to request and interpret rich, contextually useful information from an assistant that has knowledge about the task and the environment. We demonstrate the practicality of our framework on a simulated human-assisted navigation problem. Aided with an assistance-requesting policy learned by our method, a navigation agent achieves up to a 7\{{\textbackslash}texttimes\} improvement in success rate on tasks that take place in previously unseen environments, compared to fully autonomous behavior. We show that the agent can take advantage of different types of information depending on the context, and analyze the benefits and challenges of learning the assistance-requesting policy when the assistant can recursively decompose tasks into subtasks.},
  langid = {english}
}

@inproceedings{nguyenHelpAnnaVisual2019,
  title = {Help, {{Anna}}! {{Visual Navigation}} with {{Natural Multimodal Assistance}} via {{Retrospective Curiosity-Encouraging Imitation Learning}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Nguyen, Khanh and Daum{\'e} III, Hal},
  editor = {Inui, Kentaro and Jiang, Jing and Ng, Vincent and Wan, Xiaojun},
  year = {2019},
  month = nov,
  pages = {684--695},
  publisher = {Association for Computational Linguistics},
  address = {Hong Kong, China},
  doi = {10.18653/v1/D19-1063},
  urldate = {2025-06-04},
  abstract = {Mobile agents that can leverage help from humans can potentially accomplish more complex tasks than they could entirely on their own. We develop ``Help, Anna!'' (HANNA), an interactive photo-realistic simulator in which an agent fulfills object-finding tasks by requesting and interpreting natural language-and-vision assistance. An agent solving tasks in a HANNA environment can leverage simulated human assistants, called ANNA (Automatic Natural Navigation Assistants), which, upon request, provide natural language and visual instructions to direct the agent towards the goals. To address the HANNA problem, we develop a memory-augmented neural agent that hierarchically models multiple levels of decision-making, and an imitation learning algorithm that teaches the agent to avoid repeating past mistakes while simultaneously predicting its own chances of making future progress. Empirically, our approach is able to ask for help more effectively than competitive baselines and, thus, attains higher task success rate on both previously seen and previously unseen environments.}
}

@misc{nguyenIncontextExampleSelection2023,
  title = {In-Context {{Example Selection}} with {{Influences}}},
  author = {Nguyen, Tai and Wong, Eric},
  year = {2023},
  month = jun,
  number = {arXiv:2302.11042},
  eprint = {2302.11042},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2302.11042},
  urldate = {2023-11-01},
  abstract = {In-context learning (ICL) is a powerful paradigm emerged from large language models (LLMs). Despite its promises, ICL performance is known to be highly sensitive to input examples. In this work, we use \${\textbackslash}textit\{in-context influences\}\$ to analyze few-shot ICL performance directly from the in-context examples. Our proposed influence-based example selection method can identify both positive and negative examples, outperforming several baselines when evaluated on 9 SuperGLUE tasks. Our analysis uncovers up to a \$16.3{\textbackslash}\%\$ performance gap between using the most negative in-context examples compared to the most positive. In a case study, we apply our influence-based framework to quantify the phenomena of recency bias in example ordering for few-shot ICL.},
  archiveprefix = {arXiv}
}

@article{niuEHRKnowGenKnowledgeenhancedMultimodal2024,
  title = {{{EHR-KnowGen}}: {{Knowledge-enhanced}} Multimodal Learning for Disease Diagnosis Generation},
  shorttitle = {{{EHR-KnowGen}}},
  author = {Niu, Shuai and Ma, Jing and Bai, Liang and Wang, Zhihua and Guo, Li and Yang, Xian},
  year = {2024},
  month = feb,
  journal = {Information Fusion},
  volume = {102},
  pages = {102069},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2023.102069},
  urldate = {2024-01-11},
  abstract = {Electronic health records (EHRs) contain diverse patient information, including medical notes, clinical events, and laboratory test results. Integrating this multimodal data can improve disease diagnoses using deep learning models. However, effectively combining different modalities for diagnosis remains challenging. Previous approaches, such as attention mechanisms and contrastive learning, have attempted to address this but do not fully integrate the modalities into a unified feature space. This paper presents EHR-KnowGen, a multimodal learning model enhanced with external domain knowledge, for improved disease diagnosis generation from diverse patient information in EHRs. Unlike previous approaches, our model integrates different modalities into a unified feature space with soft prompts learning and leverages large language models (LLMs) to generate disease diagnoses. By incorporating external domain knowledge from different levels of granularity, we enhance the extraction and fusion of multimodal information, resulting in more accurate diagnosis generation. Experimental results on real-world EHR datasets demonstrate the superiority of our generative model over comparative methods, providing explainable evidence to enhance the understanding of diagnosis results.}
}

@misc{niuEnhancingDialogueState2024,
  title = {Enhancing {{Dialogue State Tracking Models}} through {{LLM-backed User-Agents Simulation}}},
  author = {Niu, Cheng and Wang, Xingguang and Cheng, Xuxin and Song, Juntong and Zhang, Tong},
  year = {2024},
  month = may,
  number = {arXiv:2405.13037},
  eprint = {2405.13037},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.13037},
  urldate = {2024-10-01},
  abstract = {Dialogue State Tracking (DST) is designed to monitor the evolving dialogue state in the conversations and plays a pivotal role in developing task-oriented dialogue systems. However, obtaining the annotated data for the DST task is usually a costly endeavor. In this paper, we focus on employing LLMs to generate dialogue data to reduce dialogue collection and annotation costs. Specifically, GPT-4 is used to simulate the user and agent interaction, generating thousands of dialogues annotated with DST labels. Then a two-stage fine-tuning on LLaMA 2 is performed on the generated data and the real data for the DST prediction. Experimental results on two public DST benchmarks show that with the generated dialogue data, our model performs better than the baseline trained solely on real data. In addition, our approach is also capable of adapting to the dynamic demands in real-world scenarios, generating dialogues in new domains swiftly. After replacing dialogue segments in any domain with the corresponding generated ones, the model achieves comparable performance to the model trained on real data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@inproceedings{norgaardWhatUsabilityEvaluators2006,
  title = {What Do Usability Evaluators Do in Practice? An Explorative Study of Think-Aloud Testing},
  shorttitle = {What Do Usability Evaluators Do in Practice?},
  booktitle = {Proceedings of the 6th Conference on {{Designing Interactive}} Systems},
  author = {N{\o}rgaard, Mie and Hornb{\ae}k, Kasper},
  year = {2006},
  month = jun,
  series = {{{DIS}} '06},
  pages = {209--218},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1142405.1142439},
  urldate = {2024-09-10},
  abstract = {Think-aloud testing is a widely employed usability evaluation method, yet its use in practice is rarely studied. We report an explorative study of 14 think-aloud sessions, the audio recordings of which were examined in detail. The study shows that immediate analysis of observations made in the think-aloud sessions is done only sporadically, if at all. When testing, evaluators seem to seek confirmation of problems that they are already aware of. During testing, evaluators often ask users about their expectations and about hypothetical situations, rather than about experienced problems. In addition, evaluators learn much about the usability of the tested system but little about its utility. The study shows how practical realities rarely discussed in the literature on usability evaluation influence sessions. We discuss implications for usability researchers and professionals, including techniques for fast-paced analysis and tools for capturing observations during sessions.},
  isbn = {978-1-59593-367-6}
}

@misc{noriCanGeneralistFoundation2023,
  title = {Can {{Generalist Foundation Models Outcompete Special-Purpose Tuning}}? {{Case Study}} in {{Medicine}}},
  shorttitle = {Can {{Generalist Foundation Models Outcompete Special-Purpose Tuning}}?},
  author = {Nori, Harsha and Lee, Yin Tat and Zhang, Sheng and Carignan, Dean and Edgar, Richard and Fusi, Nicolo and King, Nicholas and Larson, Jonathan and Li, Yuanzhi and Liu, Weishung and Luo, Renqian and McKinney, Scott Mayer and Ness, Robert Osazuwa and Poon, Hoifung and Qin, Tao and Usuyama, Naoto and White, Chris and Horvitz, Eric},
  year = {2023},
  month = nov,
  number = {arXiv:2311.16452},
  eprint = {2311.16452},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2311.16452},
  urldate = {2023-12-04},
  abstract = {Generalist foundation models such as GPT-4 have displayed surprising capabilities in a wide variety of domains and tasks. Yet, there is a prevalent assumption that they cannot match specialist capabilities of fine-tuned models. For example, most explorations to date on medical competency benchmarks have leveraged domain-specific training, as exemplified by efforts on BioGPT and Med-PaLM. We build on a prior study of GPT-4's capabilities on medical challenge benchmarks in the absence of special training. Rather than using simple prompting to highlight the model's out-of-the-box capabilities, we perform a systematic exploration of prompt engineering. We find that prompting innovation can unlock deeper specialist capabilities and show that GPT-4 easily tops prior leading results for medical benchmarks. The prompting methods we explore are general purpose, and make no specific use of domain expertise, removing the need for expert-curated content. Our experimental design carefully controls for overfitting during the prompt engineering process. We introduce Medprompt, based on a composition of several prompting strategies. With Medprompt, GPT-4 achieves state-of-the-art results on all nine of the benchmark datasets in the MultiMedQA suite. The method outperforms leading specialist models such as Med-PaLM 2 by a significant margin with an order of magnitude fewer calls to the model. Steering GPT-4 with Medprompt achieves a 27\% reduction in error rate on the MedQA dataset over the best methods to date achieved with specialist models and surpasses a score of 90\% for the first time. Beyond medical problems, we show the power of Medprompt to generalize to other domains and provide evidence for the broad applicability of the approach via studies of the strategy on exams in electrical engineering, machine learning, philosophy, accounting, law, nursing, and clinical psychology.},
  archiveprefix = {arXiv}
}

@misc{NotionAlloneWorkspace,
  title = {Notion -- {{The}} All-in-One Workspace for Your Notes, Tasks, Wikis, and Databases.},
  journal = {Notion},
  url = {https://www.notion.so},
  urldate = {2024-09-08},
  abstract = {A new tool that blends your everyday work apps into one. It's the all-in-one workspace for you and your team},
  langid = {english}
}

@article{omarovArtificialIntelligenceEnabled2022,
  title = {Artificial Intelligence Enabled Conversational Agent for Mental Healthcare},
  author = {Omarov, Batyrkhan and Narynov, Sergazi and Zhumanov, Zhandos and Alzhanova, Elmira and Gumar, Aidana and Khassanova, Mariyam},
  year = {2022},
  month = oct,
  journal = {International journal of health sciences},
  volume = {6},
  number = {3},
  pages = {1544--1555},
  issn = {2550-696X, 2550-6978},
  doi = {10.53730/ijhs.v6n3.13239},
  urldate = {2024-05-02},
  abstract = {Conversational agents are~a software program that can converse with users in the manner of a real-world conversation. Artificial intelligence (AI)~ ~is not complete without conversation modeling. The most difficult artificial intelligence~endeavor since its start has been developing an effective chatbot application. Despite chatbots may do a variety of tasks, their main duty is to accurately understand human speech and respond appropriately. Previously, manual patterns and instructions or simple statistical methods were used to create chatbots architectures. Due to its improved capacity for training, end-to-end AI~has replaced these models since 2015. The most popular technique for conversation simulation at the moment is the encoder-decoder recurrent neural network (RNN). The realm of language comprehension served as inspiration for this design. Until recently, a number of additions and changes dramatically enhanced chatbot conversational abilities. In this paper, we outline our research results~into creating an interactive digital chatbot that may provide patients with psychological assistance. To build and train the chatbot, we used resources such Rasa Natural Language Processing (NLU)~technology, which employs natural language processing (NLP)~methods. The results of the investigation showed that selecting proper responses while conversing with patients had a more than~70\%~predictive performance.},
  copyright = {http://creativecommons.org/licenses/by-nc-nd/4.0}
}

@misc{openaiGPT4TechnicalReport2023,
  title = {{{GPT-4 Technical Report}}},
  author = {OpenAI},
  year = {2023},
  month = dec,
  number = {arXiv:2303.08774},
  eprint = {2303.08774},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.08774},
  urldate = {2024-01-10},
  abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
  archiveprefix = {arXiv}
}

@article{oppenlaenderStatePilotStudy2024,
  title = {The {{State}} of {{Pilot Study Reporting}} in {{Crowdsourcing}}: {{A Reflection}} on {{Best Practices}} and {{Guidelines}}},
  shorttitle = {The {{State}} of {{Pilot Study Reporting}} in {{Crowdsourcing}}},
  author = {Oppenlaender, Jonas and Abbas, Tahir and Gadiraju, Ujwal},
  year = {2024},
  month = apr,
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {8},
  number = {CSCW1},
  pages = {184:1--184:45},
  doi = {10.1145/3641023},
  urldate = {2024-09-11},
  abstract = {Pilot studies are an essential cornerstone of the design of crowdsourcing campaigns, yet they are often only mentioned in passing in the scholarly literature. A lack of details surrounding pilot studies in crowdsourcing research hinders the replication of studies and the reproduction of findings, stalling potential scientific advances. We conducted a systematic literature review on the current state of pilot study reporting at the intersection of crowdsourcing and HCI research. Our review of ten years of literature included 171 articles published in the proceedings of the Conference on Human Computation and Crowdsourcing (AAAI HCOMP) and the ACM Digital Library. We found that pilot studies in crowdsourcing research (i.e., crowd pilot studies) are often under-reported in the literature. Important details, such as the number of workers and rewards to workers, are often not reported. On the basis of our findings, we reflect on the current state of practice and formulate a set of best practice guidelines for reporting crowd pilot studies in crowdsourcing research. We also provide implications for the design of crowdsourcing platforms and make practical suggestions for supporting crowd pilot study reporting.}
}

@inproceedings{ostrowskiKnowledgebasedSoftwareTesting1999,
  title = {Knowledge-Based Software Testing Agent Using Evolutionary Learning with Cultural Algorithms},
  booktitle = {Proceedings of the 1999 {{Congress}} on {{Evolutionary Computation-CEC99}} ({{Cat}}. {{No}}. {{99TH8406}})},
  author = {Ostrowski, D.A. and Reynolds, R.G.},
  year = {1999},
  month = jul,
  volume = {3},
  pages = {1657-1663 Vol. 3},
  doi = {10.1109/CEC.1999.785473},
  urldate = {2024-09-30},
  abstract = {Software testing is extremely difficult in the context of large scale engineering applications. We suggest that the application of the white and black box testing methods within a cultural algorithm environment will present a successful approach to fault detection. In order to utilize both a functional approach and a structural approach, two cultural algorithms will be applied within this tool. The first cultural algorithm will utilize the black box testing by learning equivalence classes of faulty input/output pairs. These equivalence classes are then passed over to the second cultural algorithm that will apply program slicing techniques to determine program slices from the data. The goal will be to pinpoint specific faults within the program design. Through the searching of the program code, this approach can be considered as behavioral mining of a program.},
  keywords = {Application software,Cultural differences,Laboratories,Large-scale systems,Programming,Software algorithms,Software maintenance,Software systems,Software testing,System testing}
}

@article{ouyangSQuADBioASQAnalysis,
  title = {{{SQuAD}} to {{BioASQ}}: Analysis of General to Specific},
  author = {Ouyang, Karen},
  pages = {7},
  abstract = {As biomedical information in the form of publications and electronic health records (EHR) increases at an increasingly fast pace, there is clear utility in having systems that can automatically handle information extraction, summarization, and question answering tasks. While there have been significant strides in improving language tasks for general language, addressing domain-specific contexts still remains challenging. In this project, I apply and fine-tune models to the SQuAD dataset and further modify/adapt for biomedical domain-specific question answering. I evaluated and compared performance on the SQuAD dataset and BioASQ, a biomedical literature QA dataset, with the goal of analyzing and developing approaches to leverage unsupervised language models for domain-specific applications. Upon generating various fine-tuned models, the best performance for general language SQuAD QA achieved an F1 score of 76.717, EM score of 73.379, and for biomedical-specific BioASQ QA achieved an F1 score of 70.348 and EM score of 49.902.},
  langid = {english}
}

@misc{ouyangTrainingLanguageModels2022,
  title = {Training Language Models to Follow Instructions with Human Feedback},
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  year = {2022},
  month = mar,
  number = {arXiv:2203.02155},
  eprint = {2203.02155},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.02155},
  urldate = {2023-05-19},
  abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
  archiveprefix = {arXiv}
}

@inproceedings{paliourasBioASQChallengeLargescale2012,
  title = {{{BioASQ}}: {{A}} Challenge on Large-Scale Biomedical Semantic Indexing and Question Answering},
  author = {Paliouras, Georgios},
  year = {2012},
  month = nov,
  url = {sites/default/files/PublicDocuments/2012-Paliouras-BioASQ.pdf},
  keywords = {BioASQ public document}
}

@article{pampariEmrQALargeCorpus2018,
  title = {{{emrQA}}: {{A Large Corpus}} for {{Question Answering}} on {{Electronic Medical Records}}},
  shorttitle = {{{emrQA}}},
  author = {Pampari, Anusri and Raghavan, Preethi and Liang, Jennifer and Peng, Jian},
  year = {2018},
  month = sep,
  journal = {arXiv:1809.00732 [cs]},
  eprint = {1809.00732},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1809.00732},
  urldate = {2021-11-30},
  abstract = {We propose a novel methodology to generate domain-specific large-scale question answering (QA) datasets by re-purposing existing annotations for other NLP tasks. We demonstrate an instance of this methodology in generating a large-scale QA dataset for electronic medical records by leveraging existing expert annotations on clinical notes for various NLP tasks from the community shared i2b2 datasets{\S}. The resulting corpus (emrQA) has 1 million question-logical form and 400,000+ questionanswer evidence pairs. We characterize the dataset and explore its learning potential by training baseline models for question to logical form and question to answer mapping.},
  archiveprefix = {arXiv},
  langid = {english}
}

@misc{panDynaThinkFastSlow2024,
  title = {{{DynaThink}}: {{Fast}} or {{Slow}}? {{A Dynamic Decision-Making Framework}} for {{Large Language Models}}},
  shorttitle = {{{DynaThink}}},
  author = {Pan, Jiabao and Zhang, Yan and Zhang, Chen and Liu, Zuozhu and Wang, Hongwei and Li, Haizhou},
  year = {2024},
  month = jul,
  number = {arXiv:2407.01009},
  eprint = {2407.01009},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.01009},
  urldate = {2024-08-13},
  abstract = {Large language models (LLMs) have demonstrated emergent capabilities across diverse reasoning tasks via popular Chains-of-Thought (COT) prompting. However, such a simple and fast COT approach often encounters limitations in dealing with complicated problems, while a thorough method, which considers multiple reasoning pathways and verifies each step carefully, results in slower inference. This paper addresses the challenge of enabling LLMs to autonomously select between fast and slow inference methods, thereby optimizing both efficiency and effectiveness. We introduce a dynamic decision-making framework that categorizes tasks into two distinct pathways: 'Fast', designated for tasks where the LLM quickly identifies a high-confidence solution, and 'Slow', allocated for tasks that the LLM perceives as complex and for which it has low confidence in immediate solutions as well as requiring more reasoning paths to verify. Experiments on five popular reasoning benchmarks demonstrated the superiority of the DynaThink over baselines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{panWebCanvasBenchmarkingWeb2024,
  title = {{{WebCanvas}}: {{Benchmarking Web Agents}} in {{Online Environments}}},
  shorttitle = {{{WebCanvas}}},
  author = {Pan, Yichen and Kong, Dehan and Zhou, Sida and Cui, Cheng and Leng, Yifei and Jiang, Bing and Liu, Hangyu and Shang, Yanyi and Zhou, Shuyan and Wu, Tongshuang and Wu, Zhengyang},
  year = {2024},
  month = jul,
  number = {arXiv:2406.12373},
  eprint = {2406.12373},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.12373},
  urldate = {2024-07-17},
  abstract = {For web agents to be practically useful, they must adapt to the continuously evolving web environment characterized by frequent updates to user interfaces and content. However, most existing benchmarks only capture the static aspects of the web. To bridge this gap, we introduce WebCanvas, an innovative online evaluation framework for web agents that effectively addresses the dynamic nature of web interactions. WebCanvas contains three main components to facilitate realistic assessments: (1) A novel evaluation metric which reliably capture critical intermediate actions or states necessary for task completions while disregarding noise caused by insignificant events or changed web-elements. (2) A benchmark dataset called Mind2Web-Live, a refined version of original Mind2Web static dataset containing 542 tasks with 2439 intermediate evaluation states; (3) Lightweight and generalizable annotation tools and testing pipelines that enables the community to collect and maintain the high-quality, up-to-date dataset. Building on WebCanvas, we open-source an agent framework with extensible modules for reasoning, providing a foundation for the community to conduct online inference and evaluations. Our best-performing agent achieves a task success rate of 23.1\% and a task completion rate of 48.8\% on the Mind2Web-Live test set. Additionally, we analyze the performance discrepancies across various websites, domains, and experimental environments. We encourage the community to contribute further insights on online agent evaluation, thereby advancing this field of research.},
  archiveprefix = {arXiv},
  keywords = {68T50,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,I.2.7}
}

@inproceedings{pappasAUEBNLPBioASQBiomedical2020,
  title = {{{AUEB-NLP}} at {{BioASQ}} 8: {{Biomedical Document}} and {{Snippet Retrieval}}.},
  shorttitle = {{{AUEB-NLP}} at {{BioASQ}} 8},
  booktitle = {{{CLEF}} ({{Working Notes}})},
  author = {Pappas, Dimitris and Stavropoulos, Petros and Androutsopoulos, Ion},
  year = {2020}
}

@inproceedings{pappasBioMRCDatasetBiomedical2020,
  title = {{{BioMRC}}: {{A Dataset}} for {{Biomedical Machine Reading Comprehension}}},
  shorttitle = {{{BioMRC}}},
  booktitle = {Proceedings of the 19th {{SIGBioMed Workshop}} on {{Biomedical Language Processing}}},
  author = {Pappas, Dimitris and Stavropoulos, Petros and Androutsopoulos, Ion and McDonald, Ryan},
  year = {2020},
  month = jul,
  pages = {140--149},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.bionlp-1.15},
  urldate = {2021-12-25},
  abstract = {We introduceBIOMRC, a large-scale cloze-style biomedical MRC dataset. Care was taken to reduce noise, compared to the previous BIOREAD dataset of Pappas et al. (2018). Experiments show that simple heuristics do not perform well on the new dataset and that two neural MRC models that had been tested on BIOREAD perform much better on BIOMRC, indicating that the new dataset is indeed less noisy or at least that its task is more feasible. Non-expert human performance is also higher on the new dataset compared to BIOREAD, and biomedical experts perform even better. We also introduce a new BERT-based MRC model, the best version of which substantially outperforms all other methods tested, reaching or surpassing the accuracy of biomedical experts in some experiments. We make the new dataset available in three different sizes, also releasing our code, and providing a leaderboard.},
  langid = {english},
  annotation = {00009}
}

@inproceedings{pappasBioReadNewDataset2018,
  title = {{{BioRead}}: {{A New Dataset}} for {{Biomedical Reading Comprehension}}},
  shorttitle = {{{BioRead}}},
  booktitle = {Proceedings of the {{Eleventh International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}} 2018)},
  author = {Pappas, Dimitris and Androutsopoulos, Ion and Papageorgiou, Haris},
  year = {2018},
  month = may,
  publisher = {European Language Resources Association (ELRA)},
  address = {Miyazaki, Japan},
  url = {https://aclanthology.org/L18-1439},
  urldate = {2021-12-25},
  langid = {english},
  annotation = {00012}
}

@inproceedings{parikhElimiNetModelEliminating2018,
  title = {{{ElimiNet}}: {{A Model}} for {{Eliminating Options}} for {{Reading Comprehension}} with {{Multiple Choice Questions}}},
  shorttitle = {{{ElimiNet}}},
  booktitle = {Proceedings of the {{Twenty-Seventh International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Parikh, Soham and Sai, Ananya and Nema, Preksha and Khapra, Mitesh},
  year = {2018},
  month = jul,
  pages = {4272--4278},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  address = {Stockholm, Sweden},
  doi = {10.24963/ijcai.2018/594},
  urldate = {2022-04-13},
  abstract = {The task of Reading Comprehension with Multiple Choice Questions, requires a human (or machine) to read a given \{passage, question\} pair and select one of the n given options. The current state of the art model for this task first computes a questionaware representation for the passage and then selects the option which has the maximum similarity with this representation. However, when humans perform this task they do not just focus on option selection but use a combination of elimination and selection. Specifically, a human would first try to eliminate the most irrelevant option and then read the passage again in the light of this new information (and perhaps ignore portions corresponding to the eliminated option). This process could be repeated multiple times till the reader is finally ready to select the correct option. We propose ElimiNet, a neural network-based model which tries to mimic this process. Specifically, it has gates which decide whether an option can be eliminated given the \{passage, question\} pair and if so it tries to make the passage representation orthogonal to this eliminated option (akin to ignoring portions of the passage corresponding to the eliminated option). The model makes multiple rounds of partial elimination to refine the passage representation and finally uses a selection module to pick the best option. We evaluate our model on the recently released large scale RACE dataset and show that it outperforms the current state of the art model on 7 out of the 13 question types in this dataset. Further, we show that taking an ensemble of our elimination-selection based method with a selection based method gives us an improvement of 3.1\% over the best-reported performance on this dataset.},
  isbn = {978-0-9992411-2-7},
  langid = {english}
}

@misc{parkGenerativeAgentSimulations2024,
  title = {Generative {{Agent Simulations}} of 1,000 {{People}}},
  author = {Park, Joon Sung and Zou, Carolyn Q. and Shaw, Aaron and Hill, Benjamin Mako and Cai, Carrie and Morris, Meredith Ringel and Willer, Robb and Liang, Percy and Bernstein, Michael S.},
  year = {2024},
  month = nov,
  number = {arXiv:2411.10109},
  eprint = {2411.10109},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.10109},
  urldate = {2025-03-15},
  abstract = {The promise of human behavioral simulation--general-purpose computational agents that replicate human behavior across domains--could enable broad applications in policymaking and social science. We present a novel agent architecture that simulates the attitudes and behaviors of 1,052 real individuals--applying large language models to qualitative interviews about their lives, then measuring how well these agents replicate the attitudes and behaviors of the individuals that they represent. The generative agents replicate participants' responses on the General Social Survey 85\% as accurately as participants replicate their own answers two weeks later, and perform comparably in predicting personality traits and outcomes in experimental replications. Our architecture reduces accuracy biases across racial and ideological groups compared to agents given demographic descriptions. This work provides a foundation for new tools that can help investigate individual and collective behavior.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning}
}

@inproceedings{parkGenerativeAgentsInteractive2023,
  title = {Generative {{Agents}}: {{Interactive Simulacra}} of {{Human Behavior}}},
  shorttitle = {Generative {{Agents}}},
  booktitle = {Proceedings of the 36th {{Annual ACM Symposium}} on {{User Interface Software}} and {{Technology}}},
  author = {Park, Joon Sung and O'Brien, Joseph and Cai, Carrie Jun and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
  year = {2023},
  month = oct,
  series = {{{UIST}} '23},
  pages = {1--22},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3586183.3606763},
  urldate = {2024-07-04},
  abstract = {Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents: computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty-five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors. For example, starting with only a single user-specified notion that one agent wants to throw a Valentine's Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture---observation, planning, and reflection---each contribute critically to the believability of agent behavior. By fusing large language models with computational interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.},
  isbn = {979-8-4007-0132-0},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning}
}

@misc{parkGrammarAlignedDecoding2024,
  title = {Grammar-{{Aligned Decoding}}},
  author = {Park, Kanghee and Wang, Jiayu and {Berg-Kirkpatrick}, Taylor and Polikarpova, Nadia and D'Antoni, Loris},
  year = {2024},
  month = may,
  number = {arXiv:2405.21047},
  eprint = {2405.21047},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.21047},
  urldate = {2024-09-23},
  abstract = {Large Language Models (LLMs) struggle with reliably generating highly structured outputs, such as program code, mathematical formulas, or well-formed markup. Constrained decoding approaches mitigate this problem by greedily restricting what tokens an LLM can output at each step to guarantee that the output matches a given constraint. Specifically, in grammar-constrained decoding (GCD), the LLM's output must follow a given grammar. In this paper we demonstrate that GCD techniques (and in general constrained decoding techniques) can distort the LLM's distribution, leading to outputs that are grammatical but appear with likelihoods that are not proportional to the ones given by the LLM, and so ultimately are low-quality. We call the problem of aligning sampling with a grammar constraint, grammar-aligned decoding (GAD), and propose adaptive sampling with approximate expected futures (ASAp), a decoding algorithm that guarantees the output to be grammatical while provably producing outputs that match the conditional probability of the LLM's distribution conditioned on the given grammar constraint. Our algorithm uses prior sample outputs to soundly overapproximate the future grammaticality of different output prefixes. Our evaluation on code generation and structured NLP tasks shows how ASAp often produces outputs with higher likelihood (according to the LLM's distribution) than existing GCD techniques, while still enforcing the desired grammatical constraints.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@article{parkLRAGELegalRetrieval,
  title = {{{LRAGE}}: {{Legal Retrieval Augmented Generation Evaluation Tool}}},
  author = {Park, Minhu and Oh, Hongseok and Choi, Eunkyung and Hwang, Wonseok},
  abstract = {Recently, building retrieval-augmented generation (RAG) systems to enhance the capability of large language models (LLMs) has become a common practice. Especially in the legal domain, previous judicial decisions play a significant role under the doctrine of stare decisis which emphasizes the importance of making decisions based on (retrieved) prior documents. However, the overall performance of RAG system depends on many components: (1) retrieval corpora, (2) retrieval algorithms, (3) rerankers, (4) LLM backbones, and (5) evaluation metrics. Here we propose LRAGE, an open-source tool for holistic evaluation of RAG systems focusing on the legal domain. LRAGE provides GUI and CLI interfaces to facilitate seamless experiments and investigate how changes in the aforementioned five components affect the overall accuracy. We validated LRAGE using multilingual legal benches including Korean (KBL), English (LegalBench), and Chinese (LawBench) by demonstrating how the overall accuracy changes when varying the five components mentioned above. The source code is available at https://github. com/hoorangyee/LRAGE.},
  langid = {english}
}

@misc{parkSocialSimulacraCreating2022,
  title = {Social {{Simulacra}}: {{Creating Populated Prototypes}} for {{Social Computing Systems}}},
  shorttitle = {Social {{Simulacra}}},
  author = {Park, Joon Sung and Popowski, Lindsay and Cai, Carrie J. and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
  year = {2022},
  month = aug,
  number = {arXiv:2208.04024},
  eprint = {2208.04024},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2208.04024},
  urldate = {2023-06-25},
  abstract = {Social computing prototypes probe the social behaviors that may arise in an envisioned system design. This prototyping practice is currently limited to recruiting small groups of people. Unfortunately, many challenges do not arise until a system is populated at a larger scale. Can a designer understand how a social system might behave when populated, and make adjustments to the design before the system falls prey to such challenges? We introduce social simulacra, a prototyping technique that generates a breadth of realistic social interactions that may emerge when a social computing system is populated. Social simulacra take as input the designer's description of a community's design -- goal, rules, and member personas -- and produce as output an instance of that design with simulated behavior, including posts, replies, and anti-social behaviors. We demonstrate that social simulacra shift the behaviors that they generate appropriately in response to design changes, and that they enable exploration of "what if?" scenarios where community members or moderators intervene. To power social simulacra, we contribute techniques for prompting a large language model to generate thousands of distinct community members and their social interactions with each other; these techniques are enabled by the observation that large language models' training data already includes a wide variety of positive and negative behavior on social media platforms. In evaluations, we show that participants are often unable to distinguish social simulacra from actual community behavior and that social computing designers successfully refine their social computing designs when using social simulacra.},
  archiveprefix = {arXiv}
}

@article{parrANTLRPredicatedLLParser1995,
  title = {{{ANTLR}}: A Predicated-{{{\emph{LL}}}}{\emph{(k)}} Parser Generator},
  shorttitle = {{{ANTLR}}},
  author = {Parr, T. J. and Quong, R. W.},
  year = {1995},
  month = jul,
  journal = {Software---Practice \& Experience},
  volume = {25},
  number = {7},
  pages = {789--810},
  issn = {0038-0644},
  doi = {10.1002/spe.4380250705},
  urldate = {2022-06-06}
}

@inproceedings{pascualPlugandPlayMethodControlled2021,
  title = {A {{Plug-and-Play Method}} for {{Controlled Text Generation}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2021},
  author = {Pascual, Damian and Egressy, Beni and Meister, Clara and Cotterell, Ryan and Wattenhofer, Roger},
  year = {2021},
  month = nov,
  pages = {3973--3997},
  publisher = {Association for Computational Linguistics},
  address = {Punta Cana, Dominican Republic},
  doi = {10.18653/v1/2021.findings-emnlp.334},
  urldate = {2022-12-25},
  abstract = {Large pre-trained language models have repeatedly shown their ability to produce fluent text. Yet even when starting from a prompt, generation can continue in many plausible directions. Current decoding methods with the goal of controlling generation, e.g., to ensure specific words are included, either require additional models or fine-tuning, or work poorly when the task at hand is semantically unconstrained, e.g., story generation. In this work, we present a plug-and-play decoding method for controlled language generation that is so simple and intuitive, it can be described in a single sentence: given a topic or keyword, we add a shift to the probability distribution over our vocabulary towards semantically similar words. We show how annealing this distribution can be used to impose hard constraints on language generation, something no other plug-and-play method is currently able to do with SOTA language generators. Despite the simplicity of this approach, we see it works incredibly well in practice: decoding from GPT-2 leads to diverse and fluent sentences while guaranteeing the appearance of given guide words. We perform two user studies, revealing that (1) our method outperforms competing methods in human evaluations; and (2) forcing the guide words to appear in the generated text has no impact on the fluency of the generated text.}
}

@misc{patelLargeLanguageModels2024,
  title = {Large {{Language Models Can Self-Improve At Web Agent Tasks}}},
  author = {Patel, Ajay and Hofmarcher, Markus and {Leoveanu-Condrei}, Claudiu and Dinu, Marius-Constantin and {Callison-Burch}, Chris and Hochreiter, Sepp},
  year = {2024},
  month = may,
  number = {arXiv:2405.20309},
  eprint = {2405.20309},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2405.20309},
  urldate = {2024-08-08},
  abstract = {Training models to act as agents that can effectively navigate and perform actions in a complex environment, such as a web browser, has typically been challenging due to lack of training data. Large language models (LLMs) have recently demonstrated some capability to navigate novel environments as agents in a zero-shot or few-shot fashion, purely guided by natural language instructions as prompts. Recent research has also demonstrated LLMs have the capability to exceed their base performance through self-improvement, i.e. fine-tuning on data generated by the model itself. In this work, we explore the extent to which LLMs can self-improve their performance as agents in long-horizon tasks in a complex environment using the WebArena benchmark. In WebArena, an agent must autonomously navigate and perform actions on web pages to achieve a specified objective. We explore fine-tuning on three distinct synthetic training data mixtures and achieve a 31\% improvement in task completion rate over the base model on the WebArena benchmark through a self-improvement procedure. We additionally contribute novel evaluation metrics for assessing the performance, robustness, capabilities, and quality of trajectories of our fine-tuned agent models to a greater degree than simple, aggregate-level benchmark scores currently used to measure self-improvement.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@inproceedings{pawelczakBenefitsDrawbacksSource2018,
  title = {Benefits and Drawbacks of Source Code Plagiarism Detection in Engineering Education},
  booktitle = {2018 {{IEEE Global Engineering Education Conference}} ({{EDUCON}})},
  author = {Pawelczak, Dieter},
  year = {2018},
  month = apr,
  pages = {1048--1056},
  issn = {2165-9567},
  doi = {10.1109/EDUCON.2018.8363346},
  abstract = {Source code plagiarism is wide spread in beginners' programming courses. Especially, if programming is a minor subject, as for instance in engineering degrees. It is very tempting for students during a programming assignment to use a working copy of a fellow student rather than struggling with the time-consuming coding by themselves. But as learning programming requires a significant personal commitment, we confirm the results of other studies, that cheating leads to higher failure rates and lower scores in the examination. Automatic plagiarism detection systems are therefore measures against cheating. We analyzed the students' achievements and opinions during the last 5 years of operating an automated assessment system with plagiarism detection. The paper discusses in detail the benefits of such a system, e.g. the equal treatment of all students compared to manual plagiarism checks, and shows also the disadvantages, e.g. code obfuscation, that students perform in order to circumvent the system.}
}

@misc{PDFUsabilityTesting,
  title = {[{{PDF}}] {{Usability}} Testing: A Review of Some Methodological and Technical Aspects of the Method {\textbar} {{Semantic Scholar}}},
  url = {https://www.semanticscholar.org/reader/d9dc32226f65f3b16734d37388fc973eb5f6774a},
  urldate = {2024-09-10}
}

@article{pennycookShiftingAttentionAccuracy2021,
  title = {Shifting Attention to Accuracy Can Reduce Misinformation Online},
  author = {Pennycook, Gordon and Epstein, Ziv and Mosleh, Mohsen and Arechar, Antonio A. and Eckles, Dean and Rand, David G.},
  year = {2021},
  month = apr,
  journal = {Nature},
  volume = {592},
  number = {7855},
  pages = {590--595},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-03344-2},
  urldate = {2023-06-25},
  abstract = {In recent years, there has been a great deal of concern about the proliferation of false and misleading news on social media1--4. Academics and practitioners alike have asked why people share such misinformation, and sought solutions to reduce the sharing of misinformation5--7. Here, we attempt to address both of these questions. First, we find that the veracity of headlines has little effect on sharing intentions, despite having a large effect on judgments of accuracy. This dissociation suggests that sharing does not necessarily indicate belief. Nonetheless, most participants say it is important to share only accurate news. To shed light on this apparent contradiction, we carried out four survey experiments and a field experiment on Twitter; the results show that subtly shifting attention to accuracy increases the quality of news that people subsequently share. Together with additional computational analyses, these findings indicate that people often share misinformation because their attention is focused on factors other than accuracy---and therefore they fail to implement a strongly~held preference for accurate sharing. Our results challenge the popular claim that people value partisanship over accuracy8,9, and provide evidence for scalable attention-based interventions that social media platforms could easily implement to counter misinformation online.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english}
}

@misc{perez-mayosHowMuchPretraining2021,
  title = {How Much Pretraining Data Do Language Models Need to Learn Syntax?},
  author = {{P{\'e}rez-Mayos}, Laura and Ballesteros, Miguel and Wanner, Leo},
  year = {2021},
  number = {arXiv:2109.03160},
  eprint = {2109.03160},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2109.03160},
  urldate = {2024-04-25},
  abstract = {Transformers-based pretrained language models achieve outstanding results in many well-known NLU benchmarks. However, while pretraining methods are very convenient, they are expensive in terms of time and resources. This calls for a study of the impact of pretraining data size on the knowledge of the models. We explore this impact on the syntactic capabilities of RoBERTa, using models trained on incremental sizes of raw text data. First, we use syntactic structural probes to determine whether models pretrained on more data encode a higher amount of syntactic information. Second, we perform a targeted syntactic evaluation to analyze the impact of pretraining data size on the syntactic generalization performance of the models. Third, we compare the performance of the different models on three downstream applications: part-of-speech tagging, dependency parsing and paraphrase identification. We complement our study with an analysis of the cost-benefit trade-off of training such models. Our experiments show that while models pretrained on more data encode more syntactic knowledge and perform better on downstream applications, they do not always offer a better performance across the different syntactic phenomena and come at a higher financial and environmental cost.},
  archiveprefix = {arXiv},
  langid = {american}
}

@inproceedings{perisPrivacyTimeLanguage2023,
  title = {Privacy in the {{Time}} of {{Language Models}}},
  booktitle = {Proceedings of the {{Sixteenth ACM International Conference}} on {{Web Search}} and {{Data Mining}}},
  author = {Peris, Charith and Dupuy, Christophe and Majmudar, Jimit and Parikh, Rahil and Smaili, Sami and Zemel, Richard and Gupta, Rahul},
  year = {2023},
  month = feb,
  series = {{{WSDM}} '23},
  pages = {1291--1292},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3539597.3575792},
  urldate = {2024-09-12},
  abstract = {Pretrained large language models (LLMs) have consistently shown state-of-the-art performance across multiple natural language processing (NLP) tasks. These models are of much interest for a variety of industrial applications that use NLP as a core component. However, LLMs have also been shown to memorize portions of their training data, which can contain private information. Therefore, when building and deploying LLMs, it is of value to apply privacy-preserving techniques that protect sensitive data.In this talk, we discuss privacy measurement and preservation techniques for LLMs that can be applied in the context of industrial applications and present case studies of preliminary solutions. We discuss select strategies and metrics relevant for measuring memorization in LLMs that can, in turn, be used to measure privacy-risk in these models. We then discuss privacy-preservation techniques that can be applied at different points of the LLM training life-cycle; including our work on an algorithm for fine-tuning LLMs with improved privacy. In addition, we discuss our work on privacy-preserving solutions that can be applied to LLMs during inference and are feasible for use at run time.},
  isbn = {978-1-4503-9407-9}
}

@misc{pfauLetThinkDot2024,
  title = {Let's {{Think Dot}} by {{Dot}}: {{Hidden Computation}} in {{Transformer Language Models}}},
  shorttitle = {Let's {{Think Dot}} by {{Dot}}},
  author = {Pfau, Jacob and Merrill, William and Bowman, Samuel R.},
  year = {2024},
  month = apr,
  number = {arXiv:2404.15758},
  eprint = {2404.15758},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2404.15758},
  urldate = {2024-04-27},
  abstract = {Chain-of-thought responses from language models improve performance across most benchmarks. However, it remains unclear to what extent these performance gains can be attributed to human-like task decomposition or simply the greater computation that additional tokens allow. We show that transformers can use meaningless filler tokens (e.g., '......') in place of a chain of thought to solve two hard algorithmic tasks they could not solve when responding without intermediate tokens. However, we find empirically that learning to use filler tokens is difficult and requires specific, dense supervision to converge. We also provide a theoretical characterization of the class of problems where filler tokens are useful in terms of the quantifier depth of a first-order formula. For problems satisfying this characterization, chain-of-thought tokens need not provide information about the intermediate computational steps involved in multi-token computations. In summary, our results show that additional tokens can provide computational benefits independent of token choice. The fact that intermediate tokens can act as filler tokens raises concerns about large language models engaging in unauditable, hidden computations that are increasingly detached from the observed chain-of-thought tokens.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,I.2.6}
}

@misc{plaatAgenticLargeLanguage2025,
  title = {Agentic {{Large Language Models}}, a Survey},
  author = {Plaat, Aske and van Duijn, Max and van Stein, Niki and Preuss, Mike and van der Putten, Peter and Batenburg, Kees Joost},
  year = {2025},
  month = apr,
  number = {arXiv:2503.23037},
  eprint = {2503.23037},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.23037},
  urldate = {2025-04-21},
  abstract = {There is great interest in agentic LLMs, large language models that act as agents. We review the growing body of work in this area and provide a research agenda. Agentic LLMs are LLMs that (1) reason, (2) act, and (3) interact. We organize the literature according to these three categories. The research in the first category focuses on reasoning, reflection, and retrieval, aiming to improve decision making; the second category focuses on action models, robots, and tools, aiming for agents that act as useful assistants; the third category focuses on multi-agent systems, aiming for collaborative task solving and simulating interaction to study emergent social behavior. We find that works mutually benefit from results in other categories: retrieval enables tool use, reflection improves multi-agent collaboration, and reasoning benefits all categories. We discuss applications of agentic LLMs and provide an agenda for further research. Important applications are in medical diagnosis, logistics and financial market analysis. Meanwhile, self-reflective agents playing roles and interacting with one another augment the process of scientific research itself. Further, agentic LLMs may provide a solution for the problem of LLMs running out of training data: inference-time behavior generates new training states, such that LLMs can keep learning without needing ever larger datasets. We note that there is risk associated with LLM assistants taking action in the real world, while agentic LLMs are also likely to benefit society.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{PlanTrackPlan,
  title = {Plan \& Track - {{Plan}} Overview - Chase.Com},
  url = {https://secure.chase.com/web/auth/dashboard#/dashboard/planTrack/summaryDashboard/index},
  urldate = {2025-02-17}
}

@misc{plantYouAreWhat2022,
  title = {You {{Are What You Write}}: {{Preserving Privacy}} in the {{Era}} of {{Large Language Models}}},
  shorttitle = {You {{Are What You Write}}},
  author = {Plant, Richard and Giuffrida, Valerio and Gkatzia, Dimitra},
  year = {2022},
  month = apr,
  number = {arXiv:2204.09391},
  eprint = {2204.09391},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2204.09391},
  urldate = {2024-02-13},
  abstract = {Large scale adoption of large language models has introduced a new era of convenient knowledge transfer for a slew of natural language processing tasks. However, these models also run the risk of undermining user trust by exposing unwanted information about the data subjects, which may be extracted by a malicious party, e.g. through adversarial attacks. We present an empirical investigation into the extent of the personal information encoded into pre-trained representations by a range of popular models, and we show a positive correlation between the complexity of a model, the amount of data used in pre-training, and data leakage. In this paper, we present the first wide coverage evaluation and comparison of some of the most popular privacy-preserving algorithms, on a large, multi-lingual dataset on sentiment analysis annotated with demographic information (location, age and gender). The results show since larger and more complex models are more prone to leaking private information, use of privacy-preserving methods is highly desirable. We also find that highly privacy-preserving technologies like differential privacy (DP) can have serious model utility effects, which can be ameliorated using hybrid or metric-DP techniques.},
  archiveprefix = {arXiv}
}

@misc{poesiaSynchromeshReliableCode2022,
  title = {Synchromesh: {{Reliable}} Code Generation from Pre-Trained Language Models},
  shorttitle = {Synchromesh},
  author = {Poesia, Gabriel and Polozov, Oleksandr and Le, Vu and Tiwari, Ashish and Soares, Gustavo and Meek, Christopher and Gulwani, Sumit},
  year = {2022},
  month = jan,
  number = {arXiv:2201.11227},
  eprint = {2201.11227},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2201.11227},
  urldate = {2024-09-24},
  abstract = {Large pre-trained language models have been used to generate code, providing a flexible interface for synthesizing programs from natural language specifications. However, they often violate syntactic and semantic rules of their output language, limiting their practical usability. In this paper, we propose SYNCHROMESH: a framework for substantially improving the reliability of pre-trained models for code generation. SYNCHROMESH comprises two components. First, it retrieves few-shot examples from a training bank using Target Similarity Tuning (TST), a novel method for semantic example selection. TST learns to recognize utterances that describe similar target programs despite differences in surface natural language features. Then, SYNCHROMESH feeds the examples to a pre-trained language model and samples programs using Constrained Semantic Decoding (CSD): a general framework for constraining the output to a set of valid programs in the target language. CSD leverages constraints on partial outputs to sample complete correct programs, and needs neither re-training nor fine-tuning of the language model. We evaluate our methods by synthesizing code from natural language descriptions using GPT-3 and Codex in three real-world languages: SQL queries, Vega-Lite visualizations and SMCalFlow programs. These domains showcase rich constraints that CSD is able to enforce, including syntax, scope, typing rules, and contextual logic. We observe substantial complementary gains from CSD and TST in prediction accuracy and in effectively preventing run-time errors.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Programming Languages}
}

@article{polsonCognitiveWalkthroughsMethod1992,
  title = {Cognitive Walkthroughs: A Method for Theory-Based Evaluation of User Interfaces},
  shorttitle = {Cognitive Walkthroughs},
  author = {Polson, Peter G. and Lewis, Clayton and Rieman, John and Wharton, Cathleen},
  year = {1992},
  month = may,
  journal = {International Journal of Man-Machine Studies},
  volume = {36},
  number = {5},
  pages = {741--773},
  issn = {0020-7373},
  doi = {10.1016/0020-7373(92)90039-N},
  urldate = {2025-04-09},
  abstract = {This paper presents a new methodology for performing theory-based evaluations of user interface designs early in the design cycle. The methodology is an adaptation of the design walkthrough techniques that have been used for many years in the software engineering community. Traditional walkthroughs involve hand simulation of sections of code to ensure that they implement specified functionality. The method we present involves hand simulation of the cognitive activities of a user, to ensure that the user can easily learn to perform tasks that the system is intended to support. The cognitive walkthrough methodology, described in detail, is based on a theory of learning by exploration presented in this paper. There is a summary of preliminary results of effectiveness and comparisons with other design methods.}
}

@article{polychronisWorkingTogetherUndermine2023,
  title = {Working {{Together}} (to {{Undermine Democratic Institutions}}): {{Challenging}} the {{Social Bot Paradigm}} in {{SSIO Research}}},
  shorttitle = {Working {{Together}} (to {{Undermine Democratic Institutions}})},
  author = {Polychronis, Cole and Kogan, Marina},
  year = {2023},
  month = oct,
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  volume = {7},
  number = {CSCW2},
  pages = {242:1--242:30},
  doi = {10.1145/3610033},
  urldate = {2024-05-03},
  abstract = {Unlike most other forms of coordinated, inauthentic behavior occurring online, the goals of state-sponsored information operations, or SSIOs, are often complex and multifaceted. These goals range from flooding conversations with a certain narrative, to increasing the public's engagement with news sources of questionable quality, to stoking tensions between ideologically opposed groups to weaken public trust. The prevailing theoretical framework for understanding SSIOs is to treat them as a social botnet: a behaviorally homogeneous cluster of coordinated activity. However, the social bot framework is both at odds with some of the behaviors observed in early SSIOs and more broadly with the wide swathe of goals these operations set out to accomplish. To examine the fit of the social bot framework in the SSIO context, we develop a novel bag-of-words based method for clustering and describing user activity traces. Applying this method to a comprehensive repository of SSIOs conducted on Twitter over the last decade, we find that SSIOs violate both the core assumption of the social bot framework, and how it is operationalized in practical work. Instead, we find that SSIOs exhibit a clear division of labor and propose cooperative work with social roles as a more effective theoretical framework for understanding SSIOs. Through applying this framework, we find that the roles that SSIO agents take on have become more stable and simple over time, which holds substantial implications for developing methods for detection of these operations in the wild.},
  keywords = {cooperative work,disinformation,information operations,misinformation,sequence analysis,social bots,social roles}
}

@misc{PytorchTorchtune5eb04cd934ad84efff61e5dbf7a054fd7af184ec,
  title = {Pytorch/Torchtune at 5eb04cd934ad84efff61e5dbf7a054fd7af184ec},
  journal = {GitHub},
  url = {https://github.com/pytorch/torchtune},
  urldate = {2024-12-04},
  abstract = {PyTorch native finetuning library. Contribute to pytorch/torchtune development by creating an account on GitHub.},
  langid = {english}
}

@misc{qianChatDevCommunicativeAgents2024,
  title = {{{ChatDev}}: {{Communicative Agents}} for {{Software Development}}},
  shorttitle = {{{ChatDev}}},
  author = {Qian, Chen and Liu, Wei and Liu, Hongzhang and Chen, Nuo and Dang, Yufan and Li, Jiahao and Yang, Cheng and Chen, Weize and Su, Yusheng and Cong, Xin and Xu, Juyuan and Li, Dahai and Liu, Zhiyuan and Sun, Maosong},
  year = {2024},
  month = jun,
  number = {arXiv:2307.07924},
  eprint = {2307.07924},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.07924},
  urldate = {2024-07-05},
  abstract = {Software development is a complex task that necessitates cooperation among multiple members with diverse skills. Numerous studies used deep learning to improve specific phases in a waterfall model, such as design, coding, and testing. However, the deep learning model in each phase requires unique designs, leading to technical inconsistencies across various phases, which results in a fragmented and ineffective development process. In this paper, we introduce ChatDev, a chat-powered software development framework in which specialized agents driven by large language models (LLMs) are guided in what to communicate (via chat chain) and how to communicate (via communicative dehallucination). These agents actively contribute to the design, coding, and testing phases through unified language-based communication, with solutions derived from their multi-turn dialogues. We found their utilization of natural language is advantageous for system design, and communicating in programming language proves helpful in debugging. This paradigm demonstrates how linguistic communication facilitates multi-agent collaboration, establishing language as a unifying bridge for autonomous task-solving among LLM agents. The code and data are available at https://github.com/OpenBMB/ChatDev.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Multiagent Systems,Computer Science - Software Engineering}
}

@inproceedings{qianITunesPapersRedefining2019,
  title = {Beyond {{iTunes}} for {{Papers}}: {{Redefining}} the {{Unit}} of {{Interaction}} in {{Literature Review Tools}}},
  shorttitle = {Beyond {{iTunes}} for {{Papers}}},
  booktitle = {Companion {{Publication}} of the 2019 {{Conference}} on {{Computer Supported Cooperative Work}} and {{Social Computing}}},
  author = {Qian, Xin and Erhart, Matt J. and Kittur, Aniket and Lutters, Wayne G. and Chan, Joel},
  year = {2019},
  month = nov,
  series = {{{CSCW}} '19 {{Companion}}},
  pages = {341--346},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3311957.3359455},
  urldate = {2024-02-04},
  abstract = {Conducting an effective literature review is an essential step in all scientific work. However, the process is difficult, particularly for interdisciplinary work. Here, we articulate a key avenue for improvement for literature review tools: supporting the appropriate unit of interaction, which we argue is a "grounded claim", a concise statement linked to key contextual details such as evidence. However, there are significant cognitive and interaction costs in creating them. We share insights from our development of a prototype literature review tool, the Knowledge Compressor, that aims to lower these costs.},
  isbn = {978-1-4503-6692-2}
}

@misc{qianTellMeMore2024,
  title = {Tell {{Me More}}! {{Towards Implicit User Intention Understanding}} of {{Language Model Driven Agents}}},
  author = {Qian, Cheng and He, Bingxiang and Zhuang, Zhong and Deng, Jia and Qin, Yujia and Cong, Xin and Zhang, Zhong and Zhou, Jie and Lin, Yankai and Liu, Zhiyuan and Sun, Maosong},
  year = {2024},
  month = feb,
  number = {arXiv:2402.09205},
  eprint = {2402.09205},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.09205},
  urldate = {2024-08-08},
  abstract = {Current language model-driven agents often lack mechanisms for effective user participation, which is crucial given the vagueness commonly found in user instructions. Although adept at devising strategies and performing tasks, these agents struggle with seeking clarification and grasping precise user intentions. To bridge this gap, we introduce Intention-in-Interaction (IN3), a novel benchmark designed to inspect users' implicit intentions through explicit queries. Next, we propose the incorporation of model experts as the upstream in agent designs to enhance user-agent interaction. Employing IN3, we empirically train Mistral-Interact, a powerful model that proactively assesses task vagueness, inquires user intentions, and refines them into actionable goals before starting downstream agent task execution. Integrating it into the XAgent framework, we comprehensively evaluate the enhanced agent system regarding user instruction understanding and execution, revealing that our approach notably excels at identifying vague user tasks, recovering and summarizing critical missing information, setting precise and necessary agent execution goals, and minimizing redundant tool usage, thus boosting overall efficiency. All the data and codes are released.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction}
}

@misc{qinToolLearningFoundation2023,
  title = {Tool {{Learning}} with {{Foundation Models}}},
  author = {Qin, Yujia and Hu, Shengding and Lin, Yankai and Chen, Weize and Ding, Ning and Cui, Ganqu and Zeng, Zheni and Huang, Yufei and Xiao, Chaojun and Han, Chi and Fung, Yi Ren and Su, Yusheng and Wang, Huadong and Qian, Cheng and Tian, Runchu and Zhu, Kunlun and Liang, Shihao and Shen, Xingyu and Xu, Bokai and Zhang, Zhen and Ye, Yining and Li, Bowen and Tang, Ziwei and Yi, Jing and Zhu, Yuzhang and Dai, Zhenning and Yan, Lan and Cong, Xin and Lu, Yaxi and Zhao, Weilin and Huang, Yuxiang and Yan, Junxi and Han, Xu and Sun, Xian and Li, Dahai and Phang, Jason and Yang, Cheng and Wu, Tongshuang and Ji, Heng and Liu, Zhiyuan and Sun, Maosong},
  year = {2023},
  month = jun,
  number = {arXiv:2304.08354},
  eprint = {2304.08354},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.08354},
  urldate = {2024-06-11},
  abstract = {Humans possess an extraordinary ability to create and utilize tools, allowing them to overcome physical limitations and explore new frontiers. With the advent of foundation models, AI systems have the potential to be equally adept in tool use as humans. This paradigm, i.e., tool learning with foundation models, combines the strengths of specialized tools and foundation models to achieve enhanced accuracy, efficiency, and automation in problem-solving. Despite its immense potential, there is still a lack of a comprehensive understanding of key challenges, opportunities, and future endeavors in this field. To this end, we present a systematic investigation of tool learning in this paper. We first introduce the background of tool learning, including its cognitive origins, the paradigm shift of foundation models, and the complementary roles of tools and models. Then we recapitulate existing tool learning research into tool-augmented and tool-oriented learning. We formulate a general tool learning framework: starting from understanding the user instruction, models should learn to decompose a complex task into several subtasks, dynamically adjust their plan through reasoning, and effectively conquer each sub-task by selecting appropriate tools. We also discuss how to train models for improved tool-use capabilities and facilitate the generalization in tool learning. Considering the lack of a systematic tool learning evaluation in prior works, we experiment with 18 representative tools and show the potential of current foundation models in skillfully utilizing tools. Finally, we discuss several open problems that require further investigation for tool learning. Overall, we hope this paper could inspire future research in integrating tools with foundation models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@inproceedings{qiuRethinkingItemOrder2019,
  title = {Rethinking the {{Item Order}} in {{Session-based Recommendation}} with {{Graph Neural Networks}}},
  booktitle = {Proceedings of the 28th {{ACM International Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Qiu, Ruihong and Li, Jingjing and Huang, Zi and YIn, Hongzhi},
  year = {2019},
  month = nov,
  series = {{{CIKM}} '19},
  pages = {579--588},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3357384.3358010},
  urldate = {2024-09-24},
  abstract = {Predicting a user's preference in a short anonymous interaction session instead of long-term history is a challenging problem in the real-life session-based recommendation, e.g., e-commerce and media stream. Recent research of the session-based recommender system mainly focuses on sequential patterns by utilizing the attention mechanism, which is straightforward for the session's natural sequence sorted by time. However, the user's preference is much more complicated than a solely consecutive time pattern in the transition of item choices. In this paper, therefore, we study the item transition pattern by constructing a session graph and propose a novel model which collaboratively considers the sequence order and the latent order in the session graph for a session-based recommender system. We formulate the next item recommendation within the session as a graph classification problem. Specifically, we propose a weighted attention graph layer and a Readout function to learn embeddings of items and sessions for the next item recommendation. Extensive experiments have been conducted on two benchmark E-commerce datasets, Yoochoose and Diginetica, and the experimental results show that our model outperforms other state-of-the-art methods.},
  isbn = {978-1-4503-6976-3}
}

@article{QualityQuantitySynthetic2023,
  title = {Quality {$>$} {{Quantity}}: {{Synthetic Corpora}} from {{Foundation Models}} for {{Closed-Domain Extractive Question Answering}}},
  shorttitle = {Quality {$>$} {{Quantity}}},
  year = {2023},
  month = jun,
  url = {https://openreview.net/forum?id=OxoP1qFotz&referrer=%5BReviewer%20Console%5D(%2Fgroup%3Fid%3DEMNLP%2F2023%2FConference%2FReviewers%23assigned-papers)},
  urldate = {2023-08-12},
  abstract = {Domain adaptation, the process of training a model in one domain and applying it to another, has been extensively explored in machine learning. While training a domain-specific foundation model (FM) from scratch is an option, recent methods have focused on adapting pre-trained FMs for domain-specific tasks. However, our experiments reveal that either approach does not consistently achieve state-of-the-art (SOTA) results in the target domain. In this work, we study extractive question answering within closed domains and introduce the concept of targeted pre-training. This involves determining and generating relevant data to further pre-train our models, as opposed to the conventional philosophy of utilizing domain-specific FMs trained on a wide range of data. Our proposed framework uses Galactica to generate synthetic, ``targeted'' corpora that align with specific writing styles and topics, such as research papers and radiology reports. This process can be viewed as a form of knowledge distillation. We apply our method to two biomedical extractive question answering datasets, COVID-QA and RadQA, achieving a new benchmark on the former and demonstrating overall improvements on the latter. Code available upon publication.},
  langid = {english}
}

@article{radfordLanguageModelsAre,
  title = {Language {{Models}} Are {{Unsupervised Multitask Learners}}},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  langid = {english},
  keywords = {No DOI found}
}

@article{rajpurkarKnowWhatYou2018a,
  title = {Know {{What You Don}}'t {{Know}}: {{Unanswerable Questions}} for {{SQuAD}}},
  shorttitle = {Know {{What You Don}}'t {{Know}}},
  author = {Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
  year = {2018},
  month = jun,
  journal = {arXiv:1806.03822 [cs]},
  eprint = {1806.03822},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1806.03822},
  urldate = {2021-12-25},
  abstract = {Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuAD 2.0, the latest version of the Stanford Question Answering Dataset (SQuAD). SQuAD 2.0 combines existing SQuAD data with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD 2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuAD 2.0 is a challenging natural language understanding task for existing models: a strong neural system that gets 86\% F1 on SQuAD 1.1 achieves only 66\% F1 on SQuAD 2.0.},
  archiveprefix = {arXiv},
  langid = {english},
  annotation = {00000}
}

@inproceedings{rajpurkarSQuAD1000002016,
  title = {{{SQuAD}}: 100,000+ {{Questions}} for {{Machine Comprehension}} of {{Text}}},
  shorttitle = {{{SQuAD}}},
  booktitle = {Proceedings of the 2016 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  year = {2016},
  pages = {2383--2392},
  publisher = {Association for Computational Linguistics},
  address = {Austin, Texas},
  doi = {10.18653/v1/D16-1264},
  urldate = {2022-01-17},
  annotation = {03991}
}

@article{rajpurkarSQuAD1000002016a,
  title = {{{SQuAD}}: 100,000+ {{Questions}} for {{Machine Comprehension}} of {{Text}}},
  shorttitle = {{{SQuAD}}},
  author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  year = {2016},
  month = oct,
  journal = {arXiv:1606.05250 [cs]},
  eprint = {1606.05250},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1606.05250},
  urldate = {2021-12-25},
  abstract = {We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0\%, a significant improvement over a simple baseline (20\%). However, human performance (86.8\%) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at https://stanford-qa.com},
  archiveprefix = {arXiv},
  langid = {english},
  annotation = {00000}
}

@misc{rajputRecommenderSystemsGenerative2023,
  title = {Recommender {{Systems}} with {{Generative Retrieval}}},
  author = {Rajput, Shashank and Mehta, Nikhil and Singh, Anima and Keshavan, Raghunandan H. and Vu, Trung and Heldt, Lukasz and Hong, Lichan and Tay, Yi and Tran, Vinh Q. and Samost, Jonah and Kula, Maciej and Chi, Ed H. and Sathiamoorthy, Maheswaran},
  year = {2023},
  month = nov,
  number = {arXiv:2305.05065},
  eprint = {2305.05065},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.05065},
  urldate = {2024-09-05},
  abstract = {Modern recommender systems perform large-scale retrieval by first embedding queries and item candidates in the same unified space, followed by approximate nearest neighbor search to select top candidates given a query embedding. In this paper, we propose a novel generative retrieval approach, where the retrieval model autoregressively decodes the identifiers of the target candidates. To that end, we create semantically meaningful tuple of codewords to serve as a Semantic ID for each item. Given Semantic IDs for items in a user session, a Transformer-based sequence-to-sequence model is trained to predict the Semantic ID of the next item that the user will interact with. To the best of our knowledge, this is the first Semantic ID-based generative model for recommendation tasks. We show that recommender systems trained with the proposed paradigm significantly outperform the current SOTA models on various datasets. In addition, we show that incorporating Semantic IDs into the sequence-to-sequence model enhances its ability to generalize, as evidenced by the improved retrieval performance observed for items with no prior interaction history.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning}
}

@misc{ramrakhyaGroundingMultimodalLLMs2025,
  title = {Grounding {{Multimodal LLMs}} to {{Embodied Agents}} That {{Ask}} for {{Help}} with {{Reinforcement Learning}}},
  author = {Ramrakhya, Ram and Chang, Matthew and Puig, Xavier and Desai, Ruta and Kira, Zsolt and Mottaghi, Roozbeh},
  year = {2025},
  month = apr,
  number = {arXiv:2504.00907},
  eprint = {2504.00907},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.00907},
  urldate = {2025-06-04},
  abstract = {Embodied agents operating in real-world environments must interpret ambiguous and under-specified human instructions. A capable household robot should recognize ambiguity and ask relevant clarification questions to infer the user intent accurately, leading to more effective task execution. To study this problem, we introduce the Ask-to-Act task, where an embodied agent must fetch a specific object instance given an ambiguous instruction in a home environment. The agent must strategically ask minimal, yet relevant, clarification questions to resolve ambiguity while navigating under partial observability. To solve this problem, we propose a novel approach that fine-tunes multimodal large language models (MLLMs) as vision-language-action (VLA) policies using online reinforcement learning (RL) with LLM-generated rewards. Our method eliminates the need for large-scale human demonstrations or manually engineered rewards for training such agents. We benchmark against strong zero-shot baselines, including GPT-4o, and supervised fine-tuned MLLMs, on our task. Our results demonstrate that our RL-finetuned MLLM outperforms all baselines by a significant margin (\$19.1\$-\$40.3{\textbackslash}\%\$), generalizing well to novel scenes and tasks. To the best of our knowledge, this is the first demonstration of adapting MLLMs as VLA agents that can act and ask for help using LLM-generated rewards with online RL.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence}
}

@misc{raposoMixtureofDepthsDynamicallyAllocating2024,
  title = {Mixture-of-{{Depths}}: {{Dynamically}} Allocating Compute in Transformer-Based Language Models},
  shorttitle = {Mixture-of-{{Depths}}},
  author = {Raposo, David and Ritter, Sam and Richards, Blake and Lillicrap, Timothy and Humphreys, Peter Conway and Santoro, Adam},
  year = {2024},
  month = apr,
  number = {arXiv:2404.02258},
  eprint = {2404.02258},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2404.02258},
  urldate = {2024-04-04},
  abstract = {Transformer-based language models spread FLOPs uniformly across input sequences. In this work we demonstrate that transformers can instead learn to dynamically allocate FLOPs (or compute) to specific positions in a sequence, optimising the allocation along the sequence for different layers across the model depth. Our method enforces a total compute budget by capping the number of tokens (\$k\$) that can participate in the self-attention and MLP computations at a given layer. The tokens to be processed are determined by the network using a top-\$k\$ routing mechanism. Since \$k\$ is defined a priori, this simple procedure uses a static computation graph with known tensor sizes, unlike other conditional computation techniques. Nevertheless, since the identities of the \$k\$ tokens are fluid, this method can expend FLOPs non-uniformly across the time and model depth dimensions. Thus, compute expenditure is entirely predictable in sum total, but dynamic and context-sensitive at the token-level. Not only do models trained in this way learn to dynamically allocate compute, they do so efficiently. These models match baseline performance for equivalent FLOPS and wall-clock times to train, but require a fraction of the FLOPs per forward pass, and can be upwards of 50{\textbackslash}\% faster to step during post-training sampling.},
  archiveprefix = {arXiv}
}

@article{rasmussenChallengeDataAnnotation2022,
  title = {The {{Challenge}} of {{Data Annotation}} in {{Deep Learning}}---{{A Case Study}} on {{Whole Plant Corn Silage}}},
  author = {Rasmussen, Christoffer B{\o}gelund and Kirk, Kristian and Moeslund, Thomas B.},
  year = {2022},
  month = jan,
  journal = {Sensors},
  volume = {22},
  number = {4},
  pages = {1596},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1424-8220},
  doi = {10.3390/s22041596},
  urldate = {2024-02-13},
  abstract = {Recent advances in computer vision are primarily driven by the usage of deep learning, which is known to require large amounts of data, and creating datasets for this purpose is not a trivial task. Larger benchmark datasets often have detailed processes with multiple stages and users with different roles during annotation. However, this can be difficult to implement in smaller projects where resources can be limited. Therefore, in this work we present our processes for creating an image dataset for kernel fragmentation and stover overlengths in Whole Plant Corn Silage. This includes the guidelines for annotating object instances in respective classes and statistics of gathered annotations. Given the challenging image conditions, where objects are present in large amounts of occlusion and clutter, the datasets appear appropriate for training models. However, we experience annotator inconsistency, which can hamper evaluation. Based on this we argue the importance of having an evaluation form independent of the manual annotation where we evaluate our models with physically based sieving metrics. Additionally, instead of the traditional time-consuming manual annotation approach, we evaluate Semi-Supervised Learning as an alternative, showing competitive results while requiring fewer annotations. Specifically, given a relatively large supervised set of around 1400 images we can improve the Average Precision by a number of percentage points. Additionally, we show a significantly large improvement when using an extremely small set of just over 100 images, with over 3{\texttimes} in Average Precision and up to 20 percentage points when estimating the quality.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english}
}

@article{ratnerSnorkelRapidTraining2017,
  title = {Snorkel: {{Rapid Training Data Creation}} with {{Weak Supervision}}},
  shorttitle = {Snorkel},
  author = {Ratner, Alexander and Bach, Stephen H. and Ehrenberg, Henry and Fries, Jason and Wu, Sen and R{\'e}, Christopher},
  year = {2017},
  month = nov,
  journal = {Proceedings of the VLDB Endowment. International Conference on Very Large Data Bases},
  volume = {11},
  number = {3},
  pages = {269--282},
  issn = {2150-8097},
  doi = {10.14778/3157794.3157797},
  urldate = {2023-08-02},
  abstract = {Labeling training data is increasingly the largest bottleneck in deploying machine learning systems. We present Snorkel, a first-of-its-kind system that enables users to train state-of- the-art models without hand labeling any training data. Instead, users write labeling functions that express arbitrary heuristics, which can have unknown accuracies and correlations. Snorkel denoises their outputs without access to ground truth by incorporating the first end-to-end implementation of our recently proposed machine learning paradigm, data programming. We present a flexible interface layer for writing labeling functions based on our experience over the past year collaborating with companies, agencies, and research labs. In a user study, subject matter experts build models 2.8{\texttimes} faster and increase predictive performance an average 45.5\% versus seven hours of hand labeling. We study the modeling tradeoffs in this new setting and propose an optimizer for automating tradeoff decisions that gives up to 1.8{\texttimes} speedup per pipeline execution. In two collaborations, with the U.S. Department of Veterans Affairs and the U.S. Food and Drug Administration, and on four open-source text and image data sets representative of other deployments, Snorkel provides 132\% average improvements to predictive performance over prior heuristic approaches and comes within an average 3.60\% of the predictive performance of large hand-curated training sets.},
  pmcid = {PMC5951191},
  pmid = {29770249}
}

@inproceedings{reimersSentenceBERTSentenceEmbeddings2019,
  title = {Sentence-{{BERT}}: {{Sentence Embeddings}} Using {{Siamese BERT-Networks}}},
  shorttitle = {Sentence-{{BERT}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Reimers, Nils and Gurevych, Iryna},
  year = {2019},
  month = nov,
  pages = {3982--3992},
  publisher = {Association for Computational Linguistics},
  address = {Hong Kong, China},
  doi = {10.18653/v1/D19-1410},
  urldate = {2023-06-24},
  abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textbackslash}textasciitilde65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.}
}

@incollection{renAgentBasedModeling2014,
  title = {Agent {{Based Modeling}} to {{Inform}} the {{Design}} of {{Multiuser Systems}}},
  booktitle = {Ways of {{Knowing}} in {{HCI}}},
  author = {Ren, Yuqing and Kraut, Robert E.},
  editor = {Olson, Judith S. and Kellogg, Wendy A.},
  year = {2014},
  pages = {395--419},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-1-4939-0378-8\_16},
  urldate = {2024-09-12},
  abstract = {Agent Based Modeling studies group activity by simulating the individuals in it and allowing group-level phenomena to emerge. It can be used to integrate theories to inform designs of technology for groups. Researchers use theories as the basis of the rules of how individuals behave (e.g., what motivates users to contribute to an online community). They can run virtual experiments by changing parameters of the model (e.g., the topical focus in an online community) to see what collective behaviors emerge.},
  isbn = {978-1-4939-0378-8},
  langid = {english},
  keywords = {Online Community,Social Science Theory,Topical Breadth,Transactive Memory,Virtual Experiment}
}

@inproceedings{renBASESLargescaleWeb2024,
  title = {{{BASES}}: {{Large-scale Web Search User Simulation}} with {{Large Language Model}} Based {{Agents}}},
  shorttitle = {{{BASES}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2024},
  author = {Ren, Ruiyang and Qiu, Peng and Qu, Yingqi and Liu, Jing and Zhao, Xin and Wu, Hua and Wen, Ji-Rong and Wang, Haifeng},
  editor = {{Al-Onaizan}, Yaser and Bansal, Mohit and Chen, Yun-Nung},
  year = {2024},
  month = nov,
  pages = {902--917},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.findings-emnlp.50},
  urldate = {2024-12-11},
  abstract = {Due to the excellent capacities of large language models (LLMs), it becomes feasible to develop LLM-based agents for reliable user simulation. Considering the scarcity and limit (e.g., privacy issues) of real user data, in this paper, we conduct large-scale user simulations for the web search scenario to improve the analysis and modeling of user search behavior. Specially, we propose BASES, a novel user simulation framework with LLM-based agents, designed to facilitate comprehensive simulations of web search user behaviors. Our simulation framework can generate unique user profiles at scale, which subsequently leads to diverse search behaviors. To demonstrate the effectiveness of BASES, we conduct evaluation experiments based on two human benchmarks in both Chinese and English, demonstrating that BASES can effectively simulate large-scale human-like search behaviors. To further accommodate the research on web search, we develop WARRIORS, a new large-scale dataset encompassing web search user behaviors, including both Chinese and English versions, which can greatly bolster research in the field of information retrieval.}
}

@misc{renSurveyDeepActive2021,
  title = {A {{Survey}} of {{Deep Active Learning}}},
  author = {Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Huang, Po-Yao and Li, Zhihui and Gupta, Brij B. and Chen, Xiaojiang and Wang, Xin},
  year = {2021},
  month = dec,
  number = {arXiv:2009.00236},
  eprint = {2009.00236},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2009.00236},
  urldate = {2025-02-18},
  abstract = {Active learning (AL) attempts to maximize the performance gain of the model by marking the fewest samples. Deep learning (DL) is greedy for data and requires a large amount of data supply to optimize massive parameters, so that the model learns how to extract high-quality features. In recent years, due to the rapid development of internet technology, we are in an era of information torrents and we have massive amounts of data. In this way, DL has aroused strong interest of researchers and has been rapidly developed. Compared with DL, researchers have relatively low interest in AL. This is mainly because before the rise of DL, traditional machine learning requires relatively few labeled samples. Therefore, early AL is difficult to reflect the value it deserves. Although DL has made breakthroughs in various fields, most of this success is due to the publicity of the large number of existing annotation datasets. However, the acquisition of a large number of high-quality annotated datasets consumes a lot of manpower, which is not allowed in some fields that require high expertise, especially in the fields of speech recognition, information extraction, medical images, etc. Therefore, AL has gradually received due attention. A natural idea is whether AL can be used to reduce the cost of sample annotations, while retaining the powerful learning capabilities of DL. Therefore, deep active learning (DAL) has emerged. Although the related research has been quite abundant, it lacks a comprehensive survey of DAL. This article is to fill this gap, we provide a formal classification method for the existing work, and a comprehensive and systematic overview. In addition, we also analyzed and summarized the development of DAL from the perspective of application. Finally, we discussed the confusion and problems in DAL, and gave some possible development directions for DAL.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{retzlaffHumanintheLoopReinforcementLearning2024,
  title = {Human-in-the-{{Loop Reinforcement Learning}}: {{A Survey}} and {{Position}} on {{Requirements}}, {{Challenges}}, and {{Opportunities}}},
  shorttitle = {Human-in-the-{{Loop Reinforcement Learning}}},
  author = {Retzlaff, Carl Orge and Das, Srijita and Wayllace, Christabel and Mousavi, Payam and Afshari, Mohammad and Yang, Tianpei and Saranti, Anna and Angerschmid, Alessa and Taylor, Matthew E. and Holzinger, Andreas},
  year = {2024},
  month = jan,
  journal = {Journal of Artificial Intelligence Research},
  volume = {79},
  pages = {359--415},
  issn = {1076-9757},
  doi = {10.1613/jair.1.15348},
  urldate = {2025-06-04},
  abstract = {Artificial intelligence (AI) and especially reinforcement learning (RL) have the potential to enable agents to learn and perform tasks autonomously with superhuman performance. However, we consider RL as fundamentally a Human-in-the-Loop (HITL) paradigm, even when an agent eventually performs its task autonomously.~ In cases where the reward function is challenging or impossible to define, HITL approaches are considered particularly advantageous. The application of Reinforcement Learning from Human Feedback (RLHF) in systems such as ChatGPT demonstrates the effectiveness of optimizing for user experience and integrating their feedback into the training loop. In HITL RL, human input is integrated during the agent's learning process, allowing iterative updates and fine-tuning based on human feedback, thus enhancing the agent's performance. Since the human is an essential part of this process, we argue that human-centric approaches are the key to successful RL, a fact that has not been adequately considered in the existing literature. This paper aims to inform readers about current explainability methods in HITL RL. It also shows how the application of explainable AI (xAI) and specific improvements to existing explainability approaches can enable a better human-agent interaction in HITL RL for all types of users, whether for lay people, domain experts, or machine learning specialists. Accounting for the workflow in HITL RL and based on software and machine learning methodologies, this article identifies four phases for human involvement for creating HITL RL systems: (1) Agent Development, (2) Agent Learning, (3) Agent Evaluation, and (4) Agent Deployment. We highlight human involvement, explanation requirements, new challenges, and goals for each phase. We furthermore identify low-risk, high-return opportunities for explainability research in HITL RL and present long-term research goals to advance the field. Finally, we propose a vision of human-robot collaboration that allows both parties to reach their full potential and cooperate effectively.},
  copyright = {Copyright (c) 2024 Journal of Artificial Intelligence Research},
  langid = {english},
  keywords = {Explainable AI,human computer interaction,human-in-the-loop,Reinforcement Learning}
}

@inproceedings{richardsonMCTestChallengeDataset2013,
  title = {{{MCTest}}: {{A Challenge Dataset}} for the {{Open-Domain Machine Comprehension}} of {{Text}}},
  shorttitle = {{{MCTest}}},
  booktitle = {Proceedings of the 2013 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Richardson, Matthew and Burges, Christopher J.C. and Renshaw, Erin},
  year = {2013},
  month = oct,
  pages = {193--203},
  publisher = {Association for Computational Linguistics},
  address = {Seattle, Washington, USA},
  url = {https://aclanthology.org/D13-1020},
  urldate = {2021-12-25},
  annotation = {00610}
}

@misc{RootAWSOrganizations,
  title = {Root {\textbar} {{AWS Organizations}} {\textbar} {{Global}}},
  url = {https://us-east-1.console.aws.amazon.com/organizations/v2/home/root},
  urldate = {2025-02-28}
}

@inproceedings{rouzegarEnhancingTextClassification2024a,
  title = {Enhancing {{Text Classification}} through {{LLM-Driven Active Learning}} and {{Human Annotation}}},
  booktitle = {Proceedings of {{The}} 18th {{Linguistic Annotation Workshop}} ({{LAW-XVIII}})},
  author = {Rouzegar, Hamidreza and Makrehchi, Masoud},
  editor = {Henning, Sophie and Stede, Manfred},
  year = {2024},
  month = mar,
  pages = {98--111},
  publisher = {Association for Computational Linguistics},
  address = {St. Julians, Malta},
  url = {https://aclanthology.org/2024.law-1.10},
  urldate = {2024-04-09},
  abstract = {In the context of text classification, the financial burden of annotation exercises for creating training data is a critical issue. Active learning techniques, particularly those rooted in uncertainty sampling, offer a cost-effective solution by pinpointing the most instructive samples for manual annotation. Similarly, Large Language Models (LLMs) such as GPT-3.5 provide an alternative for automated annotation but come with concerns regarding their reliability. This study introduces a novel methodology that integrates human annotators and LLMs within an Active Learning framework. We conducted evaluations on three public datasets. IMDB for sentiment analysis, a Fake News dataset for authenticity discernment, and a Movie Genres dataset for multi-label classification.The proposed framework integrates human annotation with the output of LLMs, depending on the model uncertainty levels. This strategy achieves an optimal balance between cost efficiency and classification performance. The empirical results show a substantial decrease in the costs associated with data annotation while either maintaining or improving model accuracy.},
  langid = {american}
}

@inproceedings{rubinLearningRetrievePrompts2022,
  title = {Learning {{To Retrieve Prompts}} for {{In-Context Learning}}},
  booktitle = {Proceedings of the 2022 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Rubin, Ohad and Herzig, Jonathan and Berant, Jonathan},
  editor = {Carpuat, Marine and {de Marneffe}, Marie-Catherine and Meza Ruiz, Ivan Vladimir},
  year = {2022},
  month = jul,
  pages = {2655--2671},
  publisher = {Association for Computational Linguistics},
  address = {Seattle, United States},
  doi = {10.18653/v1/2022.naacl-main.191},
  urldate = {2023-11-14},
  abstract = {In-context learning is a recent paradigm in natural language understanding, where a large pre-trained language model (LM) observes a test instance and a few training examples as its input, and directly decodes the output without any update to its parameters. However, performance has been shown to strongly depend on the selected training examples (termed prompts). In this work, we propose an efficient method for retrieving prompts for in-context learning using annotated data and an LM. Given an input-output pair, we estimate the probability of the output given the input and a candidate training example as the prompt, and label training examples as positive or negative based on this probability. We then train an efficient dense retriever from this data, which is used to retrieve training examples as prompts at test time. We evaluate our approach on three sequence-to-sequence tasks where language utterances are mapped to meaning representations, and find that it substantially outperforms prior work and multiple baselines across the board.}
}

@article{rumelhartLearningRepresentationsBackpropagating1986,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  year = {1986},
  month = oct,
  journal = {Nature},
  volume = {323},
  number = {6088},
  pages = {533--536},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/323533a0},
  urldate = {2023-09-12},
  abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal `hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
  copyright = {1986 Springer Nature Limited},
  langid = {english}
}

@misc{samuelPersonaGymEvaluatingPersona2024,
  title = {{{PersonaGym}}: {{Evaluating Persona Agents}} and {{LLMs}}},
  shorttitle = {{{PersonaGym}}},
  author = {Samuel, Vinay and Zou, Henry Peng and Zhou, Yue and Chaudhari, Shreyas and Kalyan, Ashwin and Rajpurohit, Tanmay and Deshpande, Ameet and Narasimhan, Karthik and Murahari, Vishvak},
  year = {2024},
  month = jul,
  number = {arXiv:2407.18416},
  eprint = {2407.18416},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.18416},
  urldate = {2024-08-08},
  abstract = {Persona agents, which are LLM agents that act according to an assigned persona, have demonstrated impressive contextual response capabilities across various applications. These persona agents offer significant enhancements across diverse sectors, such as education, healthcare, and entertainment, where model developers can align agent responses to different user requirements thereby broadening the scope of agent applications. However, evaluating persona agent performance is incredibly challenging due to the complexity of assessing persona adherence in free-form interactions across various environments that are relevant to each persona agent. We introduce PersonaGym, the first dynamic evaluation framework for assessing persona agents, and PersonaScore, the first automated human-aligned metric grounded in decision theory for comprehensive large-scale evaluation of persona agents. Our evaluation of 6 open and closed-source LLMs, using a benchmark encompassing 200 personas and 10,000 questions, reveals significant opportunities for advancement in persona agent capabilities across state-of-the-art models. For example, Claude 3.5 Sonnet only has a 2.97\% relative improvement in PersonaScore than GPT 3.5 despite being a much more advanced model. Importantly, we find that increased model size and complexity do not necessarily imply enhanced persona agent capabilities thereby highlighting the pressing need for algorithmic and architectural invention towards faithful and performant persona agents.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@inproceedings{sangMBTIPersonalityPrediction2022,
  title = {{{MBTI Personality Prediction}} for {{Fictional Characters Using Movie Scripts}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2022},
  author = {Sang, Yisi and Mou, Xiangyang and Yu, Mo and Wang, Dakuo and Li, Jing and Stanton, Jeffrey},
  year = {2022},
  month = dec,
  pages = {6715--6724},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.findings-emnlp.500},
  urldate = {2023-09-26},
  abstract = {An NLP model that understands stories should be able to understand the characters in them. To support the development of neural models for this purpose, we construct a benchmark, Story2Personality. The task is to predict a movie character's MBTI or Big 5 personality types based on the narratives of the character. Experiments show that our task is challenging for the existing text classification models, as none is able to largely outperform random guesses. We further proposed a multi-view model for personality prediction using both verbal and non-verbal descriptions, which gives improvement compared to using only verbal descriptions. The uniqueness and challenges in our dataset call for the development of narrative comprehension techniques from the perspective of understanding characters.}
}

@inproceedings{sannerLargeLanguageModels2023,
  title = {Large {{Language Models}} Are {{Competitive Near Cold-start Recommenders}} for {{Language-}} and {{Item-based Preferences}}},
  booktitle = {Proceedings of the 17th {{ACM Conference}} on {{Recommender Systems}}},
  author = {Sanner, Scott and Balog, Krisztian and Radlinski, Filip and Wedin, Ben and Dixon, Lucas},
  year = {2023},
  month = sep,
  series = {{{RecSys}} '23},
  pages = {890--896},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3604915.3608845},
  urldate = {2024-09-24},
  abstract = {Traditional recommender systems leverage users' item preference history to recommend novel content that users may like. However, modern dialog interfaces that allow users to express language-based preferences offer a fundamentally different modality for preference input. Inspired by recent successes of prompting paradigms for large language models (LLMs), we study their use for making recommendations from both item-based and language-based preferences in comparison to state-of-the-art item-based collaborative filtering (CF) methods. To support this investigation, we collect a new dataset consisting of both item-based and language-based preferences elicited from users along with their ratings on a variety of (biased) recommended items and (unbiased) random items. Among numerous experimental results, we find that LLMs provide competitive recommendation performance for pure language-based preferences (no item preferences) in the near cold-start case in comparison to item-based CF methods, despite having no supervised training for this specific task (zero-shot) or only a few labels (few-shot). This is particularly promising as language-based preference representations are more explainable and scrutable than item-based or vector-based representations.},
  isbn = {979-8-4007-0241-9}
}

@article{sarkelaMethodEmpathyBasedStories2020,
  title = {The {{Method}} of {{Empathy-Based Stories}} as a {{Tool}} for {{Research}} and {{Teaching}}},
  author = {S{\"a}rkel{\"a}, Elina and Suoranta, Juha},
  year = {2020},
  month = feb,
  journal = {The Qualitative Report},
  volume = {25},
  number = {2},
  pages = {399--415},
  doi = {10.46743/2160-3715/2020.4124},
  abstract = {In this article we describe a qualitative research method, the ``Method of Empathy-Based Stories'' (MEBS) and ponder its value in classroom teaching. Our research question is as follows: What is MEBS and what are its possible uses in research and teaching? We gathered empathy-based stories written by students (N = 15) and analysed them with thematic analysis. The dominating themes in writings were the threat of climate change and various coping strategies. MEBS allowed students to describe their ways of thinking and acting, and to take part in the discussion. In general, the use of MEBS can generate hypotheses and interpretive horizons and stir questions yet to be asked. The main purpose of using MEBS in qualitative research and in teaching is to inspire qualitative researchers' and research participants' interpretive imagination.},
  keywords = {516 Educational sciences,MEBS,Method of Empathy-Based Stories,Non-Active Role Playing,qualitative research methods,teaching}
}

@article{satheeshResumeRankingBased2020,
  title = {Resume {{Ranking}} Based on {{Job Description}} Using {{SpaCy NER}} Model},
  author = {Satheesh, Dr K and Jahnavi, A and Iswarya, L and Ayesha, K and Bhanusekhar, G and Hanisha, K},
  year = {2020},
  volume = {07},
  number = {05},
  pages = {5},
  langid = {english}
}

@article{schaferEmpiricalEvaluationUsing2024,
  title = {An {{Empirical Evaluation}} of {{Using Large Language Models}} for {{Automated Unit Test Generation}}},
  author = {Sch{\"a}fer, Max and Nadi, Sarah and Eghbali, Aryaz and Tip, Frank},
  year = {2024},
  month = jan,
  journal = {IEEE Transactions on Software Engineering},
  volume = {50},
  number = {1},
  pages = {85--105},
  issn = {1939-3520},
  doi = {10.1109/TSE.2023.3334955},
  urldate = {2024-01-16},
  abstract = {Unit tests play a key role in ensuring the correctness of software. However, manually creating unit tests is a laborious task, motivating the need for automation. Large Language Models (LLMs) have recently been applied to various aspects of software development, including their suggested use for automated generation of unit tests, but while requiring additional training or few-shot learning on examples of existing tests. This paper presents a large-scale empirical evaluation on the effectiveness of LLMs for automated unit test generation without requiring additional training or manual effort. Concretely, we consider an approach where the LLM is provided with prompts that include the signature and implementation of a function under test, along with usage examples extracted from documentation. Furthermore, if a generated test fails, our approach attempts to generate a new test that fixes the problem by re-prompting the model with the failing test and error message. We implement our approach in TestPilot, an adaptive LLM-based test generation tool for JavaScript that automatically generates unit tests for the methods in a given project's API. We evaluate TestPilot using OpenAI's gpt3.5-turbo LLM on 25 npm packages with a total of 1,684 API functions. The generated tests achieve a median statement coverage of 70.2\% and branch coverage of 52.8\%. In contrast, the state-of-the feedback-directed JavaScript test generation technique, Nessie, achieves only 51.3\% statement coverage and 25.6\% branch coverage. Furthermore, experiments with excluding parts of the information included in the prompts show that all components contribute towards the generation of effective test suites. We also find that 92.8\% of TestPilot's generated tests have {\textbackslash}leq{$\leq$} 50\% similarity with existing tests (as measured by normalized edit distance), with none of them being exact copies. Finally, we run TestPilot with two additional LLMs, OpenAI's older code-cushman-002 LLM and StarCoder, an LLM for which the training process is publicly documented. Overall, we observed similar results with the former (68.2\% median statement coverage), and somewhat worse results with the latter (54.0\% median statement coverage), suggesting that the effectiveness of the approach is influenced by the size and training set of the LLM, but does not fundamentally depend on the specific model.}
}

@article{schickToolformerLanguageModels2023,
  title = {Toolformer: {{Language Models Can Teach Themselves}} to {{Use Tools}}},
  shorttitle = {Toolformer},
  author = {Schick, Timo and {Dwivedi-Yu}, Jane and Dessi, Roberto and Raileanu, Roberta and Lomeli, Maria and Hambro, Eric and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  year = {2023},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {68539--68551},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/d842425e4bf79ba039352da0f658a906-Abstract-Conference.html},
  urldate = {2024-06-11},
  langid = {english},
  keywords = {No DOI found}
}

@misc{schmidgallAgentClinicMultimodalAgent2024,
  title = {{{AgentClinic}}: A Multimodal Agent Benchmark to Evaluate {{AI}} in Simulated Clinical Environments},
  shorttitle = {{{AgentClinic}}},
  author = {Schmidgall, Samuel and Ziaei, Rojin and Harris, Carl and Reis, Eduardo and Jopling, Jeffrey and Moor, Michael},
  year = {2024},
  month = may,
  number = {arXiv:2405.07960},
  eprint = {2405.07960},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2405.07960},
  urldate = {2024-07-10},
  abstract = {Diagnosing and managing a patient is a complex, sequential decision making process that requires physicians to obtain information -- such as which tests to perform -- and to act upon it. Recent advances in artificial intelligence (AI) and large language models (LLMs) promise to profoundly impact clinical care. However, current evaluation schemes overrely on static medical question-answering benchmarks, falling short on interactive decision-making that is required in real-life clinical work. Here, we present AgentClinic: a multimodal benchmark to evaluate LLMs in their ability to operate as agents in simulated clinical environments. In our benchmark, the doctor agent must uncover the patient's diagnosis through dialogue and active data collection. We present two open medical agent benchmarks: a multimodal image and dialogue environment, AgentClinic-NEJM, and a dialogue-only environment, AgentClinic-MedQA. We embed cognitive and implicit biases both in patient and doctor agents to emulate realistic interactions between biased agents. We find that introducing bias leads to large reductions in diagnostic accuracy of the doctor agents, as well as reduced compliance, confidence, and follow-up consultation willingness in patient agents. Evaluating a suite of state-of-the-art LLMs, we find that several models that excel in benchmarks like MedQA are performing poorly in AgentClinic-MedQA. We find that the LLM used in the patient agent is an important factor for performance in the AgentClinic benchmark. We show that both having limited interactions as well as too many interaction reduces diagnostic accuracy in doctor agents. The code and data for this work is publicly available at https://AgentClinic.github.io.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Human-Computer Interaction}
}

@article{schmidtSimulatingHumanHCD2024,
  title = {Simulating the {{Human}} in {{HCD}} with {{ChatGPT}}: {{Redesigning Interaction Design}} with {{AI}}},
  shorttitle = {Simulating the {{Human}} in {{HCD}} with {{ChatGPT}}},
  author = {Schmidt, Albrecht and Elagroudy, Passant and Draxler, Fiona and Kreuter, Frauke and Welsch, Robin},
  year = {2024},
  month = jan,
  journal = {interactions},
  volume = {31},
  number = {1},
  pages = {24--31},
  issn = {1072-5520},
  doi = {10.1145/3637436},
  urldate = {2024-09-05}
}

@inproceedings{schroderRevisitingUncertaintybasedQuery2022,
  title = {Revisiting {{Uncertainty-based Query Strategies}} for {{Active Learning}} with {{Transformers}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Schr{\"o}der, Christopher and Niekler, Andreas and Potthast, Martin},
  year = {2022},
  pages = {2194--2203},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.findings-acl.172},
  urldate = {2023-11-16},
  abstract = {Active learning is the iterative construction of a classification model through targeted labeling, enabling significant labeling cost savings. As most research on active learning has been carried out before transformer-based language models (``transformers'') became popular, despite its practical importance, comparably few papers have investigated how transformers can be combined with active learning to date. This can be attributed to the fact that using state-of-the-art query strategies for transformers induces a prohibitive runtime overhead, which effectively nullifies, or even outweighs the desired cost savings. For this reason, we revisit uncertainty-based query strategies, which had been largely outperformed before, but are particularly suited in the context of fine-tuning transformers. In an extensive evaluation, we connect transformers to experiments from previous research, assessing their performance on five widely used text classification benchmarks. For active learning with transformers, several other uncertainty-based approaches outperform the well-known prediction entropy query strategy, thereby challenging its status as most popular uncertainty baseline in active learning for text classification.},
  langid = {english}
}

@misc{Selenium,
  title = {Selenium},
  journal = {Selenium},
  url = {https://www.selenium.dev/},
  urldate = {2024-08-14},
  abstract = {Selenium automates browsers. That's it! What you do with that power is entirely up to you. Primarily it is for automating web applications for testing purposes, but is certainly not limited to just that. Boring web-based administration tasks can (and should) also be automated as well. Getting Started Selenium WebDriver Selenium WebDriver If you want to create robust, browser-based regression automation suites and tests, scale and distribute scripts across many environments, then you want to use Selenium WebDriver, a collection of language specific bindings to drive a browser - the way it is meant to be driven.},
  langid = {english}
}

@inproceedings{senerActiveLearningConvolutional2018,
  title = {Active {{Learning}} for {{Convolutional Neural Networks}}: {{A Core-Set Approach}}},
  shorttitle = {Active {{Learning}} for {{Convolutional Neural Networks}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Sener, Ozan and Savarese, Silvio},
  year = {2018},
  month = feb,
  url = {https://openreview.net/forum?id=H1aIuk-RW},
  urldate = {2023-11-16},
  abstract = {Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe; training a deep model on a very large dataset of supervised examples. However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive. One way to ease this problem is coming up with smart ways for choosing images to be labelled from a very large collection (i.e. active learning). Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to CNNs when applied in batch setting. Inspired by these limitations, we define the problem of active learning as core-set selection, i.e. choosing set of points such that a model learned over the selected subset is competitive for the remaining data points. We further present a theoretical result characterizing the performance of any selected subset using the geometry of the datapoints. As an active learning algorithm, we choose the subset which is expected to yield best result according to our characterization. Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin.},
  langid = {english}
}

@inproceedings{seoBidirectionalAttentionFlow2017,
  title = {Bidirectional {{Attention Flow}} for {{Machine Comprehension}}},
  booktitle = {The {{International Conference}} on {{Learning Representations}}},
  author = {Seo, Minjoon and Kembhavi, Aniruddha and Farhadi, Ali and Hajishirzi, Hannaneh},
  year = {2017},
  eprint = {1611.01603},
  url = {http://arxiv.org/abs/1611.01603},
  urldate = {2021-11-30},
  abstract = {Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bidirectional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.},
  archiveprefix = {arXiv},
  langid = {english}
}

@misc{SetNextGenMidway,
  title = {Set up {{NextGen Midway}} --- {{Developer Setup}}},
  url = {https://docs.hub.amazon.dev/dev-setup/clouddesktop-midway-nextgen/},
  urldate = {2024-09-11}
}

@misc{shaoPrivacyLensEvaluatingPrivacy2024,
  title = {{{PrivacyLens}}: {{Evaluating Privacy Norm Awareness}} of {{Language Models}} in {{Action}}},
  shorttitle = {{{PrivacyLens}}},
  author = {Shao, Yijia and Li, Tianshi and Shi, Weiyan and Liu, Yanchen and Yang, Diyi},
  year = {2024},
  month = aug,
  number = {arXiv:2409.00138},
  eprint = {2409.00138},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.00138},
  urldate = {2024-09-12},
  abstract = {As language models (LMs) are widely utilized in personalized communication scenarios (e.g., sending emails, writing social media posts) and endowed with a certain level of agency, ensuring they act in accordance with the contextual privacy norms becomes increasingly critical. However, quantifying the privacy norm awareness of LMs and the emerging privacy risk in LM-mediated communication is challenging due to (1) the contextual and long-tailed nature of privacy-sensitive cases, and (2) the lack of evaluation approaches that capture realistic application scenarios. To address these challenges, we propose PrivacyLens, a novel framework designed to extend privacy-sensitive seeds into expressive vignettes and further into agent trajectories, enabling multi-level evaluation of privacy leakage in LM agents' actions. We instantiate PrivacyLens with a collection of privacy norms grounded in privacy literature and crowdsourced seeds. Using this dataset, we reveal a discrepancy between LM performance in answering probing questions and their actual behavior when executing user instructions in an agent setup. State-of-the-art LMs, like GPT-4 and Llama-3-70B, leak sensitive information in 25.68\% and 38.69\% of cases, even when prompted with privacy-enhancing instructions. We also demonstrate the dynamic nature of PrivacyLens by extending each seed into multiple trajectories to red-team LM privacy leakage risk. Dataset and code are available at https://github.com/SALT-NLP/PrivacyLens.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Cryptography and Security}
}

@inproceedings{sharmaActiveLearningRationales2015,
  title = {Active {{Learning}} with {{Rationales}} for {{Text Classification}}},
  booktitle = {Proceedings of the 2015 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Sharma, Manali and Zhuang, Di and Bilgic, Mustafa},
  editor = {Mihalcea, Rada and Chai, Joyce and Sarkar, Anoop},
  year = {2015},
  month = may,
  pages = {441--451},
  publisher = {Association for Computational Linguistics},
  address = {Denver, Colorado},
  doi = {10.3115/v1/N15-1047},
  urldate = {2023-11-13}
}

@book{shawHandbookUsabilityTesting1996,
  title = {Handbook of Usability Testing: {{How}} to Plan, Design, and Conduct Effective Tests},
  shorttitle = {Handbook of Usability Testing},
  author = {Shaw, Debora},
  year = {1996},
  month = mar,
  volume = {47},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/%28SICI%291097-4571%28199603%2947%3A3%3C258%3A%3AAID-ASI18%3E3.0.CO%3B2-%23},
  urldate = {2024-09-10},
  abstract = {Semantic Scholar extracted view of "Handbook of usability testing: How to plan, design, and conduct effective tests" by Debora Shaw},
  copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
  langid = {english}
}

@article{shawHandbookUsabilityTesting1996a,
  title = {Handbook of Usability Testing: {{How}} to Plan, Design, and Conduct Effective Tests},
  shorttitle = {Handbook of Usability Testing},
  author = {Shaw, Debora},
  year = {1996},
  month = mar,
  journal = {Journal of the American Society for Information Science},
  volume = {47},
  number = {3},
  pages = {258--259},
  issn = {0002-8231, 1097-4571},
  doi = {10.1002/(SICI)1097-4571(199603)47:3{$<$}258::AID-ASI18{$>$}3.0.CO;2-\#},
  urldate = {2024-09-10},
  abstract = {Semantic Scholar extracted view of "Handbook of usability testing: How to plan, design, and conduct effective tests" by Debora Shaw},
  copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
  langid = {english}
}

@inproceedings{shenDeepActiveLearning2017,
  title = {Deep {{Active Learning}} for {{Named Entity Recognition}}},
  booktitle = {Proceedings of the 2nd {{Workshop}} on {{Representation Learning}} for {{NLP}}},
  author = {Shen, Yanyao and Yun, Hyokun and Lipton, Zachary and Kronrod, Yakov and Anandkumar, Animashree},
  editor = {Blunsom, Phil and Bordes, Antoine and Cho, Kyunghyun and Cohen, Shay and Dyer, Chris and Grefenstette, Edward and Hermann, Karl Moritz and Rimell, Laura and Weston, Jason and Yih, Scott},
  year = {2017},
  month = aug,
  pages = {252--256},
  publisher = {Association for Computational Linguistics},
  address = {Vancouver, Canada},
  doi = {10.18653/v1/W17-2630},
  urldate = {2023-11-13},
  abstract = {Deep neural networks have advanced the state of the art in named entity recognition. However, under typical training procedures, advantages over classical methods emerge only with large datasets. As a result, deep learning is employed only when large public datasets or a large budget for manually labeling data is available. In this work, we show otherwise: by combining deep learning with active learning, we can outperform classical methods even with a significantly smaller amount of training data.}
}

@misc{shenMindMachineRise2025,
  title = {From {{Mind}} to {{Machine}}: {{The Rise}} of {{Manus AI}} as a {{Fully Autonomous Digital Agent}}},
  shorttitle = {From {{Mind}} to {{Machine}}},
  author = {Shen, Minjie and Yang, Qikai},
  year = {2025},
  month = may,
  number = {arXiv:2505.02024},
  eprint = {2505.02024},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.02024},
  urldate = {2025-06-04},
  abstract = {Manus AI is a general-purpose AI agent introduced in early 2025, marking a significant advancement in autonomous artificial intelligence. Developed by the Chinese startup Monica.im, Manus is designed to bridge the gap between "mind" and "hand" - combining the reasoning and planning capabilities of large language models with the ability to execute complex, end-to-end tasks that produce tangible outcomes. This paper presents a comprehensive overview of Manus AI, exploring its core technical architecture, diverse applications across sectors such as healthcare, finance, manufacturing, robotics, and gaming, as well as its key strengths, current limitations, and future potential. Positioned as a preview of what lies ahead, Manus AI represents a shift toward intelligent agents that can translate high-level intentions into real-world actions, heralding a new era of human-AI collaboration.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence}
}

@inproceedings{shiEvaluationNeuralCode2022,
  title = {On the {{Evaluation}} of {{Neural Code Summarization}}},
  booktitle = {Proceedings of the 44th {{International Conference}} on {{Software Engineering}}},
  author = {Shi, Ensheng and Wang, Yanlin and Du, Lun and Chen, Junjie and Han, Shi and Zhang, Hongyu and Zhang, Dongmei and Sun, Hongbin},
  year = {2022},
  month = may,
  eprint = {2107.07112},
  primaryclass = {cs},
  pages = {1597--1608},
  doi = {10.1145/3510003.3510060},
  urldate = {2023-03-29},
  abstract = {Source code summaries are important for program comprehension and maintenance. However, there are plenty of programs with missing, outdated, or mismatched summaries. Recently, deep learning techniques have been exploited to automatically generate summaries for given code snippets. To achieve a profound understanding of how far we are from solving this problem and provide suggestions to future research, in this paper, we conduct a systematic and in-depth analysis of 5 state-of-the-art neural code summarization models on 6 widely used BLEU variants, 4 pre-processing operations and their combinations, and 3 widely used datasets. The evaluation results show that some important factors have a great influence on the model evaluation, especially on the performance of models and the ranking among the models. However, these factors might be easily overlooked. Specifically, (1) the BLEU metric widely used in existing work of evaluating code summarization models has many variants. Ignoring the differences among these variants could greatly affect the validity of the claimed results. Furthermore, we conduct human evaluations and find that the metric BLEU-DC is most correlated to human perception; (2) code pre-processing choices can have a large (from -18{\textbackslash}\% to +25{\textbackslash}\%) impact on the summarization performance and should not be neglected. We also explore the aggregation of pre-processing combinations and boost the performance of models; (3) some important characteristics of datasets (corpus sizes, data splitting methods, and duplication ratios) have a significant impact on model evaluation. Based on the experimental results, we give actionable suggestions for evaluating code summarization and choosing the best method in different scenarios. We also build a shared code summarization toolbox to facilitate future research.},
  archiveprefix = {arXiv}
}

@article{shiffmanEcologicalMomentaryAssessment2008,
  title = {Ecological Momentary Assessment},
  author = {Shiffman, Saul and Stone, Arthur A. and Hufford, Michael R.},
  year = {2008},
  journal = {Annual Review of Clinical Psychology},
  volume = {4},
  pages = {1--32},
  issn = {1548-5943},
  doi = {10.1146/annurev.clinpsy.3.022806.091415},
  abstract = {Assessment in clinical psychology typically relies on global retrospective self-reports collected at research or clinic visits, which are limited by recall bias and are not well suited to address how behavior changes over time and across contexts. Ecological momentary assessment (EMA) involves repeated sampling of subjects' current behaviors and experiences in real time, in subjects' natural environments. EMA aims to minimize recall bias, maximize ecological validity, and allow study of microprocesses that influence behavior in real-world contexts. EMA studies assess particular events in subjects' lives or assess subjects at periodic intervals, often by random time sampling, using technologies ranging from written diaries and telephones to electronic diaries and physiological sensors. We discuss the rationale for EMA, EMA designs, methodological and practical issues, and comparisons of EMA and recall data. EMA holds unique promise to advance the science and practice of clinical psychology by shedding light on the dynamics of behavior in real-world settings.},
  langid = {english},
  pmid = {18509902},
  keywords = {Data Collection,Humans,Medical History Taking,Psychology Clinical,Social Behavior,Social Environment,Time Factors}
}

@misc{shiLargeLanguageModels2023,
  title = {Large {{Language Models Can Be Easily Distracted}} by {{Irrelevant Context}}},
  author = {Shi, Freda and Chen, Xinyun and Misra, Kanishka and Scales, Nathan and Dohan, David and Chi, Ed and Sch{\"a}rli, Nathanael and Zhou, Denny},
  year = {2023},
  month = feb,
  number = {arXiv:2302.00093},
  eprint = {2302.00093},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.00093},
  urldate = {2023-03-13},
  abstract = {Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of cutting-edge prompting techniques for large language models, and find that the model performance is dramatically decreased when irrelevant information is included. We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.},
  archiveprefix = {arXiv}
}

@inproceedings{shinUnderstandingHumanAIWorkflows2024,
  title = {Understanding {{Human-AI Workflows}} for {{Generating Personas}}},
  booktitle = {Proceedings of the 2024 {{ACM Designing Interactive Systems Conference}}},
  author = {Shin, Joongi and Hedderich, Michael A. and Rey, Bart{\l}omiej Jakub and Lucero, Andr{\'e}s and Oulasvirta, Antti},
  year = {2024},
  month = jul,
  series = {{{DIS}} '24},
  pages = {757--781},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3643834.3660729},
  urldate = {2024-07-05},
  abstract = {One barrier to deeper adoption of user-research methods is the amount of labor required to create high-quality representations of collected data. Trained user researchers need to analyze datasets and produce informative summaries pertaining to the original data. While Large Language Models (LLMs) could assist in generating summaries, they are known to hallucinate and produce biased responses. In this paper, we study human--AI workflows that differently delegate subtasks in user research between human experts and LLMs. Studying persona generation as our case, we found that LLMs are not good at capturing key characteristics of user data on their own. Better results are achieved when we leverage human skill in grouping user data by their key characteristics and exploit LLMs for summarizing pre-grouped data into personas. Personas generated via this collaborative approach can be more representative and empathy-evoking than ones generated by human experts or LLMs alone. We also found that LLMs could mimic generated personas and enable interaction with personas, thereby helping user researchers empathize with them. We conclude that LLMs, by facilitating the analysis of user data, may promote widespread application of qualitative methods in user research.},
  isbn = {979-8-4007-0583-0}
}

@misc{shiReconReducingConflicting2023,
  title = {Recon: {{Reducing Conflicting Gradients}} from the {{Root}} for {{Multi-Task Learning}}},
  shorttitle = {Recon},
  author = {Shi, Guangyuan and Li, Qimai and Zhang, Wenlong and Chen, Jiaxin and Wu, Xiao-Ming},
  year = {2023},
  month = feb,
  number = {arXiv:2302.11289},
  eprint = {2302.11289},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.11289},
  urldate = {2023-04-11},
  abstract = {A fundamental challenge for multi-task learning is that different tasks may conflict with each other when they are solved jointly, and a cause of this phenomenon is conflicting gradients during optimization. Recent works attempt to mitigate the influence of conflicting gradients by directly altering the gradients based on some criteria. However, our empirical study shows that ``gradient surgery'' cannot effectively reduce the occurrence of conflicting gradients. In this paper, we take a different approach to reduce conflicting gradients from the root. In essence, we investigate the task gradients w.r.t. each shared network layer, select the layers with high conflict scores, and turn them to task-specific layers. Our experiments show that such a simple approach can greatly reduce the occurrence of conflicting gradients in the remaining shared layers and achieve better performance, with only a slight increase in model parameters in many cases. Our approach can be easily applied to improve various state-of-the-art methods including gradient manipulation methods and branched architecture search methods. Given a network architecture (e.g., ResNet18), it only needs to search for the conflict layers once, and the network can be modified to be used with different methods on the same or even different datasets to gain performance improvement. The source code is available at https://github.com/moukamisama/Recon.},
  archiveprefix = {arXiv}
}

@article{shoeybiMegatronLMTrainingMultiBillion2020,
  title = {Megatron-{{LM}}: {{Training Multi-Billion Parameter Language Models Using Model Parallelism}}},
  shorttitle = {Megatron-{{LM}}},
  author = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  year = {2020},
  month = mar,
  journal = {arXiv:1909.08053 [cs]},
  eprint = {1909.08053},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1909.08053},
  urldate = {2022-01-19},
  abstract = {Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76\% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30\% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5\% compared to SOTA accuracy of 63.2\%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9\% compared to SOTA accuracy of 89.4\%).},
  archiveprefix = {arXiv},
  annotation = {00333}
}

@misc{singhBetterGeneralizationSemantic2024,
  title = {Better {{Generalization}} with {{Semantic IDs}}: {{A Case Study}} in {{Ranking}} for {{Recommendations}}},
  shorttitle = {Better {{Generalization}} with {{Semantic IDs}}},
  author = {Singh, Anima and Vu, Trung and Mehta, Nikhil and Keshavan, Raghunandan and Sathiamoorthy, Maheswaran and Zheng, Yilin and Hong, Lichan and Heldt, Lukasz and Wei, Li and Tandon, Devansh and Chi, Ed H. and Yi, Xinyang},
  year = {2024},
  month = may,
  number = {arXiv:2306.08121},
  eprint = {2306.08121},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.08121},
  urldate = {2024-09-23},
  abstract = {Randomly-hashed item ids are used ubiquitously in recommendation models. However, the learned representations from random hashing prevents generalization across similar items, causing problems of learning unseen and long-tail items, especially when item corpus is large, power-law distributed, and evolving dynamically. In this paper, we propose using content-derived features as a replacement for random ids. We show that simply replacing ID features with content-based embeddings can cause a drop in quality due to reduced memorization capability. To strike a good balance of memorization and generalization, we propose to use Semantic IDs -- a compact discrete item representation learned from frozen content embeddings using RQ-VAE that captures the hierarchy of concepts in items -- as a replacement for random item ids. Similar to content embeddings, the compactness of Semantic IDs poses a problem of easy adaption in recommendation models. We propose novel methods for adapting Semantic IDs in industry-scale ranking models, through hashing sub-pieces of of the Semantic-ID sequences. In particular, we find that the SentencePiece model that is commonly used in LLM tokenization outperforms manually crafted pieces such as N-grams. To the end, we evaluate our approaches in a real-world ranking model for YouTube recommendations. Our experiments demonstrate that Semantic IDs can replace the direct use of video IDs by improving the generalization ability on new and long-tail item slices without sacrificing overall model quality.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning}
}

@inproceedings{smithSkillExtractionDomainSpecific2021,
  title = {Skill {{Extraction}} for {{Domain-Specific Text Retrieval}} in a {{Job-Matching Platform}}},
  booktitle = {Experimental {{IR Meets Multilinguality}}, {{Multimodality}}, and {{Interaction}}},
  author = {Smith, Ellery and Weiler, Andreas and Braschler, Martin},
  editor = {Candan, K. Sel{\c c}uk and Ionescu, Bogdan and Goeuriot, Lorraine and Larsen, Birger and M{\"u}ller, Henning and Joly, Alexis and Maistro, Maria and Piroi, Florina and Faggioli, Guglielmo and Ferro, Nicola},
  year = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {116--128},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-85251-1\_10},
  abstract = {We discuss a domain-specific retrieval application for matching job seekers with open positions that uses a novel syntactic method of extracting skill-terms from the text of natural language job advertisements. Our new method is contrasted with two word embeddings methods, using word2vec. We define the notion of a skill headword, and present an algorithm that learns syntactic dependency patterns to recognize skill-terms. In all metrics, our syntactic method outperforms both word embeddings methods. Moreover, the word embeddings approaches were unable to model a meaningful distinction between skill-terms and non-skill-terms, while our syntactic approach was able to perform this successfully. We also show how these extracted skills can be used to automatically construct a semantic job-skills ontology, and facilitate a job-to-candidate matching system.},
  isbn = {978-3-030-85251-1},
  langid = {english}
}

@misc{Snipaste,
  title = {Snipaste},
  url = {https://docs.snipaste.com/zh-cn/},
  urldate = {2024-08-24}
}

@misc{sodhiStePStackedLLM2024,
  title = {{{SteP}}: {{Stacked LLM Policies}} for {{Web Actions}}},
  shorttitle = {{{SteP}}},
  author = {Sodhi, Paloma and Branavan, S. R. K. and Artzi, Yoav and McDonald, Ryan},
  year = {2024},
  month = apr,
  number = {arXiv:2310.03720},
  eprint = {2310.03720},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.03720},
  urldate = {2024-07-15},
  abstract = {Performing tasks on the web presents fundamental challenges to large language models (LLMs), including combinatorially large open-world tasks and variations across web interfaces. Simply specifying a large prompt to handle all possible behaviors and states is extremely complex, and results in behavior leaks between unrelated behaviors. Decomposition to distinct policies can address this challenge, but requires carefully handing off control between policies. We propose Stacked LLM Policies for Web Actions (SteP), an approach to dynamically compose policies to solve a diverse set of web tasks. SteP defines a Markov Decision Process where the state is a stack of policies representing the control state, i.e., the chain of policy calls. Unlike traditional methods that are restricted to static hierarchies, SteP enables dynamic control that adapts to the complexity of the task. We evaluate SteP against multiple baselines and web environments including WebArena, MiniWoB++, and a CRM simulator. On WebArena, SteP improves (14.9\% to 35.8\%) over SOTA that use GPT-4 policies, while on MiniWob++, SteP is competitive with prior works while using significantly less data. Our code and data is available at https://asappresearch.github.io/webagents-step.},
  archiveprefix = {arXiv},
  keywords = {CoLM 2024,Computer Science - Machine Learning}
}

@misc{sohl-dicksteinDeepUnsupervisedLearning2015,
  title = {Deep {{Unsupervised Learning}} Using {{Nonequilibrium Thermodynamics}}},
  author = {{Sohl-Dickstein}, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},
  year = {2015},
  month = nov,
  number = {arXiv:1503.03585},
  eprint = {1503.03585},
  primaryclass = {cond-mat, q-bio, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1503.03585},
  urldate = {2023-11-14},
  abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
  archiveprefix = {arXiv}
}

@article{sparckSTATISTICALINTERPRETATIONTERM1972,
  title = {A {{STATISTICAL INTERPRETATION OF TERM SPECIFICITY AND ITS APPLICATION IN RETRIEVAL}}},
  author = {SPARCK, JONES KAREN},
  year = {1972},
  month = jan,
  journal = {Journal of Documentation},
  volume = {28},
  number = {1},
  pages = {11--21},
  publisher = {MCB UP Ltd},
  issn = {0022-0418},
  doi = {10.1108/eb026526},
  urldate = {2023-06-24},
  abstract = {The exhaustivity of document descriptions and the specificity of index terms are usually regarded as independent. It is suggested that specificity should be interpreted statistically, as a function of term use rather than of term meaning. The effects on retrieval of variations in term specificity are examined, experiments with three test collections showing in particular that frequently-occurring terms are required for good overall performance. It is argued that terms should be weighted according to collection frequency, so that matches on less frequent, more specific, terms are of greater value than matches on frequent terms. Results for the test collections show that considerable improvements in performance are obtained with this very simple procedure.}
}

@misc{spathisFirstStepHardest2023,
  title = {The First Step Is the Hardest: {{Pitfalls}} of {{Representing}} and {{Tokenizing Temporal Data}} for {{Large Language Models}}},
  shorttitle = {The First Step Is the Hardest},
  author = {Spathis, Dimitris and Kawsar, Fahim},
  year = {2023},
  month = sep,
  number = {arXiv:2309.06236},
  eprint = {2309.06236},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2309.06236},
  urldate = {2024-03-18},
  abstract = {Large Language Models (LLMs) have demonstrated remarkable generalization across diverse tasks, leading individuals to increasingly use them as personal assistants and universal computing engines. Nevertheless, a notable obstacle emerges when feeding numerical/temporal data into these models, such as data sourced from wearables or electronic health records. LLMs employ tokenizers in their input that break down text into smaller units. However, tokenizers are not designed to represent numerical values and might struggle to understand repetitive patterns and context, treating consecutive values as separate tokens and disregarding their temporal relationships. Here, we discuss recent works that employ LLMs for human-centric tasks such as in mobile health sensing and present a case study showing that popular LLMs tokenize temporal data incorrectly. To address that, we highlight potential solutions such as prompt tuning with lightweight embedding layers as well as multimodal adapters, that can help bridge this "modality gap". While the capability of language models to generalize to other modalities with minimal or no finetuning is exciting, this paper underscores the fact that their outputs cannot be meaningful if they stumble over input nuances.},
  archiveprefix = {arXiv}
}

@misc{srivastavaNICEOptimizeInContext2024,
  title = {{{NICE}}: {{To Optimize In-Context Examples}} or {{Not}}?},
  shorttitle = {{{NICE}}},
  author = {Srivastava, Pragya and Golechha, Satvik and Deshpande, Amit and Sharma, Amit},
  year = {2024},
  month = feb,
  number = {arXiv:2402.06733},
  eprint = {2402.06733},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2402.06733},
  urldate = {2024-02-13},
  abstract = {Recent works have shown that large language models (LLMs) work remarkably well on a wide range of tasks through in-context learning and optimization of in-context examples (ICE). However, most of these studies assume either a fixed or no instruction provided in the prompt, leading to the apparent consensus that the optimization of in-context examples is critical for better performance. We challenge this consensus for instruction-tuned LLMs by investigating the necessity of optimizing in-context examples when task-specific instructions are provided, and find that there are tasks for which various ways of optimizing in-context examples yield diminishing returns. We introduce a task-specific metric called {\textbackslash}metriclong\{\} ({\textbackslash}metric) that quantifies the learnability of tasks from a given instruction, and provides a heuristic that helps decide whether to optimize for instructions or ICE for any new task. On a wide range of tasks and a systematically created instruction set with gradually added details, we validate our hypothesis empirically by computing {\textbackslash}metric with query-dependent bins of examples, comparing different instructions with ICE selection methods, and performing label perturbation experiments. We conclude that tasks can be divided into two broad classes based on the {\textbackslash}metric metric, where the returns on ICE optimization follow predictable trends when instructions are provided in the prompt.},
  archiveprefix = {arXiv}
}

@inproceedings{stahlkeArtificialPlayfulnessTool2019,
  title = {Artificial {{Playfulness}}: {{A Tool}} for {{Automated Agent-Based Playtesting}}},
  shorttitle = {Artificial {{Playfulness}}},
  booktitle = {Extended {{Abstracts}} of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Stahlke, Samantha . and Nova, Atiya and {Mirza-Babaei}, Pejman},
  year = {2019},
  month = may,
  series = {{{CHI EA}} '19},
  pages = {1--6},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3290607.3313039},
  urldate = {2024-09-10},
  abstract = {Usertesting is commonly employed in games user research (GUR) to understand the experience of players interacting with digital games. However, recruitment and testing with human users can be laborious and resource-intensive, particularly for independent developers. To help mitigate these obstacles, we are developing a framework for simulated testing sessions with agents driven by artificial intelligence (AI). Specifically, we aim to imitate the navigation of human players in a virtual world. By mimicking the tendency of users to wander, explore, become lost, and so on, these agents may be used to identify basic issues with a game's world and level design, enabling informed iteration earlier in the development process. Here, we detail our progress in developing a framework for configurable agent navigation and simple visualization of simulated data. Ultimately, we hope to provide a basis for the development of a tool for simulation-driven usability testing in games.},
  isbn = {978-1-4503-5971-9}
}

@misc{steyversCalibrationGapModel2024,
  title = {The {{Calibration Gap}} between {{Model}} and {{Human Confidence}} in {{Large Language Models}}},
  author = {Steyvers, Mark and Tejeda, Heliodoro and Kumar, Aakriti and Belem, Catarina and Karny, Sheer and Hu, Xinyue and Mayer, Lukas and Smyth, Padhraic},
  year = {2024},
  month = jan,
  number = {arXiv:2401.13835},
  eprint = {2401.13835},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.13835},
  urldate = {2024-01-28},
  abstract = {For large language models (LLMs) to be trusted by humans they need to be well-calibrated in the sense that they can accurately assess and communicate how likely it is that their predictions are correct. Recent work has focused on the quality of internal LLM confidence assessments, but the question remains of how well LLMs can communicate this internal model confidence to human users. This paper explores the disparity between external human confidence in an LLM's responses and the internal confidence of the model. Through experiments involving multiple-choice questions, we systematically examine human users' ability to discern the reliability of LLM outputs. Our study focuses on two key areas: (1) assessing users' perception of true LLM confidence and (2) investigating the impact of tailored explanations on this perception. The research highlights that default explanations from LLMs often lead to user overestimation of both the model's confidence and its' accuracy. By modifying the explanations to more accurately reflect the LLM's internal confidence, we observe a significant shift in user perception, aligning it more closely with the model's actual confidence levels. This adjustment in explanatory approach demonstrates potential for enhancing user trust and accuracy in assessing LLM outputs. The findings underscore the importance of transparent communication of confidence levels in LLMs, particularly in high-stakes applications where understanding the reliability of AI-generated information is essential.},
  archiveprefix = {arXiv}
}

@article{sturgeonPopulationbasedStudyCardiovascular2019,
  title = {A Population-Based Study of Cardiovascular Disease Mortality Risk in {{US}} Cancer Patients},
  author = {Sturgeon, Kathleen M and Deng, Lei and Bluethmann, Shirley M and Zhou, Shouhao and Trifiletti, Daniel M and Jiang, Changchuan and Kelly, Scott P and Zaorsky, Nicholas G},
  year = {2019},
  month = dec,
  journal = {European Heart Journal},
  volume = {40},
  number = {48},
  pages = {3889--3897},
  issn = {0195-668X},
  doi = {10.1093/eurheartj/ehz766},
  urldate = {2024-03-29},
  abstract = {This observational study characterized cardiovascular disease (CVD) mortality risk for multiple cancer sites, with respect to the following: (i) continuous calendar year, (ii) age at diagnosis, and (iii) follow-up time after diagnosis.The Surveillance, Epidemiology, and End Results program was used to compare the US general population to 3 234 256 US cancer survivors (1973--2012). Standardized mortality ratios (SMRs) were calculated using coded cause of death from CVDs (heart disease, hypertension, cerebrovascular disease, atherosclerosis, and aortic aneurysm/dissection). Analyses were adjusted by age, race, and sex. Among 28 cancer types, 1 228 328 patients (38.0\%) died from cancer and 365 689 patients (11.3\%) died from CVDs. Among CVDs, 76.3\% of deaths were due to heart disease. In eight cancer sites, CVD mortality risk surpassed index-cancer mortality risk in at least one calendar year. Cardiovascular disease mortality risk was highest in survivors diagnosed at \&lt;35\,years of age. Further, CVD mortality risk is highest (SMR 3.93, 95\% confidence interval 3.89--3.97) within the first year after cancer diagnosis, and CVD mortality risk remains elevated throughout follow-up compared to the general population.The majority of deaths from CVD occur in patients diagnosed with breast, prostate, or bladder cancer. We observed that from the point of cancer diagnosis forward into survivorship cancer patients (all sites) are at elevated risk of dying from CVDs compared to the general US population. In endometrial cancer, the first year after diagnosis poses a very high risk of dying from CVDs, supporting early involvement of cardiologists in such patients.}
}

@inproceedings{suCAiRECOVIDQuestionAnswering2020,
  title = {{{CAiRE-COVID}}: {{A Question Answering}} and {{Query-focused Multi-Document Summarization System}} for {{COVID-19 Scholarly Information Management}}},
  shorttitle = {{{CAiRE-COVID}}},
  booktitle = {Proceedings of the 1st {{Workshop}} on {{NLP}} for {{COVID-19}} ({{Part}} 2) at {{EMNLP}} 2020},
  author = {Su, Dan and Xu, Yan and Yu, Tiezheng and Siddique, Farhad Bin and Barezi, Elham and Fung, Pascale},
  year = {2020},
  month = dec,
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.nlpcovid19-2.14},
  urldate = {2022-08-17},
  abstract = {We present CAiRE-COVID, a real-time question answering (QA) and multi-document summarization system, which won one of the 10 tasks in the Kaggle COVID-19 Open Research Dataset Challenge, judged by medical experts. Our system aims to tackle the recent challenge of mining the numerous scientific articles being published on COVID-19 by answering high priority questions from the community and summarizing salient question-related information. It combines information extraction with state-of-the-art QA and query-focused multi-document summarization techniques, selecting and highlighting evidence snippets from existing literature given a query. We also propose query-focused abstractive and extractive multi-document summarization methods, to provide more relevant information related to the question. We further conduct quantitative experiments that show consistent improvements on various metrics for each module. We have launched our website CAiRE-COVID for broader use by the medical community, and have open-sourced the code for our system, to bootstrap further study by other researches.}
}

@inproceedings{suEnhancingHierarchyAwareGraph2023,
  title = {Enhancing {{Hierarchy-Aware Graph Networks}} with {{Deep Dual Clustering}} for {{Session-based Recommendation}}},
  booktitle = {Proceedings of the {{ACM Web Conference}} 2023},
  author = {Su, Jiajie and Chen, Chaochao and Liu, Weiming and Wu, Fei and Zheng, Xiaolin and Lyu, Haoming},
  year = {2023},
  month = apr,
  pages = {165--176},
  publisher = {ACM},
  address = {Austin TX USA},
  doi = {10.1145/3543507.3583247},
  urldate = {2024-09-24},
  abstract = {Session-based Recommendation aims at predicting the next interacted item based on short anonymous behavior sessions. However, existing solutions neglect to model two inherent properties of sequential representing distributions, i.e., hierarchy structures resulted from item popularity and collaborations existing in both intra- and inter-session. Tackling with these two factors at the same time is challenging. On the one hand, traditional Euclidean space utilized in previous studies fails to capture hierarchy structures due to a restricted representation ability. On the other hand, the intuitive apply of hyperbolic geometry could extract hierarchical patterns but more emphasis on degree distribution weakens intraand inter-session collaborations. To address the challenges, we propose a Hierarchy-Aware Dual Clustering Graph Network (HADCG) model for session-based recommendation. Towards the frst challenge, we design the hierarchy-aware graph modeling module which converts sessions into hyperbolic session graphs, adopting hyperbolic geometry in propagation and attention mechanism so as to integrate chronological and hierarchical information. As for the second challenge, we introduce the deep dual clustering module which develops a two-level clustering strategy, i.e., information regularizer for intra-session clustering and contrastive learner for inter-session clustering, to enhance hyperbolic representation learning from collaborative perspectives and further promote recommendation performance. Extensive experiments on three real-world datasets demonstrate the efectiveness of the proposed HADCG.},
  isbn = {978-1-4503-9416-1},
  langid = {english}
}

@misc{suImproveQueryFocused2021,
  title = {Improve {{Query Focused Abstractive Summarization}} by {{Incorporating Answer Relevance}}},
  author = {Su, Dan and Yu, Tiezheng and Fung, Pascale},
  year = {2021},
  month = may,
  number = {arXiv:2105.12969},
  eprint = {2105.12969},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2105.12969},
  urldate = {2022-09-06},
  abstract = {Query focused summarization (QFS) models aim to generate summaries from source documents that can answer the given query. Most previous work on QFS only considers the query relevance criterion when producing the summary. However, studying the effect of answer relevance in the summary generating process is also important. In this paper, we propose QFS-BART, a model that incorporates the explicit answer relevance of the source documents given the query via a question answering model, to generate coherent and answer-related summaries. Furthermore, our model can take advantage of large pre-trained models which improve the summarization performance significantly. Empirical results on the Debatepedia dataset show that the proposed model achieves the new state-of-the-art performance.},
  archiveprefix = {arXiv}
}

@misc{suInfoEntropyLossMitigate2023,
  title = {{{InfoEntropy Loss}} to {{Mitigate Bias}} of {{Learning Difficulties}} for {{Generative Language Models}}},
  author = {Su, Zhenpeng and Wu, Xing and Bai, Xue and Lin, Zijia and Chen, Hui and Ding, Guiguang and Zhou, Wei and Hu, Songlin},
  year = {2023},
  month = oct,
  number = {arXiv:2310.19531},
  eprint = {2310.19531},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2310.19531},
  urldate = {2023-10-31},
  abstract = {Generative language models are usually pretrained on large text corpus via predicting the next token (i.e., sub-word/word/phrase) given the previous ones. Recent works have demonstrated the impressive performance of large generative language models on downstream tasks. However, existing generative language models generally neglect an inherent challenge in text corpus during training, i.e., the imbalance between frequent tokens and infrequent ones. It can lead a language model to be dominated by common and easy-to-learn tokens, thereby overlooking the infrequent and difficult-to-learn ones. To alleviate that, we propose an Information Entropy Loss (InfoEntropy Loss) function. During training, it can dynamically assess the learning difficulty of a to-be-learned token, according to the information entropy of the corresponding predicted probability distribution over the vocabulary. Then it scales the training loss adaptively, trying to lead the model to focus more on the difficult-to-learn tokens. On the Pile dataset, we train generative language models at different scales of 436M, 1.1B, and 6.7B parameters. Experiments reveal that models incorporating the proposed InfoEntropy Loss can gain consistent performance improvement on downstream benchmarks.},
  archiveprefix = {arXiv}
}

@inproceedings{suLearnbyinteractDataCentricFramework2024,
  title = {Learn-by-Interact: {{A Data-Centric Framework For Self-Adaptive Agents}} in {{Realistic Environments}}},
  shorttitle = {Learn-by-Interact},
  booktitle = {The {{Thirteenth International Conference}} on {{Learning Representations}}},
  author = {Su, Hongjin and Sun, Ruoxi and Yoon, Jinsung and Yin, Pengcheng and Yu, Tao and Arik, Sercan O.},
  year = {2024},
  month = oct,
  url = {https://openreview.net/forum?id=3UKOzGWCVY},
  urldate = {2025-02-17},
  abstract = {Autonomous agents powered by large language models (LLMs) have the potential to enhance human capabilities, assisting with digital tasks from sending emails to performing data analysis. The abilities of existing LLMs at such tasks are often hindered by the lack of high-quality agent data from the corresponding environments they interact with. We propose LEARN-BY-INTERACT, a data-centric framework to adapt LLM agents to any given environments without human annotations. LEARN-BY-INTERACT synthesizes trajectories of agent-environment interactions based on documentations, and constructs instructions by summarizing or abstracting the interaction histories, a process called backward construction. We assess the quality of our synthetic data by using them in both training-based scenarios and training-free in-context learning (ICL), where we craft innovative retrieval approaches optimized for agents. Extensive experiments on SWE-bench, WebArena, OSWorld, and Spider2-V spanning across realistic coding, web, and desktop environments show the effectiveness of LEARN-BY-INTERACT in various downstream agentic tasks --- baseline results are improved up to 11.1\% for ICL with Claude-3.5 and 23.1\% for training with Codestral-22B. We further demonstrate the critical role of backward construction, which provides up to 10.6\% improvement for training. Our ablation studies demonstrate the efficiency provided by our synthesized data in ICL and the superiority of our retrieval pipeline over alternative approaches like conventional retrieval-augmented generation (RAG). We expect that LEARN-BY-INTERACT will serve as a foundation for agent data synthesis as LLMs are increasingly deployed at real-world environments.},
  langid = {english}
}

@misc{sunChallengingBoundariesReasoning2025,
  title = {Challenging the {{Boundaries}} of {{Reasoning}}: {{An Olympiad-Level Math Benchmark}} for {{Large Language Models}}},
  shorttitle = {Challenging the {{Boundaries}} of {{Reasoning}}},
  author = {Sun, Haoxiang and Min, Yingqian and Chen, Zhipeng and Zhao, Wayne Xin and Liu, Zheng and Wang, Zhongyuan and Fang, Lei and Wen, Ji-Rong},
  year = {2025},
  month = mar,
  number = {arXiv:2503.21380},
  eprint = {2503.21380},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.21380},
  urldate = {2025-04-07},
  abstract = {In recent years, the rapid development of large reasoning models has resulted in the saturation of existing benchmarks for evaluating mathematical reasoning, highlighting the urgent need for more challenging and rigorous evaluation frameworks. To address this gap, we introduce OlymMATH, a novel Olympiad-level mathematical benchmark, designed to rigorously test the complex reasoning capabilities of LLMs. OlymMATH features 200 meticulously curated problems, each manually verified and available in parallel English and Chinese versions. The problems are systematically organized into two distinct difficulty tiers: (1) AIME-level problems (easy) that establish a baseline for mathematical reasoning assessment, and (2) significantly more challenging problems (hard) designed to push the boundaries of current state-of-the-art models. In our benchmark, these problems span four core mathematical fields, each including a verifiable numerical solution to enable objective, rule-based evaluation. Empirical results underscore the significant challenge presented by OlymMATH, with state-of-the-art models including DeepSeek-R1 and OpenAI's o3-mini demonstrating notably limited accuracy on the hard subset. Furthermore, the benchmark facilitates comprehensive bilingual assessment of mathematical reasoning abilities-a critical dimension that remains largely unaddressed in mainstream mathematical reasoning benchmarks. We release the OlymMATH benchmark at the STILL project: https://github.com/RUCAIBox/Slow\_Thinking\_with\_LLMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@inproceedings{sunLargeLanguageModels2024,
  title = {Large {{Language Models}} for {{Intent-Driven Session Recommendations}}},
  booktitle = {Proceedings of the 47th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Sun, Zhu and Liu, Hongyang and Qu, Xinghua and Feng, Kaidong and Wang, Yan and Ong, Yew Soon},
  year = {2024},
  month = jul,
  series = {{{SIGIR}} '24},
  pages = {324--334},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3626772.3657688},
  urldate = {2024-09-19},
  abstract = {The goal of intent-aware session recommendation (ISR) approaches is to capture user intents within a session for accurate next-item prediction. However, the capability of these approaches is limited by assuming all sessions have a uniform and fixed number of intents. In reality, user sessions can vary, where the number of intentions may differ from one to another. Moreover, they can only learn user intents in the latent space, which further restricts the model's transparency. To ease these issues, we propose a simple yet effective paradigm for ISR motivated by the advanced reasoning capability of large language models (LLMs). Specifically, we first create an initial prompt to instruct LLMs to predict the next item by inferring varying user intents reflected in a session. Then, we propose an effective optimization mechanism to automatically optimize prompts with an iterative self-reflection. Finally, we leverage the robust generalizability of LLMs across diverse domains to efficiently select the optimal prompt for ISR. As such, the proposed paradigm effectively guides LLMs to identify varying user intents at a semantic level, thus delivering more accurate and comprehensible recommendations. Extensive experiments on three real-world datasets verify the superiority of our proposed method.},
  isbn = {979-8-4007-0431-4}
}

@misc{sunPreservePromoteAttack2021,
  title = {Preserve, {{Promote}}, or {{Attack}}? {{GNN Explanation}} via {{Topology Perturbation}}},
  shorttitle = {Preserve, {{Promote}}, or {{Attack}}?},
  author = {Sun, Yi and Valente, Abel and Liu, Sijia and Wang, Dakuo},
  year = {2021},
  month = mar,
  number = {arXiv:2103.13944},
  eprint = {2103.13944},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.13944},
  urldate = {2025-06-02},
  abstract = {Prior works on formalizing explanations of a graph neural network (GNN) focus on a single use case - to preserve the prediction results through identifying important edges and nodes. In this paper, we develop a multi-purpose interpretation framework by acquiring a mask that indicates topology perturbations of the input graphs. We pack the framework into an interactive visualization system (GNNViz) which can fulfill multiple purposes: Preserve,Promote, or Attack GNN's predictions. We illustrate our approach's novelty and effectiveness with three case studies: First, GNNViz can assist non expert users to easily explore the relationship between graph topology and GNN's decision (Preserve), or to manipulate the prediction (Promote or Attack) for an image classification task on MS-COCO; Second, on the Pokec social network dataset, our framework can uncover unfairness and demographic biases; Lastly, it compares with state-of-the-art GNN explainer baseline on a synthetic dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@article{sunsteinPeoplePreferSystem2016,
  title = {People {{Prefer System}} 2 {{Nudges}} ({{Kind}} Of)},
  author = {Sunstein, Cass R.},
  year = {2016/2017},
  journal = {Duke Law Journal},
  volume = {66},
  pages = {121},
  url = {https://heinonline.org/HOL/Page?handle=hein.journals/duklr66&id=125&div=&collection=},
  keywords = {No DOI found}
}

@misc{suRoFormerEnhancedTransformer2023,
  title = {{{RoFormer}}: {{Enhanced Transformer}} with {{Rotary Position Embedding}}},
  shorttitle = {{{RoFormer}}},
  author = {Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Wen, Bo and Liu, Yunfeng},
  year = {2023},
  month = nov,
  number = {arXiv:2104.09864},
  eprint = {2104.09864},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.09864},
  urldate = {2024-02-14},
  abstract = {Position encoding recently has shown effective in the transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding(RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: {\textbackslash}url\{https://huggingface.co/docs/transformers/model\_doc/roformer\}.},
  archiveprefix = {arXiv}
}

@misc{sushilComparativeStudyZeroshot2024,
  title = {A Comparative Study of Zero-Shot Inference with Large Language Models and Supervised Modeling in Breast Cancer Pathology Classification},
  author = {Sushil, Madhumita and Zack, Travis and Mandair, Divneet and Zheng, Zhiwei and Wali, Ahmed and Yu, Yan-Ning and Quan, Yuwei and Butte, Atul J.},
  year = {2024},
  month = jan,
  number = {arXiv:2401.13887},
  eprint = {2401.13887},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.13887},
  urldate = {2024-01-29},
  abstract = {Although supervised machine learning is popular for information extraction from clinical notes, creating large annotated datasets requires extensive domain expertise and is time-consuming. Meanwhile, large language models (LLMs) have demonstrated promising transfer learning capability. In this study, we explored whether recent LLMs can reduce the need for large-scale data annotations. We curated a manually-labeled dataset of 769 breast cancer pathology reports, labeled with 13 categories, to compare zero-shot classification capability of the GPT-4 model and the GPT-3.5 model with supervised classification performance of three model architectures: random forests classifier, long short-term memory networks with attention (LSTM-Att), and the UCSF-BERT model. Across all 13 tasks, the GPT-4 model performed either significantly better than or as well as the best supervised model, the LSTM-Att model (average macro F1 score of 0.83 vs. 0.75). On tasks with high imbalance between labels, the differences were more prominent. Frequent sources of GPT-4 errors included inferences from multiple samples and complex task design. On complex tasks where large annotated datasets cannot be easily collected, LLMs can reduce the burden of large-scale data labeling. However, if the use of LLMs is prohibitive, the use of simpler supervised models with large annotated datasets can provide comparable results. LLMs demonstrated the potential to speed up the execution of clinical NLP studies by reducing the need for curating large annotated datasets. This may result in an increase in the utilization of NLP-based variables and outcomes in observational clinical studies.},
  archiveprefix = {arXiv}
}

@misc{swayamdiptaDatasetCartographyMapping2020,
  title = {Dataset {{Cartography}}: {{Mapping}} and {{Diagnosing Datasets}} with {{Training Dynamics}}},
  shorttitle = {Dataset {{Cartography}}},
  author = {Swayamdipta, Swabha and Schwartz, Roy and Lourie, Nicholas and Wang, Yizhong and Hajishirzi, Hannaneh and Smith, Noah A. and Choi, Yejin},
  year = {2020},
  month = oct,
  number = {arXiv:2009.10795},
  eprint = {2009.10795},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2009.10795},
  urldate = {2023-12-01},
  abstract = {Large datasets have become commonplace in NLP research. However, the increased emphasis on data quantity has made it challenging to assess the quality of data. We introduce Data Maps---a model-based tool to characterize and diagnose datasets. We leverage a largely ignored source of information: the behavior of the model on individual instances during training (training dynamics) for building data maps. This yields two intuitive measures for each example---the model's confidence in the true class, and the variability of this confidence across epochs---obtained in a single run of training. Experiments across four datasets show that these model-dependent measures reveal three distinct regions in the data map, each with pronounced characteristics. First, our data maps show the presence of "ambiguous" regions with respect to the model, which contribute the most towards out-of-distribution generalization. Second, the most populous regions in the data are "easy to learn" for the model, and play an important role in model optimization. Finally, data maps uncover a region with instances that the model finds "hard to learn"; these often correspond to labeling errors. Our results indicate that a shift in focus from quantity to quality of data could lead to robust models and improved out-of-distribution generalization.},
  archiveprefix = {arXiv}
}

@misc{szafraniecCodeTranslationCompiler2023,
  title = {Code {{Translation}} with {{Compiler Representations}}},
  author = {Szafraniec, Marc and Roziere, Baptiste and Leather, Hugh and Charton, Francois and Labatut, Patrick and Synnaeve, Gabriel},
  year = {2023},
  month = apr,
  number = {arXiv:2207.03578},
  eprint = {2207.03578},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2207.03578},
  urldate = {2024-02-20},
  abstract = {In this paper, we leverage low-level compiler intermediate representations (IR) to improve code translation. Traditional transpilers rely on syntactic information and handcrafted rules, which limits their applicability and produces unnatural-looking code. Applying neural machine translation (NMT) approaches to code has successfully broadened the set of programs on which one can get a natural-looking translation. However, they treat the code as sequences of text tokens, and still do not differentiate well enough between similar pieces of code which have different semantics in different languages. The consequence is low quality translation, reducing the practicality of NMT, and stressing the need for an approach significantly increasing its accuracy. Here we propose to augment code translation with IRs, specifically LLVM IR, with results on the C++, Java, Rust, and Go languages. Our method improves upon the state of the art for unsupervised code translation, increasing the number of correct translations by 11\% on average, and up to 79\% for the Java -{$>$} Rust pair with greedy decoding. We extend previous test sets for code translation, by adding hundreds of Go and Rust functions. Additionally, we train models with high performance on the problem of IR decompilation, generating programming source code from IR, and study using IRs as intermediary pivot for translation.},
  archiveprefix = {arXiv}
}

@inproceedings{taebAXNavReplayingAccessibility2024,
  title = {{{AXNav}}: {{Replaying Accessibility Tests}} from {{Natural Language}}},
  shorttitle = {{{AXNav}}},
  booktitle = {Proceedings of the 2024 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Taeb, Maryam and Swearngin, Amanda and Schoop, Eldon and Cheng, Ruijia and Jiang, Yue and Nichols, Jeffrey},
  year = {2024},
  month = may,
  series = {{{CHI}} '24},
  pages = {1--16},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3613904.3642777},
  urldate = {2025-02-25},
  abstract = {Developers and quality assurance testers often rely on manual testing to test accessibility features throughout the product lifecycle. Unfortunately, manual testing can be tedious, often has an overwhelming scope, and can be difficult to schedule amongst other development milestones. Recently, Large Language Models (LLMs) have been used for a variety of tasks including automation of UIs. However, to our knowledge, no one has yet explored the use of LLMs in controlling assistive technologies for the purposes of supporting accessibility testing. In this paper, we explore the requirements of a natural language based accessibility testing workflow, starting with a formative study. From this we build a system that takes a manual accessibility test instruction in natural language (e.g., ``Search for a show in VoiceOver'') as input and uses an LLM combined with pixel-based UI Understanding models to execute the test and produce a chaptered, navigable video. In each video, to help QA testers, we apply heuristics to detect and flag accessibility issues (e.g., Text size not increasing with Large Text enabled, VoiceOver navigation loops). We evaluate this system through a 10-participant user study with accessibility QA professionals who indicated that the tool would be very useful in their current work and performed tests similarly to how they would manually test the features. The study also reveals insights for future work on using LLMs for accessibility testing.},
  isbn = {979-8-4007-0330-0}
}

@inproceedings{takatsuPersonalizedExtractiveSummarization2021,
  title = {Personalized {{Extractive Summarization}} for a {{News Dialogue System}}},
  booktitle = {2021 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  author = {Takatsu, Hiroaki and Okuda, Mayu and Matsuyama, Yoichi and Honda, Hiroshi and Fujie, Shinya and Kobayashi, Tetsunori},
  year = {2021},
  month = jan,
  pages = {1044--1051},
  doi = {10.1109/SLT48900.2021.9383568},
  abstract = {In modern society, people's interests and preferences are diversifying. Along with this, the demand for personalized summarization technology is increasing. In this study, we propose a method for generating summaries tailored to each user's interests using profile features obtained from questionnaires administered to users of our spoken-dialogue news delivery system. We propose a method that collects and uses the obtained user profile features to generate a summary tailored to each user's interests, specifically, the sentence features obtained by BERT and user profile features obtained from the questionnaire result. In addition, we propose a method for extracting sentences by solving an integer linear programming problem that considers redundancy and context coherence, using the degree of interest in sentences estimated by the model. The results of our experiments confirmed that summaries generated based on the degree of interest in sentences estimated using user profile information can transmit information more efficiently than summaries based solely on the importance of sentences.}
}

@article{Talk2CareFacilitatingAsynchronous2024,
  title = {{{Talk2Care}}: {{Facilitating Asynchronous Patient-Provider Communication}} with {{Large-Language-Model}}},
  year = {2024},
  langid = {english}
}

@article{tambeIntelligentAgentsInteractive1995,
  title = {Intelligent {{Agents}} for {{Interactive Simulation Environments}}},
  author = {Tambe, Milind and Johnson, W. Lewis and Jones, Randolph M. and Koss, Frank and Laird, John E. and Rosenbloom, Paul S. and Schwamb, Karl},
  year = {1995},
  month = mar,
  journal = {AI Magazine},
  volume = {16},
  number = {1},
  pages = {15--15},
  issn = {2371-9621},
  doi = {10.1609/aimag.v16i1.1121},
  urldate = {2024-08-27},
  abstract = {Interactive simulation environments constitute one of today's promising emerging technologies, with applications in areas such as education, manufacturing, entertainment, and training. These environments are also rich domains for building and investigating intelligent automated agents, with requirements for the integration of a variety of agent capabilities but without the costs and demands of low-level perceptual processing or robotic control. Our project is aimed at developing humanlike, intelligent agents that can interact with each other, as well as with humans, in such virtual environments. Our current target is intelligent automated pilots for battlefield-simulation environments. These dynamic, interactive, multiagent environments pose interesting challenges for research on specialized agent capabilities as well as on the integration of these capabilities in the development of "complete" pilot agents. We are addressing these challenges through development of a pilot agent, called TacAir-Soar, within the Soar architecture. This article provides an overview of this domain and project by analyzing the challenges that automated pilots face in battlefield simulations, describing how TacAir-Soar is successfully able to address many of them -- TacAir-Soar pilots have already successfully participated in constrained air-combat simulations against expert human pilots -- and discussing the issues involved in resolving the remaining research challenges.},
  copyright = {Copyright (c)},
  langid = {english}
}

@article{tangRapidlyBootstrappingQuestion2020,
  title = {Rapidly {{Bootstrapping}} a {{Question Answering Dataset}} for {{COVID-19}}},
  author = {Tang, Raphael and Nogueira, Rodrigo and Zhang, Edwin and Gupta, Nikhil and Cam, Phuong and Cho, Kyunghyun and Lin, Jimmy},
  year = {2020},
  month = apr,
  journal = {arXiv:2004.11339 [cs]},
  eprint = {2004.11339},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2004.11339},
  urldate = {2021-12-25},
  abstract = {We present CovidQA, the beginnings of a question answering dataset specifically designed for COVID-19, built by hand from knowledge gathered from Kaggle's COVID-19 Open Research Dataset Challenge. To our knowledge, this is the first publicly available resource of its type, and intended as a stopgap measure for guiding research until more substantial evaluation resources become available. While this dataset, comprising 124 question-article pairs as of the present version 0.1 release, does not have sufficient examples for supervised machine learning, we believe that it can be helpful for evaluating the zero-shot or transfer capabilities of existing models on topics specifically related to COVID-19. This paper describes our methodology for constructing the dataset and presents the effectiveness of a number of baselines, including term-based techniques and various transformer-based models. The dataset is available at http://covidqa.ai/},
  archiveprefix = {arXiv},
  langid = {english},
  annotation = {00047}
}

@misc{TemplateDesignerDocumentation,
  title = {Template {{Designer Documentation}} --- {{Jinja Documentation}} (3.1.x)},
  url = {https://jinja.palletsprojects.com/en/stable/templates/},
  urldate = {2024-11-19}
}

@article{thomasGeneralInductiveApproach2006,
  title = {A General Inductive Approach for Analyzing Qualitative Evaluation Data},
  author = {Thomas, David R.},
  year = {2006},
  journal = {American Journal of Evaluation},
  volume = {27},
  number = {2},
  eprint = {https://doi.org/10.1177/1098214005283748},
  pages = {237--246},
  doi = {10.1177/1098214005283748},
  abstract = {A general inductive approach for analysis of qualitative evaluation data is described. The purposes for using an inductive approach are to (a) condense raw textual data into a brief, summary format; (b) establish clear links between the evaluation or research objectives and the summary findings derived from the raw data; and (c) develop a framework of the underlying structure of experiences or processes that are evident in the raw data. The general inductive approach provides an easily used and systematic set of procedures for analyzing qualitative data that can produce reliable and valid findings. Although the general inductive approach is not as strong as some other analytic strategies for theory or model development, it does provide a simple, straightforward approach for deriving findings in the context of focused evaluation questions. Many evaluators are likely to find using a general inductive approach less complicated than using other approaches to qualitative data analysis.}
}

@inproceedings{thompsonHereThereAnywhere2004,
  title = {Here, There, Anywhere: Remote Usability Testing That Works},
  shorttitle = {Here, There, Anywhere},
  booktitle = {Proceedings of the 5th Conference on {{Information}} Technology Education},
  author = {Thompson, Katherine E. and Rozanski, Evelyn P. and Haake, Anne R.},
  year = {2004},
  month = oct,
  series = {{{CITC5}} '04},
  pages = {132--137},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1029533.1029567},
  urldate = {2024-09-13},
  abstract = {Usability testing is regarded as a key element in user-centered design. Several studies from the Standish Group have shown that usability testing increases the chance of developing usable software. Companies are faced with many challenges: their customers demand usable products at reasonable costs and the customer base is distributed and diverse. Unfortunately, usability testing is often perceived as impractical due to the remote and distributed location of users, limited access to representative users, or a work context that is difficult to reproduce in a laboratory setting. Additionally, for some companies, the cost of transporting users or developers to remote locations can be prohibitive. Interest in remote usability testing has grown in response to these concerns.As the user advocate, the Information Technology student needs to be provided not only with hands-on experience in usability testing, but also with its nuances. These include an understanding of the wide range of formats and options available such as remote usability testing. Moreover, a remote format may be a solution for universities that do not have the resources to set up formal laboratories. There is a need to understand how best to facilitate remote testing in a classroom environment.The focus of our study was to identify appropriate tools and define methodologies for efficient and effective remote testing environments. We investigated commercial tools to determine their usefulness and cost-effectiveness for the classroom and then conducted an empirical study to compare traditional and remote usability testing of a web site using one of these tools. This paper will report on advantages and disadvantages of various remote testing tools, modifications to procedures and protocols of traditional testing, and the project findings, including usability problem identification, which establish the effectiveness of remote usability testing. Recommendations will be made for providing a credible environment for remote testing.},
  isbn = {978-1-58113-936-5}
}

@article{tianImprovingBiomedicalNamed2020,
  title = {Improving Biomedical Named Entity Recognition with Syntactic Information},
  author = {Tian, Yuanhe and Shen, Wang and Song, Yan and Xia, Fei and He, Min and Li, Kenli},
  year = {2020},
  month = dec,
  journal = {BMC Bioinformatics},
  volume = {21},
  number = {1},
  pages = {539},
  issn = {1471-2105},
  doi = {10.1186/s12859-020-03834-6},
  urldate = {2022-04-13},
  abstract = {Background:{\enspace} Biomedical named entity recognition (BioNER) is an important task for understanding biomedical texts, which can be challenging due to the lack of large-scale labeled training data and domain knowledge. To address the challenge, in addition to using powerful encoders (e.g., biLSTM and BioBERT), one possible method is to leverage extra knowledge that is easy to obtain. Previous studies have shown that auto-processed syntactic information can be a useful resource to improve model performance, but their approaches are limited to directly concatenating the embeddings of syntactic information to the input word embeddings. Therefore, such syntactic information is leveraged in an inflexible way, where inaccurate one may hurt model performance. Results:{\enspace} In this paper, we propose BioKMNER, a BioNER model for biomedical texts with key-value memory networks (KVMN) to incorporate auto-processed syntactic information. We evaluate BioKMNER on six English biomedical datasets, where our method with KVMN outperforms the strong baseline method, namely, BioBERT, from the previous study on all datasets. Specifically, the F1 scores of our best performing model are 85.29\% on BC2GM, 77.83\% on JNLPBA, 94.22\% on BC5CDR-chemical, 90.08\% on NCBI-disease, 89.24\% on LINNAEUS, and 76.33\% on Species-800, where state-of-theart performance is obtained on four of them (i.e., BC2GM, BC5CDR-chemical, NCBIdisease, and Species-800). Conclusion:{\enspace} The experimental results on six English benchmark datasets demonstrate that auto-processed syntactic information can be a useful resource for BioNER and our method with KVMN can appropriately leverage such information to improve model performance.},
  langid = {english}
}

@inproceedings{TimeLLMTimeSeries2023,
  title = {Time-{{LLM}}: {{Time Series Forecasting}} by {{Reprogramming Large Language Models}}},
  shorttitle = {Time-{{LLM}}},
  booktitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  year = {2023},
  month = oct,
  url = {https://openreview.net/forum?id=Unb5CVPtae},
  urldate = {2024-01-16},
  abstract = {Time series forecasting holds significant importance in many real-world dynamic systems and has been extensively studied. Unlike natural language process (NLP) and computer vision (CV), where a single large model can tackle multiple tasks, models for time series forecasting are often specialized, necessitating distinct designs for different tasks and applications. While pre-trained foundation models have made impressive strides in NLP and CV, their development in time series domains has been constrained by data sparsity. Recent studies have revealed that large language models (LLMs) possess robust pattern recognition and reasoning abilities over complex sequences of tokens. However, the challenge remains in effectively aligning the modalities of time series data and natural language to leverage these capabilities. In this work, we present Time-LLM, a reprogramming framework to repurpose LLMs for general time series forecasting with the backbone language models kept intact. We begin by reprogramming the input time series with text prototypes before feeding it into the frozen LLM to align the two modalities. To augment the LLM's ability to reason with time series data, we propose Prompt-as-Prefix (PaP), which enriches the input context and directs the transformation of reprogrammed input patches. The transformed time series patches from the LLM are finally projected to obtain the forecasts. Our comprehensive evaluations demonstrate that {\textbackslash}method is a powerful time series learner that outperforms state-of-the-art, specialized forecasting models. Moreover, Time-LLM excels in both few-shot and zero-shot learning scenarios.},
  langid = {english}
}

@misc{tornbergChatGPT4OutperformsExperts2023,
  title = {{{ChatGPT-4 Outperforms Experts}} and {{Crowd Workers}} in {{Annotating Political Twitter Messages}} with {{Zero-Shot Learning}}},
  author = {T{\"o}rnberg, Petter},
  year = {2023},
  month = apr,
  number = {arXiv:2304.06588},
  eprint = {2304.06588},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.06588},
  urldate = {2023-10-26},
  abstract = {This paper assesses the accuracy, reliability and bias of the Large Language Model (LLM) ChatGPT-4 on the text analysis task of classifying the political affiliation of a Twitter poster based on the content of a tweet. The LLM is compared to manual annotation by both expert classifiers and crowd workers, generally considered the gold standard for such tasks. We use Twitter messages from United States politicians during the 2020 election, providing a ground truth against which to measure accuracy. The paper finds that ChatGPT-4 has achieves higher accuracy, higher reliability, and equal or lower bias than the human classifiers. The LLM is able to correctly annotate messages that require reasoning on the basis of contextual knowledge, and inferences around the author's intentions - traditionally seen as uniquely human abilities. These findings suggest that LLM will have substantial impact on the use of textual data in the social sciences, by enabling interpretive research at a scale.},
  archiveprefix = {arXiv}
}

@article{toshevskaReviewTextStyle2022,
  title = {A {{Review}} of {{Text Style Transfer}} Using {{Deep Learning}}},
  author = {Toshevska, Martina and Gievska, Sonja},
  year = {2022},
  month = oct,
  journal = {IEEE Transactions on Artificial Intelligence},
  volume = {3},
  number = {5},
  eprint = {2109.15144},
  primaryclass = {cs},
  pages = {669--684},
  issn = {2691-4581},
  doi = {10.1109/TAI.2021.3115992},
  urldate = {2023-01-17},
  abstract = {Style is an integral component of a sentence indicated by the choice of words a person makes. Different people have different ways of expressing themselves, however, they adjust their speaking and writing style to a social context, an audience, an interlocutor or the formality of an occasion. Text style transfer is defined as a task of adapting and/or changing the stylistic manner in which a sentence is written, while preserving the meaning of the original sentence. A systematic review of text style transfer methodologies using deep learning is presented in this paper. We point out the technological advances in deep neural networks that have been the driving force behind current successes in the fields of natural language understanding and generation. The review is structured around two key stages in the text style transfer process, namely, representation learning and sentence generation in a new style. The discussion highlights the commonalities and differences between proposed solutions as well as challenges and opportunities that are expected to direct and foster further research in the field.},
  archiveprefix = {arXiv}
}

@misc{touvronLLaMAOpenEfficient2023,
  title = {{{LLaMA}}: {{Open}} and {{Efficient Foundation Language Models}}},
  shorttitle = {{{LLaMA}}},
  author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  year = {2023},
  month = feb,
  number = {arXiv:2302.13971},
  eprint = {2302.13971},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.13971},
  urldate = {2023-03-14},
  abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
  archiveprefix = {arXiv}
}

@misc{touvronLlamaOpenFoundation2023,
  title = {Llama 2: {{Open Foundation}} and {{Fine-Tuned Chat Models}}},
  shorttitle = {Llama 2},
  author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
  year = {2023},
  month = jul,
  number = {arXiv:2307.09288},
  eprint = {2307.09288},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2307.09288},
  urldate = {2023-10-16},
  abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
  archiveprefix = {arXiv}
}

@misc{tranBioInstructInstructionTuning2023,
  title = {{{BioInstruct}}: {{Instruction Tuning}} of {{Large Language Models}} for {{Biomedical Natural Language Processing}}},
  shorttitle = {{{BioInstruct}}},
  author = {Tran, Hieu and Yang, Zhichao and Yao, Zonghai and Yu, Hong},
  year = {2023},
  month = nov,
  number = {arXiv:2310.19975},
  eprint = {2310.19975},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2310.19975},
  urldate = {2024-01-20},
  abstract = {To enhance the performance of large language models (LLMs) in biomedical natural language processing (BioNLP) by introducing a domain-specific instruction dataset and examining its impact when combined with multi-task learning principles. We created the BioInstruct, comprising 25,005 instructions to instruction-tune LLMs(LLaMA 1 \& 2, 7B \& 13B version). The instructions were created by prompting the GPT-4 language model with three-seed samples randomly drawn from an 80 human curated instructions. We employed Low-Rank Adaptation(LoRA) for parameter-efficient fine-tuning. We then evaluated these instruction-tuned LLMs on several BioNLP tasks, which can be grouped into three major categories: question answering(QA), information extraction(IE), and text generation(GEN). We also examined whether categories(e.g., QA, IE, and generation) of instructions impact model performance. Comparing with LLMs without instruction-tuned, our instruction-tuned LLMs demonstrated marked performance gains: 17.3\% in QA, 5.7\% in IE, and 96\% in Generation tasks. Our 7B-parameter instruction-tuned LLaMA 1 model was competitive or even surpassed other LLMs in the biomedical domain that were also fine-tuned from LLaMA 1 with vast domain-specific data or a variety of tasks. Our results also show that the performance gain is significantly higher when instruction fine-tuning is conducted with closely related tasks. Our findings align with the observations of multi-task learning, suggesting the synergies between two tasks. The BioInstruct dataset serves as a valuable resource and instruction tuned LLMs lead to the best performing BioNLP applications.},
  archiveprefix = {arXiv}
}

@misc{triantafillouMetaDatasetDatasetDatasets2020,
  title = {Meta-{{Dataset}}: {{A Dataset}} of {{Datasets}} for {{Learning}} to {{Learn}} from {{Few Examples}}},
  shorttitle = {Meta-{{Dataset}}},
  author = {Triantafillou, Eleni and Zhu, Tyler and Dumoulin, Vincent and Lamblin, Pascal and Evci, Utku and Xu, Kelvin and Goroshin, Ross and Gelada, Carles and Swersky, Kevin and Manzagol, Pierre-Antoine and Larochelle, Hugo},
  year = {2020},
  month = apr,
  number = {arXiv:1903.03096},
  eprint = {1903.03096},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1903.03096},
  urldate = {2022-06-29},
  abstract = {Few-shot classification refers to learning a classifier for new classes given only a few examples. While a plethora of models have emerged to tackle it, we find the procedure and datasets that are used to assess their progress lacking. To address this limitation, we propose Meta-Dataset: a new benchmark for training and evaluating models that is large-scale, consists of diverse datasets, and presents more realistic tasks. We experiment with popular baselines and meta-learners on Meta-Dataset, along with a competitive method that we propose. We analyze performance as a function of various characteristics of test tasks and examine the models' ability to leverage diverse training sources for improving their generalization. We also propose a new set of baselines for quantifying the benefit of meta-learning in Meta-Dataset. Our extensive experimentation has uncovered important research challenges and we hope to inspire work in these directions.},
  archiveprefix = {arXiv}
}

@article{trischlerNewsQAMachineComprehension2017,
  title = {{{NewsQA}}: {{A Machine Comprehension Dataset}}},
  shorttitle = {{{NewsQA}}},
  author = {Trischler, Adam and Wang, Tong and Yuan, Xingdi and Harris, Justin and Sordoni, Alessandro and Bachman, Philip and Suleman, Kaheer},
  year = {2017},
  month = feb,
  journal = {arXiv:1611.09830 [cs]},
  eprint = {1611.09830},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1611.09830},
  urldate = {2021-12-25},
  abstract = {We present NewsQA, a challenging machine comprehension dataset of over 100,000 human-generated question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting of spans of text from the corresponding articles. We collect this dataset through a four-stage process designed to solicit exploratory questions that require reasoning. A thorough analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing textual entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (0.198 in F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available at https://datasets.maluuba.com/NewsQA.},
  archiveprefix = {arXiv},
  langid = {english},
  annotation = {00520}
}

@misc{Troubleshooting_translator_issuesZoteroDocumentation,
  title = {Troubleshooting\_translator\_issues [{{Zotero Documentation}}]},
  url = {https://www.zotero.org/support/troubleshooting_translator_issues},
  urldate = {2025-03-31}
}

@misc{tsaiLeveragingLLMReasoning2024,
  title = {Leveraging {{LLM Reasoning Enhances Personalized Recommender Systems}}},
  author = {Tsai, Alicia Y. and Kraft, Adam and Jin, Long and Cai, Chenwei and Hosseini, Anahita and Xu, Taibai and Zhang, Zemin and Hong, Lichan and Chi, Ed H. and Yi, Xinyang},
  year = {2024},
  month = jul,
  number = {arXiv:2408.00802},
  eprint = {2408.00802},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.00802},
  urldate = {2024-10-11},
  abstract = {Recent advancements have showcased the potential of Large Language Models (LLMs) in executing reasoning tasks, particularly facilitated by Chain-of-Thought (CoT) prompting. While tasks like arithmetic reasoning involve clear, definitive answers and logical chains of thought, the application of LLM reasoning in recommendation systems (RecSys) presents a distinct challenge. RecSys tasks revolve around subjectivity and personalized preferences, an under-explored domain in utilizing LLMs' reasoning capabilities. Our study explores several aspects to better understand reasoning for RecSys and demonstrate how task quality improves by utilizing LLM reasoning in both zero-shot and finetuning settings. Additionally, we propose RecSAVER (Recommender Systems Automatic Verification and Evaluation of Reasoning) to automatically assess the quality of LLM reasoning responses without the requirement of curated gold references or human raters. We show that our framework aligns with real human judgment on the coherence and faithfulness of reasoning responses. Overall, our work shows that incorporating reasoning into RecSys can improve personalized tasks, paving the way for further advancements in recommender system methodologies.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning}
}

@article{tsatsaronisOverviewBIOASQLargescale2015,
  title = {An Overview of the {{BIOASQ}} Large-Scale Biomedical Semantic Indexing and Question Answering Competition},
  author = {Tsatsaronis, George and Balikas, Georgios and Malakasiotis, Prodromos and Partalas, Ioannis and Zschunke, Matthias and Alvers, Michael R. and Weissenborn, Dirk and Krithara, Anastasia and Petridis, Sergios and Polychronopoulos, Dimitris and Almirantis, Yannis and Pavlopoulos, John and Baskiotis, Nicolas and Gallinari, Patrick and Arti{\'e}res, Thierry and Ngomo, Axel-Cyrille Ngonga and Heino, Norman and Gaussier, Eric and {Barrio-Alvers}, Liliana and Schroeder, Michael and Androutsopoulos, Ion and Paliouras, Georgios},
  year = {2015},
  month = apr,
  journal = {BMC Bioinformatics},
  volume = {16},
  number = {1},
  pages = {138},
  issn = {1471-2105},
  doi = {10.1186/s12859-015-0564-6},
  urldate = {2021-12-25},
  abstract = {This article provides an overview of the first BioASQ challenge, a competition on large-scale biomedical semantic indexing and question answering (QA), which took place between March and September 2013. BioASQ assesses the ability of systems to semantically index very large numbers of biomedical scientific articles, and to return concise and user-understandable answers to given natural language questions by combining information from biomedical articles and ontologies.},
  langid = {english},
  annotation = {00354}
}

@inproceedings{tullis2002empirical,
  title = {An Empirical Comparison of Lab and Remote Usability Testing of Web Sites},
  booktitle = {Usability Professionals Association Conference},
  author = {Tullis, Tom and Fleischman, Stan and McNulty, Michelle and Cianchette, Carrie and Bergel, Margaret},
  year = {2002},
  publisher = {Usability Professionals Association Conference Orlando, FL},
  keywords = {No DOI found}
}

@inproceedings{turcanDreadditRedditDataset2019,
  title = {Dreaddit: {{A Reddit Dataset}} for {{Stress Analysis}} in {{Social Media}}},
  shorttitle = {Dreaddit},
  booktitle = {Proceedings of the {{Tenth International Workshop}} on {{Health Text Mining}} and {{Information Analysis}} ({{LOUHI}} 2019)},
  author = {Turcan, Elsbeth and McKeown, Kathy},
  editor = {Holderness, Eben and Jimeno Yepes, Antonio and Lavelli, Alberto and Minard, Anne-Lyse and Pustejovsky, James and Rinaldi, Fabio},
  year = {2019},
  month = nov,
  pages = {97--107},
  publisher = {Association for Computational Linguistics},
  address = {Hong Kong},
  doi = {10.18653/v1/D19-6213},
  urldate = {2024-06-16},
  abstract = {Stress is a nigh-universal human experience, particularly in the online world. While stress can be a motivator, too much stress is associated with many negative health outcomes, making its identification useful across a range of domains. However, existing computational research typically only studies stress in domains such as speech, or in short genres such as Twitter. We present Dreaddit, a new text corpus of lengthy multi-domain social media data for the identification of stress. Our dataset consists of 190K posts from five different categories of Reddit communities; we additionally label 3.5K total segments taken from 3K posts using Amazon Mechanical Turk. We present preliminary supervised learning methods for identifying stress, both neural and traditional, and analyze the complexity and diversity of the data and characteristics of each category.}
}

@article{turingCOMPUTINGMACHINERYINTELLIGENCE1950a,
  title = {I.---{{COMPUTING MACHINERY AND INTELLIGENCE}}},
  author = {Turing, A. M.},
  year = {1950},
  month = oct,
  journal = {Mind, New Series},
  volume = {LIX},
  number = {236},
  pages = {433--460},
  issn = {1460-2113, 0026-4423},
  doi = {10.1093/mind/LIX.236.433},
  abstract = {I propose to consider the question, ``Can machines think?''{$\clubsuit$} This should begin with definitions of the meaning of the terms ``machine'' and ``think''. The definitions might be framed so as to reflect so far as possible the normal use of the words, but this attitude is dangerous. If the meaning of the words ``machine'' and ``think'' are to be found by examining how they are commonly used it is difficult to escape the conclusion that the meaning and the answer to the question, ``Can machines think?'' is to be sought in a statistical survey such as a Gallup poll.},
  langid = {english}
}

@misc{ulcarTrainingDatasetDictionary2021,
  title = {Training Dataset and Dictionary Sizes Matter in {{BERT}} Models: The Case of {{Baltic}} Languages},
  shorttitle = {Training Dataset and Dictionary Sizes Matter in {{BERT}} Models},
  author = {Ul{\v c}ar, Matej and {Robnik-{\v S}ikonja}, Marko},
  year = {2021},
  month = dec,
  number = {arXiv:2112.10553},
  eprint = {2112.10553},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.10553},
  urldate = {2022-08-09},
  abstract = {Large pretrained masked language models have become state-of-the-art solutions for many NLP problems. While studies have shown that monolingual models produce better results than multilingual models, the training datasets must be sufficiently large. We trained a trilingual LitLat BERT-like model for Lithuanian, Latvian, and English, and a monolingual Est-RoBERTa model for Estonian. We evaluate their performance on four downstream tasks: named entity recognition, dependency parsing, part-of-speech tagging, and word analogy. To analyze the importance of focusing on a single language and the importance of a large training set, we compare created models with existing monolingual and multilingual BERT models for Estonian, Latvian, and Lithuanian. The results show that the newly created LitLat BERT and Est-RoBERTa models improve the results of existing models on all tested tasks in most situations.},
  archiveprefix = {arXiv}
}

@misc{vandewaterAnotherICUBenchmark2023,
  title = {Yet {{Another ICU Benchmark}}: {{A Flexible Multi-Center Framework}} for {{Clinical ML}}},
  shorttitle = {Yet {{Another ICU Benchmark}}},
  author = {{van de Water}, Robin and Schmidt, Hendrik and Elbers, Paul and Thoral, Patrick and Arnrich, Bert and Rockenschaub, Patrick},
  year = {2023},
  month = aug,
  number = {arXiv:2306.05109},
  eprint = {2306.05109},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.05109},
  urldate = {2024-01-14},
  abstract = {Medical applications of machine learning (ML) have experienced a surge in popularity in recent years. The intensive care unit (ICU) is a natural habitat for ML given the abundance of available data from electronic health records. Models have been proposed to address numerous ICU prediction tasks like the early detection of complications. While authors frequently report state-of-the-art performance, it is challenging to verify claims of superiority. Datasets and code are not always published, and cohort definitions, preprocessing pipelines, and training setups are difficult to reproduce. This work introduces Yet Another ICU Benchmark (YAIB), a modular framework that allows researchers to define reproducible and comparable clinical ML experiments; we offer an end-to-end solution from cohort definition to model evaluation. The framework natively supports most open-access ICU datasets (MIMIC III/IV, eICU, HiRID, AUMCdb) and is easily adaptable to future ICU datasets. Combined with a transparent preprocessing pipeline and extensible training code for multiple ML and deep learning models, YAIB enables unified model development. Our benchmark comes with five predefined established prediction tasks (mortality, acute kidney injury, sepsis, kidney function, and length of stay) developed in collaboration with clinicians. Adding further tasks is straightforward by design. Using YAIB, we demonstrate that the choice of dataset, cohort definition, and preprocessing have a major impact on the prediction performance - often more so than model class - indicating an urgent need for YAIB as a holistic benchmarking tool. We provide our work to the clinical ML community to accelerate method development and enable real-world clinical implementations. Software Repository: https://github.com/rvandewater/YAIB.},
  archiveprefix = {arXiv}
}

@inproceedings{vaswaniAttentionAllYou2017,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
  urldate = {2022-01-20},
  annotation = {34483}
}

@misc{vatsSurveyHumanAITeaming2024,
  title = {A {{Survey}} on {{Human-AI Teaming}} with {{Large Pre-Trained Models}}},
  author = {Vats, Vanshika and Nizam, Marzia Binta and Liu, Minghao and Wang, Ziyuan and Ho, Richard and Prasad, Mohnish Sai and Titterton, Vincent and Malreddy, Sai Venkat and Aggarwal, Riya and Xu, Yanwen and Ding, Lei and Mehta, Jay and Grinnell, Nathan and Liu, Li and Zhong, Sijia and Gandamani, Devanathan Nallur and Tang, Xinyi and Ghosalkar, Rohan and Shen, Celeste and Shen, Rachel and Hussain, Nafisa and Ravichandran, Kesav and Davis, James},
  year = {2024},
  month = mar,
  number = {arXiv:2403.04931},
  eprint = {2403.04931},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2403.04931},
  urldate = {2024-03-13},
  abstract = {In the rapidly evolving landscape of artificial intelligence (AI), the collaboration between human intelligence and AI systems, known as Human-AI (HAI) Teaming, has emerged as a cornerstone for advancing problem-solving and decision-making processes. The advent of Large Pre-trained Models (LPtM) has significantly transformed this landscape, offering unprecedented capabilities by leveraging vast amounts of data to understand and predict complex patterns. This paper surveys the pivotal integration of LPtMs with HAI, emphasizing how these models enhance collaborative intelligence beyond traditional approaches. It examines the synergistic potential of LPtMs in augmenting human capabilities, discussing this collaboration for AI model improvements, effective teaming, ethical considerations, and their broad applied implications in various sectors. Through this exploration, the study sheds light on the transformative impact of LPtM-enhanced HAI Teaming, providing insights for future research, policy development, and strategic implementations aimed at harnessing the full potential of this collaboration for research and societal benefit.},
  archiveprefix = {arXiv}
}

@inproceedings{vaudauxPretrainedLanguageModels2023,
  title = {Pretrained {{Language Models}} v. {{Court Ruling Predictions}}: {{A Case Study}} on a {{Small Dataset}} of {{French Court}} of {{Appeal Rulings}}},
  shorttitle = {Pretrained {{Language Models}} v. {{Court Ruling Predictions}}},
  booktitle = {Proceedings of the {{Natural Legal Language Processing Workshop}} 2023},
  author = {Vaudaux, Olivia and Bazzoli, Caroline and Coavoux, Maximin and Vial, G{\'e}raldine and Verg{\`e}s, {\'E}tienne},
  editor = {{Preo{\textbackslash}textcommabelowtiuc-Pietro}, Daniel and Goanta, Catalina and Chalkidis, Ilias and Barrett, Leslie and Spanakis, Gerasimos (Jerry) and Aletras, Nikolaos},
  year = {2023},
  month = dec,
  pages = {38--43},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  url = {https://aclanthology.org/2023.nllp-1.5},
  urldate = {2023-12-07},
  abstract = {NLP systems are increasingly used in the law domain, either by legal institutions or by the industry. As a result there is a pressing need to characterize their strengths and weaknesses and understand their inner workings. This article presents a case study on the task of judicial decision prediction, on a small dataset from French Courts of Appeal. Specifically, our dataset of around 1000 decisions is about the habitual place of residency of children from divorced parents. The task consists in predicting, from the facts and reasons of the documents, whether the court rules that children should live with their mother or their father. Instead of feeding the whole document to a classifier, we carefully construct the dataset to make sure that the input to the classifier does not contain any `spoilers' (it is often the case in court rulings that information all along the document mentions the final decision). Our results are mostly negative: even classifiers based on French pretrained language models (Flaubert, JuriBERT) do not classify the decisions with a reasonable accuracy. However, they can extract the decision when it is part of the input. With regards to these results, we argue that there is a strong caveat when constructing legal NLP datasets automatically.}
}

@misc{vikramCanLargeLanguage2023,
  title = {Can {{Large Language Models Write Good Property-Based Tests}}?},
  author = {Vikram, Vasudev and Lemieux, Caroline and Padhye, Rohan},
  year = {2023},
  month = jul,
  number = {arXiv:2307.04346},
  eprint = {2307.04346},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2307.04346},
  urldate = {2024-01-16},
  abstract = {Property-based testing (PBT), while an established technique in the software testing research community, is still relatively underused in real-world software. Pain points in writing property-based tests include implementing diverse random input generators and thinking of meaningful properties to test. Developers, however, are more amenable to writing documentation; plenty of library API documentation is available and can be used as natural language specifications for property-based tests. As large language models (LLMs) have recently shown promise in a variety of coding tasks, we explore the potential of using LLMs to synthesize property-based tests. We call our approach PBT-GPT, and propose three different strategies of prompting the LLM for PBT. We characterize various failure modes of PBT-GPT and detail an evaluation methodology for automatically synthesized property-based tests. PBT-GPT achieves promising results in our preliminary studies on sample Python library APIs in \${\textbackslash}texttt\{numpy\}\$, \${\textbackslash}texttt\{networkx\}\$, and \${\textbackslash}texttt\{datetime\}\$.},
  archiveprefix = {arXiv}
}

@misc{ViteApp,
  title = {Vite {{App}}},
  url = {http://localhost:5173/},
  urldate = {2025-02-26}
}

@article{wallinMethodEmpathybasedStories2019,
  title = {The Method of Empathy-Based Stories},
  author = {Wallin, Anna and {Koro-Ljungberg}, Mirka and Eskola, Jari},
  year = {2019},
  month = oct,
  journal = {International Journal of Research \& Method in Education},
  publisher = {Routledge},
  issn = {1743-727X},
  url = {https://www.tandfonline.com/doi/abs/10.1080/1743727X.2018.1533937},
  urldate = {2025-04-10},
  abstract = {This paper focuses on the processes of the method of empathy-based stories (MEBS) and illustrates the ways in which MEBS facilitates storytelling and narration. In MEBS, the participants narrate st...},
  copyright = {{\copyright} 2018 Informa UK Limited, trading as Taylor \& Francis Group},
  langid = {english}
}

@misc{wangAgentAAutomatedScalable2025,
  title = {{{AgentA}}/{{B}}: {{Automated}} and {{Scalable Web A}}/{{BTesting}} with {{Interactive LLM Agents}}},
  shorttitle = {{{AgentA}}/{{B}}},
  author = {Wang, Dakuo and Hsu, Ting-Yao and Lu, Yuxuan and Gu, Hansu and Cui, Limeng and Xie, Yaochen and Headean, William and Yao, Bingsheng and Veeragouni, Akash and Liu, Jiapeng and Nag, Sreyashi and Wang, Jessie},
  year = {2025},
  month = apr,
  number = {arXiv:2504.09723},
  eprint = {2504.09723},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.09723},
  urldate = {2025-04-30},
  abstract = {A/B testing experiment is a widely adopted method for evaluating UI/UX design decisions in modern web applications. Yet, traditional A/B testing remains constrained by its dependence on the large-scale and live traffic of human participants, and the long time of waiting for the testing result. Through formative interviews with six experienced industry practitioners, we identified critical bottlenecks in current A/B testing workflows. In response, we present AgentA/B, a novel system that leverages Large Language Model-based autonomous agents (LLM Agents) to automatically simulate user interaction behaviors with real webpages. AgentA/B enables scalable deployment of LLM agents with diverse personas, each capable of navigating the dynamic webpage and interactively executing multi-step interactions like search, clicking, filtering, and purchasing. In a demonstrative controlled experiment, we employ AgentA/B to simulate a between-subject A/B testing with 1,000 LLM agents Amazon.com, and compare agent behaviors with real human shopping behaviors at a scale. Our findings suggest AgentA/B can emulate human-like behavior patterns.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Human-Computer Interaction}
}

@inproceedings{wangAutomatedConcatenationEmbeddings2021,
  title = {Automated {{Concatenation}} of {{Embeddings}} for {{Structured Prediction}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Wang, Xinyu and Jiang, Yong and Bach, Nguyen and Wang, Tao and Huang, Zhongqiang and Huang, Fei and Tu, Kewei},
  year = {2021},
  month = aug,
  pages = {2643--2660},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.acl-long.206},
  urldate = {2022-07-19},
  abstract = {Pretrained contextualized embeddings are powerful word representations for structured prediction tasks. Recent work found that better word representations can be obtained by concatenating different types of embeddings. However, the selection of embeddings to form the best concatenated representation usually varies depending on the task and the collection of candidate embeddings, and the ever-increasing number of embedding types makes it a more difficult problem. In this paper, we propose Automated Concatenation of Embeddings (ACE) to automate the process of finding better concatenations of embeddings for structured prediction tasks, based on a formulation inspired by recent progress on neural architecture search. Specifically, a controller alternately samples a concatenation of embeddings, according to its current belief of the effectiveness of individual embedding types in consideration for a task, and updates the belief based on a reward. We follow strategies in reinforcement learning to optimize the parameters of the controller and compute the reward based on the accuracy of a task model, which is fed with the sampled concatenation as input and trained on a task dataset. Empirical results on 6 tasks and 21 datasets show that our approach outperforms strong baselines and achieves state-of-the-art performance with fine-tuned embeddings in all the evaluations.}
}

@misc{wangBenchmarkingGeneralizationInContext2022,
  title = {Benchmarking {{Generalization}} via {{In-Context Instructions}} on 1,600+ {{Language Tasks}}},
  author = {Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Arunkumar, Anjana and Ashok, Arjun and Dhanasekaran, Arut Selvan and Naik, Atharva and Stap, David and Pathak, Eshaan and Karamanolakis, Giannis and Lai, Haizhi Gary and Purohit, Ishan and Mondal, Ishani and Anderson, Jacob and Kuznia, Kirby and Doshi, Krima and Patel, Maitreya and Pal, Kuntal Kumar and Moradshahi, Mehrad and Parmar, Mihir and Purohit, Mirali and Varshney, Neeraj and Kaza, Phani Rohitha and Verma, Pulkit and Puri, Ravsehaj Singh and Karia, Rushang and Sampat, Shailaja Keyur and Doshi, Savan and Mishra, Siddhartha and Reddy, Sujan and Patro, Sumanta and Dixit, Tanay and Shen, Xudong and Baral, Chitta and Choi, Yejin and Smith, Noah A. and Hajishirzi, Hannaneh and Khashabi, Daniel},
  year = {2022},
  month = apr,
  number = {arXiv:2204.07705},
  eprint = {2204.07705},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2204.07705},
  urldate = {2022-06-29},
  abstract = {How can we measure the generalization of models to a variety of unseen tasks when provided with their language instructions? To facilitate progress in this goal, we introduce Natural-Instructions v2, a benchmark of 1,600+ diverse language tasks and their expert-written instructions. It covers 70+ distinct task types, such as tagging, in-filling, and rewriting. These tasks are collected with contributions of NLP practitioners in the community and through an iterative peer review process to ensure their quality. With this large and diverse collection of tasks, we are able to rigorously benchmark cross-task generalization of models -- training on a subset of tasks and evaluating on the remaining unseen ones. For instance, we quantify generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances, and model sizes. Based on these insights, we introduce Tk-Instruct, an encoder-decoder Transformer that is trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples) which outperforms existing larger models on our benchmark. We hope this benchmark facilitates future progress toward more general-purpose language understanding models.},
  archiveprefix = {arXiv}
}

@inproceedings{wangCalibratingImbalancedClassifiers2022a,
  title = {Calibrating {{Imbalanced Classifiers}} with {{Focal Loss}}: {{An Empirical Study}}},
  shorttitle = {Calibrating {{Imbalanced Classifiers}} with {{Focal Loss}}},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}: {{Industry Track}}},
  author = {Wang, Cheng and Balazs, Jorge and Szarvas, Gy{\"o}rgy and Ernst, Patrick and Poddar, Lahari and Danchenko, Pavel},
  year = {2022},
  month = dec,
  pages = {145--153},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, UAE},
  url = {https://aclanthology.org/2022.emnlp-industry.14},
  urldate = {2023-06-20},
  abstract = {Imbalanced data distribution is a practical and common challenge in building production-level machine learning (ML) models in industry, where data usually exhibits long-tail distributions. For instance, in virtual AI Assistants, such as Google Assistant, Amazon Alexa and Apple Siri, the ``play music'' or ``set timer'' utterance is exposed to an order of magnitude more traffic than other skills. This can easily cause trained models to overfit to the majority classes, categories or intents, lead to model miscalibration. The uncalibrated models output unreliable (mostly overconfident) predictions, which are at high risk of affecting downstream decision-making systems. In this work, we study the calibration of production models in the industry use-case of predicting product return reason codes in customer service conversations of an online retail store; The returns reasons also exhibit class imbalance.To alleviate the resulting miscalibration in the production ML model, we streamline the model development and deployment using focal loss (CITATION).We empirically show the effectiveness of model training with focal loss in learning better calibrated models, as compared to standard cross-entropy loss. Better calibration, in turn, enables better control of the precision-recall trade-off for the models deployed in production.}
}

@misc{wangCanSmallLanguage2024,
  title = {Can {{Small Language Models}} Be {{Good Reasoners}} for {{Sequential Recommendation}}?},
  author = {Wang, Yuling and Tian, Changxin and Hu, Binbin and Yu, Yanhua and Liu, Ziqi and Zhang, Zhiqiang and Zhou, Jun and Pang, Liang and Wang, Xiao},
  year = {2024},
  month = mar,
  number = {arXiv:2403.04260},
  eprint = {2403.04260},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.04260},
  urldate = {2024-10-01},
  abstract = {Large language models (LLMs) open up new horizons for sequential recommendations, owing to their remarkable language comprehension and generation capabilities. However, there are still numerous challenges that should be addressed to successfully implement sequential recommendations empowered by LLMs. Firstly, user behavior patterns are often complex, and relying solely on one-step reasoning from LLMs may lead to incorrect or task-irrelevant responses. Secondly, the prohibitively resource requirements of LLM (e.g., ChatGPT-175B) are overwhelmingly high and impractical for real sequential recommender systems. In this paper, we propose a novel Step-by-step knowLedge dIstillation fraMework for recommendation (SLIM), paving a promising path for sequential recommenders to enjoy the exceptional reasoning capabilities of LLMs in a "slim" (i.e., resource-efficient) manner. We introduce CoT prompting based on user behavior sequences for the larger teacher model. The rationales generated by the teacher model are then utilized as labels to distill the downstream smaller student model (e.g., LLaMA2-7B). In this way, the student model acquires the step-by-step reasoning capabilities in recommendation tasks. We encode the generated rationales from the student model into a dense vector, which empowers recommendation in both ID-based and ID-agnostic scenarios. Extensive experiments demonstrate the effectiveness of SLIM over state-of-the-art baselines, and further analysis showcasing its ability to generate meaningful recommendation reasoning at affordable costs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning}
}

@misc{wangCoCoSumContextualCode2021,
  title = {{{CoCoSum}}: {{Contextual Code Summarization}} with {{Multi-Relational Graph Neural Network}}},
  shorttitle = {{{CoCoSum}}},
  author = {Wang, Yanlin and Shi, Ensheng and Du, Lun and Yang, Xiaodi and Hu, Yuxuan and Han, Shi and Zhang, Hongyu and Zhang, Dongmei},
  year = {2021},
  month = jul,
  number = {arXiv:2107.01933},
  eprint = {2107.01933},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2107.01933},
  urldate = {2022-07-18},
  abstract = {Source code summaries are short natural language descriptions of code snippets that help developers better understand and maintain source code. There has been a surge of work on automatic code summarization to reduce the burden of writing summaries manually. However, most contemporary approaches mainly leverage the information within the boundary of the method being summarized (i.e., local context), and ignore the broader context that could assist with code summarization. This paper explores two global contexts, namely intra-class and inter-class contexts, and proposes the model CoCoSUM: Contextual Code Summarization with Multi-Relational Graph Neural Networks. CoCoSUM first incorporates class names as the intra-class context to generate the class semantic embeddings. Then, relevant Unified Modeling Language (UML) class diagrams are extracted as inter-class context and are encoded into the class relational embeddings using a novel Multi-Relational Graph Neural Network (MRGNN). Class semantic embeddings and class relational embeddings, together with the outputs from code token encoder and AST encoder, are passed to a decoder armed with a two-level attention mechanism to generate high-quality, context-aware code summaries. We conduct extensive experiments to evaluate our approach and compare it with other automatic code summarization models. The experimental results show that CoCoSUM is effective and outperforms state-of-the-art methods. Our source code and experimental data are available in the supplementary materials and will be made publicly available.},
  archiveprefix = {arXiv}
}

@article{wangCoKEContextualizedKnowledge2020,
  title = {{{CoKE}}: {{Contextualized Knowledge Graph Embedding}}},
  shorttitle = {{{CoKE}}},
  author = {Wang, Quan and Huang, Pingping and Wang, Haifeng and Dai, Songtai and Jiang, Wenbin and Liu, Jing and Lyu, Yajuan and Zhu, Yong and Wu, Hua},
  year = {2020},
  month = apr,
  journal = {arXiv:1911.02168 [cs]},
  eprint = {1911.02168},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1911.02168},
  urldate = {2022-03-17},
  abstract = {Knowledge graph embedding, which projects symbolic entities and relations into continuous vector spaces, is gaining increasing attention. Previous methods allow a single static embedding for each entity or relation, ignoring their intrinsic contextual nature, i.e., entities and relations may appear in different graph contexts, and accordingly, exhibit different properties. This work presents Contextualized Knowledge Graph Embedding (CoKE), a novel paradigm that takes into account such contextual nature, and learns dynamic, flexible, and fully contextualized entity and relation embeddings. Two types of graph contexts are studied: edges and paths, both formulated as sequences of entities and relations. CoKE takes a sequence as input and uses a Transformer encoder to obtain contextualized representations. These representations are hence naturally adaptive to the input, capturing contextual meanings of entities and relations therein. Evaluation on a wide variety of public benchmarks verifies the superiority of CoKE in link prediction and path query answering. It performs consistently better than, or at least equally well as current state-of-the-art in almost every case, in particular offering an absolute improvement of 21.0\% in H@10 on path query answering. Our code is available at {\textbackslash}url\{https://github.com/PaddlePaddle/Research/tree/master/KG/CoKE\}.},
  archiveprefix = {arXiv}
}

@article{wangCrossviewTemporalGraph2023,
  title = {Cross-View Temporal Graph Contrastive Learning for Session-Based Recommendation},
  author = {Wang, Haosen and Yan, Surong and Wu, Chunqi and Han, Long and Zhou, Linghong},
  year = {2023},
  month = mar,
  journal = {Knowledge-Based Systems},
  volume = {264},
  pages = {110304},
  issn = {0950-7051},
  doi = {10.1016/j.knosys.2023.110304},
  urldate = {2024-09-24},
  abstract = {Session-based recommendation (SBR) aims at recommending items given the behavior sequences of anonymous users in a short-term session. Many recent SBR methods construct all sessions as a global graph that captures cross-session item transition patterns (i.e., users' global preference) to alleviate the problem of session data sparsity. However, these methods neglect that users' interests will drift as the time between the sessions constantly increases, limiting the performance improvement of SBR. To fill this gap, we divide all sessions into a group of time-slices and model the cross-session item transitions for every time-slice. We further construct two augmentation views (i.e., temporal graph and temporal hypergraph views) to model pairwise and high-order item transitions on SBR. In addition, we construct contrastive learning between two views to improve the recommendation performance by maximizing the mutual information between the item representations learned from the two views. Experiments on three public real-world datasets (i.e., Diginetica, Retailrocket, and Yoochoose) show that our model is consistently superior to the other state-of-the-art baselines, especially in time-sensitive datasets. For instance, our model achieves 14.42\% and 11.72\% improvements in terms of P@10 and P@20 on the Diginetica dataset, respectively.},
  keywords = {Contrastive learning,Graph neural network,Session-based recommendation,Temporal graph}
}

@article{WangDaShuJuShiDaiDeJiQiXueXiReDianGuoJiJiQiXueXiDaHuiICML2013CanHuiGanXiang2013,
  title = {{ ICML2013}},
  author = {, },
  year = {2013},
  journal = {},
  number = {009},
  pages = {3},
  langid = {chinese},
  annotation = {00000}
}

@inproceedings{wangDocuVizVisualizingCollaborative2015,
  title = {{{DocuViz}}: {{Visualizing Collaborative Writing}}},
  shorttitle = {{{DocuViz}}},
  booktitle = {Proceedings of the 33rd {{Annual ACM Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Wang, Dakuo and Olson, Judith S. and Zhang, Jingwen and Nguyen, Trung and Olson, Gary M.},
  year = {2015},
  month = apr,
  series = {{{CHI}} '15},
  pages = {1865--1874},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2702123.2702517},
  urldate = {2024-08-14},
  abstract = {Collaborative writing is on the increase. In order to write well together, authors often need to be aware of who has done what recently. We offer a new tool, DocuViz, that displays the entire revision history of Google Docs, showing more than the one-step-at-a-time view now shown in revision history and tracking changes in Word. We introduce the tool and present cases in which the tool has the potential to be useful: To authors themselves to see recent "seismic activity," indicating where in particular a co-author might want to pay attention, to instructors to see who has contributed what and which changes were made to comments from them, and to researchers interested in the new patterns of collaboration made possible by simultaneous editing capabilities.},
  isbn = {978-1-4503-3145-6}
}

@article{wangDRGLLaMATuningLLaMA2024,
  title = {{{DRG-LLaMA}} : Tuning {{LLaMA}} Model to Predict Diagnosis-Related Group for Hospitalized Patients},
  shorttitle = {{{DRG-LLaMA}}},
  author = {Wang, Hanyin and Gao, Chufan and Dantona, Christopher and Hull, Bryan and Sun, Jimeng},
  year = {2024},
  month = jan,
  journal = {npj Digital Medicine},
  volume = {7},
  number = {1},
  pages = {1--9},
  publisher = {Nature Publishing Group},
  issn = {2398-6352},
  doi = {10.1038/s41746-023-00989-3},
  urldate = {2024-01-23},
  abstract = {In the U.S. inpatient payment system, the Diagnosis-Related Group (DRG) is pivotal, but its assignment process is inefficient. The study introduces DRG-LLaMA, an advanced large language model (LLM) fine-tuned on clinical notes to enhance DRGs assignment. Utilizing LLaMA as the foundational model and optimizing it through Low-Rank Adaptation (LoRA) on 236,192 MIMIC-IV discharge summaries, our DRG-LLaMA -7B model exhibited a noteworthy macro-averaged F1 score of 0.327, a top-1 prediction accuracy of 52.0\%, and a macro-averaged Area Under the Curve (AUC) of 0.986, with a maximum input token length of 512. This model surpassed the performance of prior leading models in DRG prediction, showing a relative improvement of 40.3\% and 35.7\% in macro-averaged F1 score compared to ClinicalBERT and CAML, respectively. Applied to base DRG and complication or comorbidity (CC)/major complication or comorbidity (MCC) prediction, DRG-LLaMA achieved a top-1 prediction accuracy of 67.8\% and 67.5\%, respectively. Additionally, our findings indicate that DRG-LLaMA 's performance correlates with increased model parameters and input context lengths.},
  copyright = {2024 The Author(s)},
  langid = {english}
}

@inproceedings{wangEnablingConversationalInteraction2023,
  title = {Enabling {{Conversational Interaction}} with {{Mobile UI}} Using {{Large Language Models}}},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Wang, Bryan and Li, Gang and Li, Yang},
  year = {2023},
  month = apr,
  pages = {1--17},
  publisher = {ACM},
  address = {Hamburg Germany},
  doi = {10.1145/3544548.3580895},
  urldate = {2024-07-18},
  isbn = {978-1-4503-9421-5},
  langid = {english}
}

@misc{wangEnablingConversationalInteraction2023a,
  title = {Enabling {{Conversational Interaction}} with {{Mobile UI}} Using {{Large Language Models}}},
  author = {Wang, Bryan and Li, Gang and Li, Yang},
  year = {2023},
  month = feb,
  number = {arXiv:2209.08655},
  eprint = {2209.08655},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2209.08655},
  urldate = {2024-09-13},
  abstract = {Conversational agents show the promise to allow users to interact with mobile devices using language. However, to perform diverse UI tasks with natural language, developers typically need to create separate datasets and models for each specific task, which is expensive and effort-consuming. Recently, pre-trained large language models (LLMs) have been shown capable of generalizing to various downstream tasks when prompted with a handful of examples from the target task. This paper investigates the feasibility of enabling versatile conversational interactions with mobile UIs using a single LLM. We designed prompting techniques to adapt an LLM to mobile UIs. We experimented with four important modeling tasks that address various scenarios in conversational interaction. Our method achieved competitive performance on these challenging tasks without requiring dedicated datasets and training, offering a lightweight and generalizable approach to enable language-based mobile interaction.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction}
}

@misc{wangEnhancingRecommenderSystems2024,
  title = {Enhancing {{Recommender Systems}} with {{Large Language Model Reasoning Graphs}}},
  author = {Wang, Yan and Chu, Zhixuan and Ouyang, Xin and Wang, Simeng and Hao, Hongyan and Shen, Yue and Gu, Jinjie and Xue, Siqiao and Zhang, James Y. and Cui, Qing and Li, Longfei and Zhou, Jun and Li, Sheng},
  year = {2024},
  month = jan,
  number = {arXiv:2308.10835},
  eprint = {2308.10835},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.10835},
  urldate = {2024-10-01},
  abstract = {Recommendation systems aim to provide users with relevant suggestions, but often lack interpretability and fail to capture higher-level semantic relationships between user behaviors and profiles. In this paper, we propose a novel approach that leverages large language models (LLMs) to construct personalized reasoning graphs. These graphs link a user's profile and behavioral sequences through causal and logical inferences, representing the user's interests in an interpretable way. Our approach, LLM reasoning graphs (LLMRG), has four components: chained graph reasoning, divergent extension, self-verification and scoring, and knowledge base self-improvement. The resulting reasoning graph is encoded using graph neural networks, which serves as additional input to improve conventional recommender systems, without requiring extra user or item information. Our approach demonstrates how LLMs can enable more logical and interpretable recommender systems through personalized reasoning graphs. LLMRG allows recommendations to benefit from both engineered recommendation systems and LLM-derived reasoning graphs. We demonstrate the effectiveness of LLMRG on benchmarks and real-world scenarios in enhancing base recommendation models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval}
}

@inproceedings{wangExecutionBasedEvaluationOpenDomain2023,
  title = {Execution-{{Based Evaluation}} for {{Open-Domain Code Generation}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2023},
  author = {Wang, Zhiruo and Zhou, Shuyan and Fried, Daniel and Neubig, Graham},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  year = {2023},
  month = dec,
  pages = {1271--1290},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.findings-emnlp.89},
  urldate = {2024-01-11},
  abstract = {To extend the scope of coding queries to more realistic settings, we propose ODEX, the first Open-Domain EXecution-based natural language (NL) to Python code generation dataset. ODEX has 945 NL-Code pairs spanning 79 diverse libraries, along with 1,707 human-written test cases for execution. Our NL-Code pairs are harvested from StackOverflow forums to encourage natural and practical coding queries. Moreover, ODEX supports four natural languages as intents, in English, Spanish, Japanese, and Russian. ODEX unveils intriguing behavioral differences among top-performing code language models (LM). While CODEX achieves better overall results, CODEGEN improves effectively via scaling -- CODEGEN 6.1B performs comparably with CODEX 12B. Both models show substantial gaps between open and closed domains, but CODEGEN gaps tend to decrease with model size while CODEX gaps increase. We release ODEX to facilitate research into open-domain problems for the code generation community.}
}

@inproceedings{wangGatedSelfMatchingNetworks2017,
  title = {Gated {{Self-Matching Networks}} for {{Reading Comprehension}} and {{Question Answering}}},
  booktitle = {Proceedings of the 55th {{Annual Meeting}} of the {{Association}} for           {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Wang, Wenhui and Yang, Nan and Wei, Furu and Chang, Baobao and Zhou, Ming},
  year = {2017},
  pages = {189--198},
  publisher = {Association for Computational Linguistics},
  address = {Vancouver, Canada},
  doi = {10.18653/v1/P17-1018},
  urldate = {2022-04-13},
  abstract = {In this paper, we present the gated selfmatching networks for reading comprehension style question answering, which aims to answer questions from a given passage. We first match the question and passage with gated attention-based recurrent networks to obtain the question-aware passage representation. Then we propose a self-matching attention mechanism to refine the representation by matching the passage against itself, which effectively encodes information from the whole passage. We finally employ the pointer networks to locate the positions of answers from the passages. We conduct extensive experiments on the SQuAD dataset. The single model achieves 71.3\% on the evaluation metrics of exact match on the hidden test set, while the ensemble model further boosts the results to 75.9\%. At the time of submission of the paper, our model holds the first place on the SQuAD leaderboard for both single and ensemble model.},
  langid = {english}
}

@article{wangHumanAICollaborationData2019,
  title = {Human-{{AI Collaboration}} in {{Data Science}}: {{Exploring Data Scientists}}' {{Perceptions}} of {{Automated AI}}},
  shorttitle = {Human-{{AI Collaboration}} in {{Data Science}}},
  author = {Wang, Dakuo and Weisz, Justin D. and Muller, Michael and Ram, Parikshit and Geyer, Werner and Dugan, Casey and Tausczik, Yla and Samulowitz, Horst and Gray, Alexander},
  year = {2019},
  month = nov,
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  volume = {3},
  number = {CSCW},
  eprint = {1909.02309},
  primaryclass = {cs},
  pages = {1--24},
  issn = {2573-0142},
  doi = {10.1145/3359313},
  urldate = {2023-09-18},
  abstract = {The rapid advancement of artificial intelligence (AI) is changing our lives in many ways. One application domain is data science. New techniques in automating the creation of AI, known as AutoAI or AutoML, aim to automate the work practices of data scientists. AutoAI systems are capable of autonomously ingesting and pre-processing data, engineering new features, and creating and scoring models based on a target objectives (e.g. accuracy or run-time efficiency). Though not yet widely adopted, we are interested in understanding how AutoAI will impact the practice of data science. We conducted interviews with 20 data scientists who work at a large, multinational technology company and practice data science in various business settings. Our goal is to understand their current work practices and how these practices might change with AutoAI. Reactions were mixed: while informants expressed concerns about the trend of automating their jobs, they also strongly felt it was inevitable. Despite these concerns, they remained optimistic about their future job security due to a view that the future of data science work will be a collaboration between humans and AI systems, in which both automation and human expertise are indispensable.},
  archiveprefix = {arXiv}
}

@misc{wangHumanoidAgentsPlatform2023,
  title = {Humanoid {{Agents}}: {{Platform}} for {{Simulating Human-like Generative Agents}}},
  shorttitle = {Humanoid {{Agents}}},
  author = {Wang, Zhilin and Chiu, Yu Ying and Chiu, Yu Cheung},
  year = {2023},
  month = oct,
  number = {arXiv:2310.05418},
  eprint = {2310.05418},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.05418},
  urldate = {2024-07-09},
  abstract = {Just as computational simulations of atoms, molecules and cells have shaped the way we study the sciences, true-to-life simulations of human-like agents can be valuable tools for studying human behavior. We propose Humanoid Agents, a system that guides Generative Agents to behave more like humans by introducing three elements of System 1 processing: Basic needs (e.g. hunger, health and energy), Emotion and Closeness in Relationships. Humanoid Agents are able to use these dynamic elements to adapt their daily activities and conversations with other agents, as supported with empirical experiments. Our system is designed to be extensible to various settings, three of which we demonstrate, as well as to other elements influencing human behavior (e.g. empathy, moral values and cultural background). Our platform also includes a Unity WebGL game interface for visualization and an interactive analytics dashboard to show agent statuses over time. Our platform is available on https://www.humanoidagents.com/ and code is on https://github.com/HumanoidAgents/HumanoidAgents},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction}
}

@misc{wangKnowledgeEditingLarge2023,
  title = {Knowledge {{Editing}} for {{Large Language Models}}: {{A Survey}}},
  shorttitle = {Knowledge {{Editing}} for {{Large Language Models}}},
  author = {Wang, Song and Zhu, Yaochen and Liu, Haochen and Zheng, Zaiyi and Chen, Chen and Li, Jundong},
  year = {2023},
  month = oct,
  journal = {arXiv.org},
  url = {https://arxiv.org/abs/2310.16218v3},
  urldate = {2024-09-05},
  abstract = {Large language models (LLMs) have recently transformed both the academic and industrial landscapes due to their remarkable capacity to understand, analyze, and generate texts based on their vast knowledge and reasoning ability. Nevertheless, one major drawback of LLMs is their substantial computational cost for pre-training due to their unprecedented amounts of parameters. The disadvantage is exacerbated when new knowledge frequently needs to be introduced into the pre-trained model. Therefore, it is imperative to develop effective and efficient techniques to update pre-trained LLMs. Traditional methods encode new knowledge in pre-trained LLMs through direct fine-tuning. However, naively re-training LLMs can be computationally intensive and risks degenerating valuable pre-trained knowledge irrelevant to the update in the model. Recently, Knowledge-based Model Editing (KME) has attracted increasing attention, which aims to precisely modify the LLMs to incorporate specific knowledge, without negatively influencing other irrelevant knowledge. In this survey, we aim to provide a comprehensive and in-depth overview of recent advances in the field of KME. We first introduce a general formulation of KME to encompass different KME strategies. Afterward, we provide an innovative taxonomy of KME techniques based on how the new knowledge is introduced into pre-trained LLMs, and investigate existing KME strategies while analyzing key insights, advantages, and limitations of methods from each category. Moreover, representative metrics, datasets, and applications of KME are introduced accordingly. Finally, we provide an in-depth analysis regarding the practicality and remaining challenges of KME and suggest promising research directions for further advancement in this field.},
  langid = {english}
}

@misc{wangKnowledgeEditingLarge2023a,
  title = {Knowledge {{Editing}} for {{Large Language Models}}: {{A Survey}}},
  shorttitle = {Knowledge {{Editing}} for {{Large Language Models}}},
  author = {Wang, Song and Zhu, Yaochen and Liu, Haochen and Zheng, Zaiyi and Chen, Chen and Li, Jundong},
  year = {2023},
  month = dec,
  number = {arXiv:2310.16218},
  eprint = {2310.16218},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2310.16218},
  urldate = {2024-09-05},
  abstract = {Large language models (LLMs) have recently transformed both the academic and industrial landscapes due to their remarkable capacity to understand, analyze, and generate texts based on their vast knowledge and reasoning ability. Nevertheless, one major drawback of LLMs is their substantial computational cost for pre-training due to their unprecedented amounts of parameters. The disadvantage is exacerbated when new knowledge frequently needs to be introduced into the pre-trained model. Therefore, it is imperative to develop effective and efficient techniques to update pre-trained LLMs. Traditional methods encode new knowledge in pre-trained LLMs through direct fine-tuning. However, naively re-training LLMs can be computationally intensive and risks degenerating valuable pre-trained knowledge irrelevant to the update in the model. Recently, Knowledge-based Model Editing (KME) has attracted increasing attention, which aims to precisely modify the LLMs to incorporate specific knowledge, without negatively influencing other irrelevant knowledge. In this survey, we aim to provide a comprehensive and in-depth overview of recent advances in the field of KME. We first introduce a general formulation of KME to encompass different KME strategies. Afterward, we provide an innovative taxonomy of KME techniques based on how the new knowledge is introduced into pre-trained LLMs, and investigate existing KME strategies while analyzing key insights, advantages, and limitations of methods from each category. Moreover, representative metrics, datasets, and applications of KME are introduced accordingly. Finally, we provide an in-depth analysis regarding the practicality and remaining challenges of KME and suggest promising research directions for further advancement in this field.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@article{wangKnowledgeGraphEmbedding2014,
  title = {Knowledge {{Graph Embedding}} by {{Translating}} on {{Hyperplanes}}},
  author = {Wang, Zhen and Zhang, Jianwen and Feng, Jianlin and Chen, Zheng},
  year = {2014},
  month = jun,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {28},
  number = {1},
  issn = {2374-3468},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/8870},
  urldate = {2021-12-07},
  abstract = {We deal with embedding a large scale knowledge graph composed of entities and relations into a continuous vector space. TransE is a promising method proposed recently, which is very efficient while achieving state-of-the-art predictive performance. We discuss some mapping properties of relations which should be considered in embedding, such as reflexive, one-to-many, many-to-one, and many-to-many. We note that TransE does not do well in dealing with these properties. Some complex models are capable of preserving these mapping properties but sacrifice efficiency in the process. To make a good trade-off between model capacity and efficiency, in this paper we propose TransH which models a relation as a hyperplane together with a translation operation on it. In this way, we can well preserve the above mapping properties of relations with almost the same model complexity of TransE. Additionally, as a practical knowledge graph is often far from completed, how to construct negative examples to reduce false negative labels in training is very important. Utilizing the one-to-many/many-to-one mapping property of a relation, we propose a simple trick to reduce the possibility of false negative labeling. We conduct extensive experiments on link prediction, triplet classification and fact extraction on benchmark datasets like WordNet and Freebase. Experiments show TransH delivers significant improvements over TransE on predictive accuracy with comparable capability to scale up.},
  copyright = {Copyright (c)},
  langid = {english}
}

@inproceedings{wangMediTabScalingMedical2023,
  title = {{{MediTab}}: {{Scaling Medical Tabular Data Predictors}} via {{Data Consolidation}}, {{Enrichment}}, and {{Refinement}}},
  shorttitle = {{{MediTab}}},
  booktitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  author = {Wang, Zifeng and Gao, Chufan and Xiao, Cao and Sun, Jimeng},
  year = {2023},
  month = oct,
  eprint = {2305.12081},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2305.12081},
  urldate = {2024-02-01},
  abstract = {Tabular data prediction has been employed in medical applications such as patient health risk prediction. However, existing methods usually revolve around the algorithm design while overlooking the significance of data engineering. Medical tabular datasets frequently exhibit significant heterogeneity across different sources, with limited sample sizes per source. As such, previous predictors are often trained on manually curated small datasets that struggle to generalize across different tabular datasets during inference. This paper proposes to scale medical tabular data predictors (MediTab) to various tabular inputs with varying features. The method uses a data engine that leverages large language models (LLMs) to consolidate tabular samples to overcome the barrier across tables with distinct schema. It also aligns out-domain data with the target task using a "learn, annotate, and refinement" pipeline. The expanded training data then enables the pre-trained MediTab to infer for arbitrary tabular input in the domain without fine-tuning, resulting in significant improvements over supervised baselines: it reaches an average ranking of 1.57 and 1.00 on 7 patient outcome prediction datasets and 3 trial outcome prediction datasets, respectively. In addition, MediTab exhibits impressive zero-shot performances: it outperforms supervised XGBoost models by 8.9\% and 17.2\% on average in two prediction tasks, respectively. The code is available at https://github.com/RyanWangZf/MediTab.},
  archiveprefix = {arXiv},
  keywords = {LLM_for_clinical_diagnose}
}

@inproceedings{wangMiniLMDeepSelfAttention2020,
  title = {{{MiniLM}}: {{Deep Self-Attention Distillation}} for {{Task-Agnostic Compression}} of {{Pre-Trained Transformers}}},
  shorttitle = {{{MiniLM}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Wang, Wenhui and Wei, Furu and Dong, Li and Bao, Hangbo and Yang, Nan and Zhou, Ming},
  year = {2020},
  volume = {33},
  pages = {5776--5788},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2020/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
  urldate = {2023-11-14},
  abstract = {Pre-trained language models (e.g., BERT (Devlin et al., 2018) and its variants) have achieved remarkable success in varieties of NLP tasks. However, these models usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and online serving in real-life applications due to latency and capacity constraints. In this work, we present a simple and effective approach to compress large Transformer (Vaswani et al., 2017) based pre-trained models, termed as deep self-attention distillation. The small model (student) is trained by deeply mimicking the self-attention module, which plays a vital role in Transformer networks, of the large model (teacher). Specifically, we propose distilling the self-attention module of the last Transformer layer of the teacher, which is effective and flexible for the student. Furthermore, we introduce the scaled dot-product between values in the self-attention module as the new deep self-attention knowledge, in addition to the attention distributions (i.e., the scaled dot-product of queries and keys) that have been used in existing works. Moreover, we show that introducing a teacher assistant (Mirzadeh et al., 2019) also helps the distillation of large pre-trained Transformer models. Experimental results demonstrate that our monolingual model outperforms state-of-the-art baselines in different parameter size of student models. In particular, it retains more than 99\% accuracy on SQuAD 2.0 and several GLUE benchmark tasks using 50\% of the Transformer parameters and computations of the teacher model. We also obtain competitive results in applying deep self-attention distillation to multilingual pre-trained models.}
}

@misc{wangMultimodalRiskPrediction2023,
  title = {Multimodal {{Risk Prediction}} with {{Physiological Signals}}, {{Medical Images}} and {{Clinical Notes}}},
  author = {Wang, Yuanlong and Yin, Changchang and Zhang, Ping},
  year = {2023},
  month = may,
  pages = {2023.05.18.23290207},
  publisher = {medRxiv},
  doi = {10.1101/2023.05.18.23290207},
  urldate = {2024-01-11},
  abstract = {The broad adoption of electronic health records (EHRs) provides great opportunities to conduct healthcare research and solve various clinical problems in medicine. With recent advances and success, methods based on machine learning and deep learning have become increasingly popular in medical informatics. Combining data from multiple modalities may help in predictive tasks. To assess the expectations of multimodal data, we introduce a comprehensive fusion framework designed to integrate temporal variables, medical images, and clinical notes in Electronic Health Record (EHR) for enhanced performance in down-stream predictive tasks. Early, joint, and late fusion strategies were employed to effectively combine data from various modalities. Model performance and contribution scores show that multimodal models outperform uni-modal models in various tasks. Additionally, temporal signs contain more information than CXR images and clinical notes in three explored predictive tasks. Therefore, models integrating different data modalities can work better in predictive tasks.},
  archiveprefix = {medRxiv},
  copyright = {{\copyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english}
}

@misc{wangRAGENUnderstandingSelfEvolution2025,
  title = {{{RAGEN}}: {{Understanding Self-Evolution}} in {{LLM Agents}} via {{Multi-Turn Reinforcement Learning}}},
  shorttitle = {{{RAGEN}}},
  author = {Wang, Zihan and Wang, Kangrui and Wang, Qineng and Zhang, Pingyue and Li, Linjie and Yang, Zhengyuan and Jin, Xing and Yu, Kefan and Nguyen, Minh Nhat and Liu, Licheng and Gottlieb, Eli and Lu, Yiping and Cho, Kyunghyun and Wu, Jiajun and {Fei-Fei}, Li and Wang, Lijuan and Choi, Yejin and Li, Manling},
  year = {2025},
  month = may,
  number = {arXiv:2504.20073},
  eprint = {2504.20073},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.20073},
  urldate = {2025-06-04},
  abstract = {Training large language models (LLMs) as interactive agents presents unique challenges including long-horizon decision making and interacting with stochastic environment feedback. While reinforcement learning (RL) has enabled progress in static tasks, multi-turn agent RL training remains underexplored. We propose StarPO (State-Thinking-Actions-Reward Policy Optimization), a general framework for trajectory-level agent RL, and introduce RAGEN, a modular system for training and evaluating LLM agents. Our study on four stylized environments reveals three core findings. First, our agent RL training shows a recurring mode of Echo Trap where reward variance cliffs and gradient spikes; we address this with StarPO-S, a stabilized variant with trajectory filtering, critic incorporation, and gradient stabilization. Second, we find the shaping of RL rollouts would benefit from diverse initial states, medium interaction granularity and more frequent sampling. Third, we show that without fine-grained, reasoning-aware reward signals, agent reasoning hardly emerge through multi-turn RL and they may show shallow strategies or hallucinated thoughts. Code and environments are available at https://github.com/RAGEN-AI/RAGEN.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@inproceedings{wangRecMindLargeLanguage2024,
  title = {{{RecMind}}: {{Large Language Model Powered Agent For Recommendation}}},
  shorttitle = {{{RecMind}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{NAACL}} 2024},
  author = {Wang, Yancheng and Jiang, Ziyan and Chen, Zheng and Yang, Fan and Zhou, Yingxue and Cho, Eunah and Fan, Xing and Lu, Yanbin and Huang, Xiaojiang and Yang, Yingzhen},
  editor = {Duh, Kevin and Gomez, Helena and Bethard, Steven},
  year = {2024},
  month = jun,
  pages = {4351--4364},
  publisher = {Association for Computational Linguistics},
  address = {Mexico City, Mexico},
  doi = {10.18653/v1/2024.findings-naacl.271},
  urldate = {2025-04-21},
  abstract = {While the recommendation system (RS) has advanced significantly through deep learning, current RS approaches usually train and fine-tune models on task-specific datasets, limiting their generalizability to new recommendation tasks and their ability to leverage external knowledge due to model scale and data size constraints. Thus, we designed an LLM-powered autonomous recommender agent, RecMind, which is capable of leveraging external knowledge, utilizing tools with careful planning to provide zero-shot personalized recommendations. We propose a Self-Inspiring algorithm to improve the planning ability. At each intermediate step, the LLM ``self-inspires'' to consider all previously explored states to plan for the next step. This mechanism greatly improves the model`s ability to comprehend and utilize historical information in planning for recommendation. We evaluate RecMind`s performance in various recommendation scenarios. Our experiment shows that RecMind outperforms existing zero/few-shot LLM-based recommendation baseline methods in various tasks and achieves comparable performance to a fully trained recommendation model P5.}
}

@misc{wangSelfInstructAligningLanguage2022,
  title = {Self-{{Instruct}}: {{Aligning Language Model}} with {{Self Generated Instructions}}},
  shorttitle = {Self-{{Instruct}}},
  author = {Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
  year = {2022},
  month = dec,
  number = {arXiv:2212.10560},
  eprint = {2212.10560},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.10560},
  urldate = {2023-03-15},
  abstract = {Large "instruction-tuned" language models (finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off its own generations. Our pipeline generates instruction, input, and output samples from a language model, then prunes them before using them to finetune the original model. Applying our method to vanilla GPT3, we demonstrate a 33\% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT\_001, which is trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5\% absolute gap behind InstructGPT\_001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.},
  archiveprefix = {arXiv}
}

@article{wangSurveyLargeLanguage2024,
  title = {A {{Survey}} on {{Large Language Model}} Based {{Autonomous Agents}}},
  author = {Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and Zhao, Wayne Xin and Wei, Zhewei and Wen, Ji-Rong},
  year = {2024},
  month = dec,
  journal = {Frontiers of Computer Science},
  volume = {18},
  number = {6},
  eprint = {2308.11432},
  primaryclass = {cs},
  pages = {186345},
  issn = {2095-2228, 2095-2236},
  doi = {10.1007/s11704-024-40231-1},
  urldate = {2025-02-19},
  abstract = {Autonomous agents have long been a prominent research focus in both academic and industry communities. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of LLM-based autonomous agents from a holistic perspective. More specifically, we first discuss the construction of LLM-based autonomous agents, for which we propose a unified framework that encompasses a majority of the previous work. Then, we present a comprehensive overview of the diverse applications of LLM-based autonomous agents in the fields of social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field. To keep track of this field and continuously update our survey, we maintain a repository of relevant references at https://github.com/Paitesanshi/LLM-Agent-Survey.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Artificial Intelligence,autonomous agent,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,human-level intelligence,large language model}
}

@article{wangSurveySessionbasedRecommender2021,
  title = {A {{Survey}} on {{Session-based Recommender Systems}}},
  author = {Wang, Shoujin and Cao, Longbing and Wang, Yan and Sheng, Quan Z. and Orgun, Mehmet A. and Lian, Defu},
  year = {2021},
  month = jul,
  journal = {ACM Comput. Surv.},
  volume = {54},
  number = {7},
  pages = {154:1--154:38},
  issn = {0360-0300},
  doi = {10.1145/3465401},
  urldate = {2024-09-19},
  abstract = {Recommender systems (RSs) have been playing an increasingly important role for informed consumption, services, and decision-making in the overloaded information era and digitized economy. In recent years, session-based recommender systems (SBRSs) have emerged as a new paradigm of RSs. Different from other RSs such as content-based RSs and collaborative filtering-based RSs that usually model long-term yet static user preferences, SBRSs aim to capture short-term but dynamic user preferences to provide more timely and accurate recommendations sensitive to the evolution of their session contexts. Although SBRSs have been intensively studied, neither unified problem statements for SBRSs nor in-depth elaboration of SBRS characteristics and challenges are available. It is also unclear to what extent SBRS challenges have been addressed and what the overall research landscape of SBRSs is. This comprehensive review of SBRSs addresses the above aspects by exploring in depth the SBRS entities (e.g., sessions), behaviours (e.g., users' clicks on items), and their properties (e.g., session length). We propose a general problem statement of SBRSs, summarize the diversified data characteristics and challenges of SBRSs, and define a taxonomy to categorize the representative SBRS research. Finally, we discuss new research opportunities in this exciting and vibrant area.}
}

@misc{wangTwostageLLMFinetuning2022,
  title = {Two-Stage {{LLM Fine-tuning}} with {{Less Specialization}} and {{More Generalization}}},
  author = {Wang, Yihan and Si, Si and Li, Daliang and Lukasik, Michal and Yu, Felix and Hsieh, Cho-Jui and Dhillon, Inderjit S. and Kumar, Sanjiv},
  year = {2022},
  month = nov,
  journal = {arXiv.org},
  url = {https://arxiv.org/abs/2211.00635v3},
  urldate = {2024-09-05},
  abstract = {Pretrained large language models (LLMs) are general purpose problem solvers applicable to a diverse set of tasks with prompts. They can be further improved towards a specific task by fine-tuning on a specialized dataset. However, fine-tuning usually makes the model narrowly specialized on this dataset with reduced general in-context learning performances, which is undesirable whenever the fine-tuned model needs to handle additional tasks where no fine-tuning data is available. In this work, we first demonstrate that fine-tuning on a single task indeed decreases LLMs' general in-context learning performance. We discover one important cause of such forgetting, format specialization, where the model overfits to the format of the fine-tuned task.We further show that format specialization happens at the very beginning of fine-tuning. To solve this problem, we propose Prompt Tuning with MOdel Tuning (ProMoT), a simple yet effective two-stage fine-tuning framework that reduces format specialization and improves generalization.ProMoT offloads task-specific format learning into additional and removable parameters by first doing prompt tuning and then fine-tuning the model itself with this soft prompt attached. With experiments on several fine-tuning tasks and 8 in-context evaluation tasks, we show that ProMoT achieves comparable performance on fine-tuned tasks to standard fine-tuning, but with much less loss of in-context learning performances across a board range of out-of-domain evaluation tasks. More importantly, ProMoT can even enhance generalization on in-context learning tasks that are semantically related to the fine-tuned task, e.g. ProMoT on En-Fr translation significantly improves performance on other language pairs, and ProMoT on NLI improves performance on summarization. Experiments also show that ProMoT can improve the generalization performance of multi-task training.},
  langid = {english}
}

@misc{wangTwostageLLMFinetuning2024,
  title = {Two-Stage {{LLM Fine-tuning}} with {{Less Specialization}} and {{More Generalization}}},
  author = {Wang, Yihan and Si, Si and Li, Daliang and Lukasik, Michal and Yu, Felix and Hsieh, Cho-Jui and Dhillon, Inderjit S. and Kumar, Sanjiv},
  year = {2024},
  month = mar,
  number = {arXiv:2211.00635},
  eprint = {2211.00635},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2211.00635},
  urldate = {2024-09-05},
  abstract = {Pretrained large language models (LLMs) are general purpose problem solvers applicable to a diverse set of tasks with prompts. They can be further improved towards a specific task by fine-tuning on a specialized dataset. However, fine-tuning usually makes the model narrowly specialized on this dataset with reduced general in-context learning performances, which is undesirable whenever the fine-tuned model needs to handle additional tasks where no fine-tuning data is available. In this work, we first demonstrate that fine-tuning on a single task indeed decreases LLMs' general in-context learning performance. We discover one important cause of such forgetting, format specialization, where the model overfits to the format of the fine-tuned task. We further show that format specialization happens at the very beginning of fine-tuning. To solve this problem, we propose Prompt Tuning with MOdel Tuning (ProMoT), a simple yet effective two-stage fine-tuning framework that reduces format specialization and improves generalization. ProMoT offloads task-specific format learning into additional and removable parameters by first doing prompt tuning and then fine-tuning the model itself with this soft prompt attached. With experiments on several fine-tuning tasks and 8 in-context evaluation tasks, we show that ProMoT achieves comparable performance on fine-tuned tasks to standard fine-tuning, but with much less loss of in-context learning performances across a board range of out-of-domain evaluation tasks. More importantly, ProMoT can even enhance generalization on in-context learning tasks that are semantically related to the fine-tuned task, e.g. ProMoT on En-Fr translation significantly improves performance on other language pairs, and ProMoT on NLI improves performance on summarization. Experiments also show that ProMoT can improve the generalization performance of multi-task training.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{wangUnderstandingChainofThoughtPrompting2023,
  title = {Towards {{Understanding Chain-of-Thought Prompting}}: {{An Empirical Study}} of {{What Matters}}},
  shorttitle = {Towards {{Understanding Chain-of-Thought Prompting}}},
  author = {Wang, Boshi and Min, Sewon and Deng, Xiang and Shen, Jiaming and Wu, You and Zettlemoyer, Luke and Sun, Huan},
  year = {2023},
  month = jun,
  number = {arXiv:2212.10001},
  eprint = {2212.10001},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.10001},
  urldate = {2023-06-10},
  abstract = {Chain-of-Thought (CoT) prompting can dramatically improve the multi-step reasoning abilities of large language models (LLMs). CoT explicitly encourages the LLM to generate intermediate rationales for solving a problem, by providing a series of reasoning steps in the demonstrations. Despite its success, there is still little understanding of what makes CoT prompting effective and which aspects of the demonstrated reasoning steps contribute to its performance. In this paper, we show that CoT reasoning is possible even with invalid demonstrations - prompting with invalid reasoning steps can achieve over 80-90\% of the performance obtained using CoT under various metrics, while still generating coherent lines of reasoning during inference. Further experiments show that other aspects of the rationales, such as being relevant to the query and correctly ordering the reasoning steps, are much more important for effective CoT reasoning. Overall, these findings both deepen our understanding of CoT prompting, and open up new questions regarding LLMs' capability to learn to reason in context.},
  archiveprefix = {arXiv}
}

@misc{wangUserBehaviorSimulation2024,
  title = {User {{Behavior Simulation}} with {{Large Language Model}} Based {{Agents}}},
  author = {Wang, Lei and Zhang, Jingsen and Yang, Hao and Chen, Zhiyuan and Tang, Jiakai and Zhang, Zeyu and Chen, Xu and Lin, Yankai and Song, Ruihua and Zhao, Wayne Xin and Xu, Jun and Dou, Zhicheng and Wang, Jun and Wen, Ji-Rong},
  year = {2024},
  month = feb,
  number = {arXiv:2306.02552},
  eprint = {2306.02552},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2306.02552},
  urldate = {2024-02-25},
  abstract = {Simulating high quality user behavior data has always been a fundamental problem in human-centered applications, where the major difficulty originates from the intricate mechanism of human decision process. Recently, substantial evidences have suggested that by learning huge amounts of web knowledge, large language models (LLMs) can achieve human-like intelligence. We believe these models can provide significant opportunities to more believable user behavior simulation. To inspire such direction, we propose an LLM-based agent framework and design a sandbox environment to simulate real user behaviors. Based on extensive experiments, we find that the simulated behaviors of our method are very close to the ones of real humans. Concerning potential applications, we simulate and study two social phenomenons including (1) information cocoons and (2) user conformity behaviors. This research provides novel simulation paradigms for human-centered applications.},
  archiveprefix = {arXiv}
}

@article{WangXiaoJieJiQiYueDuLiJieDeYanJiuJinZhan2019,
  title = {{}},
  author = { and {Xiao-jie}, {\relax WANG} and  and {Zi-wei}, B. A. I. and  and Ke, L. I. and  and {Cai-xia}, {\relax YUAN}},
  year = {2019},
  month = dec,
  publisher = {},
  doi = {10.13190/j.jbupt.2019-111},
  urldate = {2021-12-25},
  abstract = {4.--42.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {cn},
  annotation = {00000}
}

@misc{wangZeroShotNextItemRecommendation2023,
  title = {Zero-{{Shot Next-Item Recommendation}} Using {{Large Pretrained Language Models}}},
  author = {Wang, Lei and Lim, Ee-Peng},
  year = {2023},
  month = apr,
  number = {arXiv:2304.03153},
  eprint = {2304.03153},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.03153},
  urldate = {2024-09-24},
  abstract = {Large language models (LLMs) have achieved impressive zero-shot performance in various natural language processing (NLP) tasks, demonstrating their capabilities for inference without training examples. Despite their success, no research has yet explored the potential of LLMs to perform next-item recommendations in the zero-shot setting. We have identified two major challenges that must be addressed to enable LLMs to act effectively as recommenders. First, the recommendation space can be extremely large for LLMs, and LLMs do not know about the target user's past interacted items and preferences. To address this gap, we propose a prompting strategy called Zero-Shot Next-Item Recommendation (NIR) prompting that directs LLMs to make next-item recommendations. Specifically, the NIR-based strategy involves using an external module to generate candidate items based on user-filtering or item-filtering. Our strategy incorporates a 3-step prompting that guides GPT-3 to carry subtasks that capture the user's preferences, select representative previously watched movies, and recommend a ranked list of 10 movies. We evaluate the proposed approach using GPT-3 on MovieLens 100K dataset and show that it achieves strong zero-shot performance, even outperforming some strong sequential recommendation models trained on the entire training dataset. These promising results highlight the ample research opportunities to use LLMs as recommenders. The code can be found at https://github.com/AGI-Edgerunners/LLM-Next-Item-Rec.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval}
}

@misc{weiChainofThoughtPromptingElicits2023,
  title = {Chain-of-{{Thought Prompting Elicits Reasoning}} in {{Large Language Models}}},
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  year = {2023},
  month = jan,
  number = {arXiv:2201.11903},
  eprint = {2201.11903},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2201.11903},
  urldate = {2023-01-19},
  abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  archiveprefix = {arXiv}
}

@inproceedings{weiFinetunedLanguageModels2021,
  title = {Finetuned {{Language Models}} Are {{Zero-Shot Learners}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Wei, Jason and Bosma, Maarten and Zhao, Vincent and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M. and Le, Quoc V.},
  year = {2021},
  month = oct,
  url = {https://openreview.net/forum?id=gEZrGCozdqR},
  urldate = {2023-11-14},
  abstract = {This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning---finetuning language models on a collection of datasets described via instructions---substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.},
  langid = {english}
}

@article{weissenbornMakingNeuralQA2017,
  title = {Making {{Neural QA}} as {{Simple}} as {{Possible}} but Not {{Simpler}}},
  author = {Weissenborn, Dirk and Wiese, Georg and Seiffe, Laura},
  year = {2017},
  month = jun,
  journal = {arXiv:1703.04816 [cs]},
  eprint = {1703.04816},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1703.04816},
  urldate = {2021-12-25},
  abstract = {Recent development of large-scale question answering (QA) datasets triggered a substantial amount of research into end-to-end neural architectures for QA. Increasingly complex systems have been conceived without comparison to simpler neural baseline systems that would justify their complexity. In this work, we propose a simple heuristic that guides the development of neural baseline systems for the extractive QA task. We find that there are two ingredients necessary for building a high-performing neural QA system: first, the awareness of question words while processing the context and second, a composition function that goes beyond simple bag-of-words modeling, such as recurrent neural networks. Our results show that FastQA, a system that meets these two requirements, can achieve very competitive performance compared with existing models. We argue that this surprising finding puts results of previous systems and the complexity of recent QA datasets into perspective.},
  archiveprefix = {arXiv},
  annotation = {00188}
}

@misc{weiWebAgentR1TrainingWeb2025,
  title = {{{WebAgent-R1}}: {{Training Web Agents}} via {{End-to-End Multi-Turn Reinforcement Learning}}},
  shorttitle = {{{WebAgent-R1}}},
  author = {Wei, Zhepei and Yao, Wenlin and Liu, Yao and Zhang, Weizhi and Lu, Qin and Qiu, Liang and Yu, Changlong and Xu, Puyang and Zhang, Chao and Yin, Bing and Yun, Hyokun and Li, Lihong},
  year = {2025},
  month = may,
  number = {arXiv:2505.16421},
  eprint = {2505.16421},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.16421},
  urldate = {2025-06-02},
  abstract = {While reinforcement learning (RL) has demonstrated remarkable success in enhancing large language models (LLMs), it has primarily focused on single-turn tasks such as solving math problems. Training effective web agents for multi-turn interactions remains challenging due to the complexity of long-horizon decision-making across dynamic web interfaces. In this work, we present WebAgent-R1, a simple yet effective end-to-end multi-turn RL framework for training web agents. It learns directly from online interactions with web environments by asynchronously generating diverse trajectories, entirely guided by binary rewards depending on task success. Experiments on the WebArena-Lite benchmark demonstrate the effectiveness of WebAgent-R1, boosting the task success rate of Qwen-2.5-3B from 6.1\% to 33.9\% and Llama-3.1-8B from 8.5\% to 44.8\%, significantly outperforming existing state-of-the-art methods and strong proprietary models such as OpenAI o3. In-depth analyses reveal the effectiveness of the thinking-based prompting strategy and test-time scaling through increased interactions for web tasks. We further investigate different RL initialization policies by introducing two variants, namely WebAgent-R1-Zero and WebAgent-R1-CoT, which highlight the importance of the warm-up training stage (i.e., behavior cloning) and provide insights on incorporating long chain-of-thought (CoT) reasoning in web agents.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@article{westonAICompleteQuestionAnswering2015,
  title = {Towards {{AI-Complete Question Answering}}: {{A Set}} of {{Prerequisite Toy Tasks}}},
  shorttitle = {Towards {{AI-Complete Question Answering}}},
  author = {Weston, Jason and Bordes, Antoine and Chopra, Sumit and Rush, Alexander M. and {van Merri{\"e}nboer}, Bart and Joulin, Armand and Mikolov, Tomas},
  year = {2015},
  month = dec,
  journal = {arXiv:1502.05698 [cs, stat]},
  eprint = {1502.05698},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1502.05698},
  urldate = {2021-12-25},
  abstract = {One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  annotation = {00961}
}

@inproceedings{whartonApplyingCognitiveWalkthroughs1992,
  title = {Applying Cognitive Walkthroughs to More Complex User Interfaces: Experiences, Issues, and Recommendations},
  shorttitle = {Applying Cognitive Walkthroughs to More Complex User Interfaces},
  booktitle = {Proceedings of the {{SIGCHI}} Conference on {{Human}} Factors in Computing Systems  - {{CHI}} '92},
  author = {Wharton, Cathleen and Bradford, Janice and Jeffries, Robin and Franzke, Marita},
  year = {1992},
  pages = {381--388},
  publisher = {ACM Press},
  address = {Monterey, California, United States},
  doi = {10.1145/142750.142864},
  urldate = {2025-04-10},
  isbn = {978-0-89791-513-7},
  langid = {english}
}

@misc{willardEfficientGuidedGeneration2023,
  title = {Efficient {{Guided Generation}} for {{Large Language Models}}},
  author = {Willard, Brandon T. and Louf, R{\'e}mi},
  year = {2023},
  month = aug,
  number = {arXiv:2307.09702},
  eprint = {2307.09702},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.09702},
  urldate = {2024-09-17},
  abstract = {In this article we show how the problem of neural text generation can be constructively reformulated in terms of transitions between the states of a finite-state machine. This framework leads to an efficient approach to guiding text generation with regular expressions and context-free grammars by allowing the construction of an index over a language model's vocabulary. The approach is model agnostic, allows one to enforce domain-specific knowledge and constraints, and enables the construction of reliable interfaces by guaranteeing the structure of the generated text. It adds little overhead to the token sequence generation process and significantly outperforms existing solutions. An implementation is provided in the open source Python library Outlines},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@article{wilsonAutomatedEssayEvaluation2016,
  title = {Automated Essay Evaluation Software in {{English Language Arts}} Classrooms: {{Effects}} on Teacher Feedback, Student Motivation, and Writing Quality},
  shorttitle = {Automated Essay Evaluation Software in {{English Language Arts}} Classrooms},
  author = {Wilson, Joshua and Czik, Amanda},
  year = {2016},
  month = sep,
  journal = {Computers \& Education},
  volume = {100},
  pages = {94--109},
  issn = {0360-1315},
  doi = {10.1016/j.compedu.2016.05.004},
  urldate = {2022-07-13},
  abstract = {Automated Essay Evaluation (AEE) systems are being increasingly adopted in the United States to support writing instruction. AEE systems are expected to assist teachers in providing increased higher-level feedback and expediting the feedback process, while supporting gains in students' writing motivation and writing quality. The current study explored these claims using a quasi-experimental study. Four eighth-grade English Language Arts (ELA) classes were assigned to a combined feedback condition in which they received feedback on their writing from their teacher and from an automated essay evaluation (AEE) system called PEG Writing{\textregistered}. Four other eighth-grade ELA classes were assigned to a teacher feedback-only condition, in which they received feedback from their teacher via GoogleDocs. Results indicated that teachers gave the same median amount feedback to students in both condition, but gave proportionately more feedback on higher-level writing skills to students in the combined PEG~+~Teacher Feedback condition. Teachers also agreed that PEG assisted them in saving one-third to half the time it took to provide feedback when they were the sole source of feedback (i.e., the GoogleDocs condition). At the conclusion of the study, students in the combined feedback condition demonstrated increases in writing persistence, though there were no differences between groups with regard to final-draft writing quality.},
  langid = {english}
}

@misc{workshopBLOOM176BParameterOpenAccess2022,
  title = {{{BLOOM}}: {{A 176B-Parameter Open-Access Multilingual Language Model}}},
  shorttitle = {{{BLOOM}}},
  author = {Workshop, BigScience and Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c c}ois and Gall{\'e}, Matthias and Tow, Jonathan and Rush, Alexander M. and Biderman, Stella and Webson, Albert and Ammanamanchi, Pawan Sasanka and Wang, Thomas and Sagot, Beno{\^i}t and Muennighoff, Niklas and {del Moral}, Albert Villanova and Ruwase, Olatunji and Bawden, Rachel and Bekman, Stas and {McMillan-Major}, Angelina and Beltagy, Iz and Nguyen, Huu and Saulnier, Lucile and Tan, Samson and Suarez, Pedro Ortiz and Sanh, Victor and Lauren{\c c}on, Hugo and Jernite, Yacine and Launay, Julien and Mitchell, Margaret and Raffel, Colin and Gokaslan, Aaron and Simhi, Adi and Soroa, Aitor and Aji, Alham Fikri and Alfassy, Amit and Rogers, Anna and Nitzav, Ariel Kreisberg and Xu, Canwen and Mou, Chenghao and Emezue, Chris and Klamm, Christopher and Leong, Colin and {van Strien}, Daniel and Adelani, David Ifeoluwa and Radev, Dragomir and Ponferrada, Eduardo Gonz{\'a}lez and Levkovizh, Efrat and Kim, Ethan and Natan, Eyal Bar and De Toni, Francesco and Dupont, G{\'e}rard and Kruszewski, Germ{\'a}n and Pistilli, Giada and Elsahar, Hady and Benyamina, Hamza and Tran, Hieu and Yu, Ian and Abdulmumin, Idris and Johnson, Isaac and {Gonzalez-Dios}, Itziar and {de la Rosa}, Javier and Chim, Jenny and Dodge, Jesse and Zhu, Jian and Chang, Jonathan and Frohberg, J{\"o}rg and Tobing, Joseph and Bhattacharjee, Joydeep and Almubarak, Khalid and Chen, Kimbo and Lo, Kyle and Von Werra, Leandro and Weber, Leon and Phan, Long and {allal}, Loubna Ben and Tanguy, Ludovic and Dey, Manan and Mu{\~n}oz, Manuel Romero and Masoud, Maraim and Grandury, Mar{\'i}a and {\v S}a{\v s}ko, Mario and Huang, Max and Coavoux, Maximin and Singh, Mayank and Jiang, Mike Tian-Jian and Vu, Minh Chien and Jauhar, Mohammad A. and Ghaleb, Mustafa and Subramani, Nishant and Kassner, Nora and Khamis, Nurulaqilla and Nguyen, Olivier and Espejel, Omar and {de Gibert}, Ona and Villegas, Paulo and Henderson, Peter and Colombo, Pierre and Amuok, Priscilla and Lhoest, Quentin and Harliman, Rheza and Bommasani, Rishi and L{\'o}pez, Roberto Luis and Ribeiro, Rui and Osei, Salomey and Pyysalo, Sampo and Nagel, Sebastian and Bose, Shamik and Muhammad, Shamsuddeen Hassan and Sharma, Shanya and Longpre, Shayne and Nikpoor, Somaieh and Silberberg, Stanislav and Pai, Suhas and Zink, Sydney and Torrent, Tiago Timponi and Schick, Timo and Thrush, Tristan and Danchev, Valentin and Nikoulina, Vassilina and Laippala, Veronika and Lepercq, Violette and Prabhu, Vrinda and Alyafeai, Zaid and Talat, Zeerak and Raja, Arun and Heinzerling, Benjamin and Si, Chenglei and Ta{\c s}ar, Davut Emre and Salesky, Elizabeth and Mielke, Sabrina J. and Lee, Wilson Y. and Sharma, Abheesht and Santilli, Andrea and Chaffin, Antoine and Stiegler, Arnaud and Datta, Debajyoti and Szczechla, Eliza and Chhablani, Gunjan and Wang, Han and Pandey, Harshit and Strobelt, Hendrik and Fries, Jason Alan and Rozen, Jos and Gao, Leo and Sutawika, Lintang and Bari, M. Saiful and {Al-shaibani}, Maged S. and Manica, Matteo and Nayak, Nihal and Teehan, Ryan and Albanie, Samuel and Shen, Sheng and {Ben-David}, Srulik and Bach, Stephen H. and Kim, Taewoon and Bers, Tali and Fevry, Thibault and Neeraj, Trishala and Thakker, Urmish and Raunak, Vikas and Tang, Xiangru and Yong, Zheng-Xin and Sun, Zhiqing and Brody, Shaked and Uri, Yallow and Tojarieh, Hadar and Roberts, Adam and Chung, Hyung Won and Tae, Jaesung and Phang, Jason and Press, Ofir and Li, Conglong and Narayanan, Deepak and Bourfoune, Hatim and Casper, Jared and Rasley, Jeff and Ryabinin, Max and Mishra, Mayank and Zhang, Minjia and Shoeybi, Mohammad and Peyrounette, Myriam and Patry, Nicolas and Tazi, Nouamane and Sanseviero, Omar and {von Platen}, Patrick and Cornette, Pierre and Lavall{\'e}e, Pierre Fran{\c c}ois and Lacroix, R{\'e}mi and Rajbhandari, Samyam and Gandhi, Sanchit and Smith, Shaden and Requena, St{\'e}phane and Patil, Suraj and Dettmers, Tim and Baruwa, Ahmed and Singh, Amanpreet and Cheveleva, Anastasia and Ligozat, Anne-Laure and Subramonian, Arjun and N{\'e}v{\'e}ol, Aur{\'e}lie and Lovering, Charles and Garrette, Dan and Tunuguntla, Deepak and Reiter, Ehud and Taktasheva, Ekaterina and Voloshina, Ekaterina and Bogdanov, Eli and Winata, Genta Indra and Schoelkopf, Hailey and Kalo, Jan-Christoph and Novikova, Jekaterina and Forde, Jessica Zosa and Clive, Jordan and Kasai, Jungo and Kawamura, Ken and Hazan, Liam and Carpuat, Marine and Clinciu, Miruna and Kim, Najoung and Cheng, Newton and Serikov, Oleg and Antverg, Omer and {van der Wal}, Oskar and Zhang, Rui and Zhang, Ruochen and Gehrmann, Sebastian and Mirkin, Shachar and Pais, Shani and Shavrina, Tatiana and Scialom, Thomas and Yun, Tian and Limisiewicz, Tomasz and Rieser, Verena and Protasov, Vitaly and Mikhailov, Vladislav and Pruksachatkun, Yada and Belinkov, Yonatan and Bamberger, Zachary and Kasner, Zden{\v e}k and Rueda, Alice and Pestana, Amanda and Feizpour, Amir and Khan, Ammar and Faranak, Amy and Santos, Ana and Hevia, Anthony and Unldreaj, Antigona and Aghagol, Arash and Abdollahi, Arezoo and Tammour, Aycha and HajiHosseini, Azadeh and Behroozi, Bahareh and Ajibade, Benjamin and Saxena, Bharat and Ferrandis, Carlos Mu{\~n}oz and Contractor, Danish and Lansky, David and David, Davis and Kiela, Douwe and Nguyen, Duong A. and Tan, Edward and Baylor, Emi and Ozoani, Ezinwanne and Mirza, Fatima and Ononiwu, Frankline and Rezanejad, Habib and Jones, Hessie and Bhattacharya, Indrani and Solaiman, Irene and Sedenko, Irina and Nejadgholi, Isar and Passmore, Jesse and Seltzer, Josh and Sanz, Julio Bonis and Dutra, Livia and Samagaio, Mairon and Elbadri, Maraim and Mieskes, Margot and Gerchick, Marissa and Akinlolu, Martha and McKenna, Michael and Qiu, Mike and Ghauri, Muhammed and Burynok, Mykola and Abrar, Nafis and Rajani, Nazneen and Elkott, Nour and Fahmy, Nour and Samuel, Olanrewaju and An, Ran and Kromann, Rasmus and Hao, Ryan and Alizadeh, Samira and Shubber, Sarmad and Wang, Silas and Roy, Sourav and Viguier, Sylvain and Le, Thanh and Oyebade, Tobi and Le, Trieu and Yang, Yoyo and Nguyen, Zach and Kashyap, Abhinav Ramesh and Palasciano, Alfredo and Callahan, Alison and Shukla, Anima and {Miranda-Escalada}, Antonio and Singh, Ayush and Beilharz, Benjamin and Wang, Bo and Brito, Caio and Zhou, Chenxi and Jain, Chirag and Xu, Chuxin and Fourrier, Cl{\'e}mentine and Peri{\~n}{\'a}n, Daniel Le{\'o}n and Molano, Daniel and Yu, Dian and Manjavacas, Enrique and Barth, Fabio and Fuhrimann, Florian and Altay, Gabriel and Bayrak, Giyaseddin and Burns, Gully and Vrabec, Helena U. and Bello, Imane and Dash, Ishani and Kang, Jihyun and Giorgi, John and Golde, Jonas and Posada, Jose David and Sivaraman, Karthik Rangasai and Bulchandani, Lokesh and Liu, Lu and Shinzato, Luisa and {de Bykhovetz}, Madeleine Hahn and Takeuchi, Maiko and P{\`a}mies, Marc and Castillo, Maria A. and Nezhurina, Marianna and S{\"a}nger, Mario and Samwald, Matthias and Cullan, Michael and Weinberg, Michael and De Wolf, Michiel and Mihaljcic, Mina and Liu, Minna and Freidank, Moritz and Kang, Myungsun and Seelam, Natasha and Dahlberg, Nathan and Broad, Nicholas Michio and Muellner, Nikolaus and Fung, Pascale and Haller, Patrick and Chandrasekhar, Ramya and Eisenberg, Renata and Martin, Robert and Canalli, Rodrigo and Su, Rosaline and Su, Ruisi and Cahyawijaya, Samuel and Garda, Samuele and Deshmukh, Shlok S. and Mishra, Shubhanshu and Kiblawi, Sid and Ott, Simon and {Sang-aroonsiri}, Sinee and Kumar, Srishti and Schweter, Stefan and Bharati, Sushil and Laud, Tanmay and Gigant, Th{\'e}o and Kainuma, Tomoya and Kusa, Wojciech and Labrak, Yanis and Bajaj, Yash Shailesh and Venkatraman, Yash and Xu, Yifan and Xu, Yingxin and Xu, Yu and Tan, Zhe and Xie, Zhongli and Ye, Zifan and Bras, Mathilde and Belkada, Younes and Wolf, Thomas},
  year = {2022},
  month = dec,
  number = {arXiv:2211.05100},
  eprint = {2211.05100},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2211.05100},
  urldate = {2023-01-17},
  abstract = {Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.},
  archiveprefix = {arXiv}
}

@article{wornowEHRSHOTEHRBenchmark2023,
  title = {{{EHRSHOT}}: {{An EHR Benchmark}} for {{Few-Shot Evaluation}} of {{Foundation Models}}},
  shorttitle = {{{EHRSHOT}}},
  author = {Wornow, Michael and Thapa, Rahul and Steinberg, Ethan and Fries, Jason and Shah, Nigam},
  year = {2023},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {67125--67137},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/d42db1f74df54cb992b3956eb7f15a6f-Abstract-Datasets_and_Benchmarks.html},
  urldate = {2024-03-13},
  langid = {english}
}

@article{wuAI4VISSurveyArtificial2022,
  title = {{{AI4VIS}}: {{Survey}} on {{Artificial Intelligence Approaches}} for {{Data Visualization}}},
  shorttitle = {{{AI4VIS}}},
  author = {Wu, Aoyu and Wang, Yun and Shu, Xinhuan and Moritz, Dominik and Cui, Weiwei and Zhang, Haidong and Zhang, Dongmei and Qu, Huamin},
  year = {2022},
  month = dec,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {28},
  number = {12},
  pages = {5049--5070},
  issn = {1941-0506},
  doi = {10.1109/TVCG.2021.3099002},
  urldate = {2023-10-12},
  abstract = {Visualizations themselves have become a data format. Akin to other data formats such as text and images, visualizations are increasingly created, stored, shared, and (re-)used with artificial intelligence (AI) techniques. In this survey, we probe the underlying vision of formalizing visualizations as an emerging data format and review the recent advance in applying AI techniques to visualization data (AI4VIS). We define visualization data as the digital representations of visualizations in computers and focus on data visualization (e.g., charts and infographics). We build our survey upon a corpus spanning ten different fields in computer science with an eye toward identifying important common interests. Our resulting taxonomy is organized around WHAT is visualization data and its representation, WHY and HOW to apply AI to visualization data. We highlight a set of common tasks that researchers apply to the visualization data and present a detailed discussion of AI approaches developed to accomplish those tasks. Drawing upon our literature review, we discuss several important research questions surrounding the management and exploitation of visualization data, as well as the role of AI in support of those processes. We make the list of surveyed papers and related material available online at.}
}

@article{wuGoogleNeuralMachine2016,
  title = {Google's {{Neural Machine Translation System}}: {{Bridging}} the {{Gap}} between {{Human}} and {{Machine Translation}}},
  shorttitle = {Google's {{Neural Machine Translation System}}},
  author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, {\L}ukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
  year = {2016},
  journal = {CoRR},
  volume = {abs/1609.08144},
  url = {http://arxiv.org/abs/1609.08144},
  urldate = {2022-01-20},
  annotation = {05182}
}

@article{wuHumanintheLoopAIEnhancing2023,
  title = {Toward {{Human-in-the-Loop AI}}: {{Enhancing Deep Reinforcement Learning}} via {{Real-Time Human Guidance}} for {{Autonomous Driving}}},
  shorttitle = {Toward {{Human-in-the-Loop AI}}},
  author = {Wu, Jingda and Huang, Zhiyu and Hu, Zhongxu and Lv, Chen},
  year = {2023},
  month = feb,
  journal = {Engineering},
  volume = {21},
  pages = {75--91},
  issn = {20958099},
  doi = {10.1016/j.eng.2022.05.017},
  urldate = {2025-06-04},
  abstract = {Due to its limited intelligence and abilities, machine learning is currently unable to handle various situations thus cannot completely replace humans in real-world applications. Because humans exhibit robustness and adaptability in complex scenarios, it is crucial to introduce humans into the training loop of artificial intelligence (AI), leveraging human intelligence to further advance machine learning algorithms. In this study, a real-time human-guidance-based (Hug)-deep reinforcement learning (DRL) method is developed for policy training in an end-to-end autonomous driving case. With our newly designed mechanism for control transfer between humans and automation, humans are able to intervene and correct the agent's unreasonable actions in real time when necessary during the model training process. Based on this human-in-the-loop guidance mechanism, an improved actor-critic architecture with modified policy and value networks is developed. The fast convergence of the proposed Hug-DRL allows real-time human guidance actions to be fused into the agent's training loop, further improving the efficiency and performance of DRL. The developed method is validated by human-in-the-loop experiments with 40 subjects and compared with other state-of-the-art learning approaches. The results suggest that the proposed method can effectively enhance the training efficiency and performance of the DRL algorithm under human guidance without imposing specific requirements on participants' expertise or experience.},
  langid = {english}
}

@misc{wuLanguageModelsPlan2024,
  title = {Do Language Models Plan Ahead for Future Tokens?},
  author = {Wu, Wilson and Morris, John X. and Levine, Lionel},
  year = {2024},
  month = mar,
  number = {arXiv:2404.00859},
  eprint = {2404.00859},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.00859},
  urldate = {2024-04-05},
  abstract = {Do transformers "think ahead" during inference at a given position? It is known transformers prepare information in the hidden states of the forward pass at \$t\$ that is then used in future forward passes \$t+{\textbackslash}tau\$. We posit two explanations for this phenomenon: pre-caching, in which off-diagonal gradient terms present in training result in the model computing features at \$t\$ irrelevant to the present inference task but useful for the future, and breadcrumbs, in which features most relevant to time step \$t\$ are already the same as those that would most benefit inference at time \$t+{\textbackslash}tau\$. We test these hypotheses by training language models without propagating gradients to past timesteps, a scheme we formalize as myopic training. In a synthetic data setting, we find clear evidence for pre-caching. In the autoregressive language modeling setting, our experiments are more suggestive of the breadcrumbs hypothesis.},
  archiveprefix = {arXiv}
}

@misc{wuPMCLLaMABuildingOpensource2023,
  title = {{{PMC-LLaMA}}: {{Towards Building Open-source Language Models}} for {{Medicine}}},
  shorttitle = {{{PMC-LLaMA}}},
  author = {Wu, Chaoyi and Lin, Weixiong and Zhang, Xiaoman and Zhang, Ya and Wang, Yanfeng and Xie, Weidi},
  year = {2023},
  month = aug,
  number = {arXiv:2304.14454},
  eprint = {2304.14454},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2304.14454},
  urldate = {2024-01-20},
  abstract = {Recently, Large Language Models (LLMs) have showcased remarkable capabilities in natural language understanding. While demonstrating proficiency in everyday conversations and question-answering situations, these models frequently struggle in domains that require precision, such as medical applications, due to their lack of domain-specific knowledge. In this paper, we describe the procedure for building a powerful, open-source language model specifically designed for medicine applications, termed as PMC-LLaMA. Our contributions are threefold: (i) we systematically investigate the process of adapting a general-purpose foundation language model towards medical domain, this involves data-centric knowledge injection through the integration of 4.8M biomedical academic papers and 30K medical textbooks, as well as comprehensive fine-tuning for alignment with domain-specific instructions; (ii) we contribute a large-scale, comprehensive dataset for instruction tuning. This dataset encompasses medical question-answering (QA), rationale for reasoning, and conversational dialogues, comprising a total of 202M tokens; (iii) we conduct thorough ablation studies to demonstrate the effectiveness of each proposed component. While evaluating on various public medical question-answering benchmarks, our lightweight PMCLLaMA, which consists of only 13 billion parameters, exhibits superior performance, even surpassing ChatGPT. All models, codes, datasets can be found in https://github.com/chaoyi-wu/PMC-LLaMA.},
  archiveprefix = {arXiv}
}

@inproceedings{wuPrecedentEnhancedLegalJudgment2023,
  title = {Precedent-{{Enhanced Legal Judgment Prediction}} with {{LLM}} and {{Domain-Model Collaboration}}},
  author = {Wu, Yiquan and Zhou, Siying and Liu, Yifei and Lu, Weiming and Liu, Xiaozhong and Zhang, Yating and Sun, Changlong and Wu, Fei and Kuang, Kun},
  year = {2023},
  month = oct,
  url = {https://www.semanticscholar.org/paper/Precedent-Enhanced-Legal-Judgment-Prediction-with-Wu-Zhou/a80546c9847710af1ba8d5f8dca9386e7a520d0a},
  urldate = {2023-10-16},
  abstract = {Legal Judgment Prediction (LJP) has become an increasingly crucial task in Legal AI, i.e., predicting the judgment of the case in terms of case fact description. Precedents are the previous legal cases with similar facts, which are the basis for the judgment of the subsequent case in national legal systems. Thus, it is worthwhile to explore the utilization of precedents in the LJP. Recent advances in deep learning have enabled a variety of techniques to be used to solve the LJP task. These can be broken down into two categories: large language models (LLMs) and domain-specific models. LLMs are capable of interpreting and generating complex natural language, while domain models are efficient in learning task-specific information. In this paper, we propose the precedent-enhanced LJP framework (PLJP), a system that leverages the strength of both LLM and domain models in the context of precedents. Specifically, the domain models are designed to provide candidate labels and find the proper precedents efficiently, and the large models will make the final prediction with an in-context precedents comprehension. Experiments on the real-world dataset demonstrate the effectiveness of our PLJP. Moreover, our work shows a promising direction for LLM and domain-model collaboration that can be generalized to other vertical domains.}
}

@misc{wuPrecedentEnhancedLegalJudgment2023a,
  title = {Precedent-{{Enhanced Legal Judgment Prediction}} with {{LLM}} and {{Domain-Model Collaboration}}},
  author = {Wu, Yiquan and Zhou, Siying and Liu, Yifei and Lu, Weiming and Liu, Xiaozhong and Zhang, Yating and Sun, Changlong and Wu, Fei and Kuang, Kun},
  year = {2023},
  month = oct,
  number = {arXiv:2310.09241},
  eprint = {2310.09241},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2310.09241},
  urldate = {2023-10-16},
  abstract = {Legal Judgment Prediction (LJP) has become an increasingly crucial task in Legal AI, i.e., predicting the judgment of the case in terms of case fact description. Precedents are the previous legal cases with similar facts, which are the basis for the judgment of the subsequent case in national legal systems. Thus, it is worthwhile to explore the utilization of precedents in the LJP. Recent advances in deep learning have enabled a variety of techniques to be used to solve the LJP task. These can be broken down into two categories: large language models (LLMs) and domain-specific models. LLMs are capable of interpreting and generating complex natural language, while domain models are efficient in learning task-specific information. In this paper, we propose the precedent-enhanced LJP framework (PLJP), a system that leverages the strength of both LLM and domain models in the context of precedents. Specifically, the domain models are designed to provide candidate labels and find the proper precedents efficiently, and the large models will make the final prediction with an in-context precedents comprehension. Experiments on the real-world dataset demonstrate the effectiveness of our PLJP. Moreover, our work shows a promising direction for LLM and domain-model collaboration that can be generalized to other vertical domains.},
  archiveprefix = {arXiv}
}

@article{wuSessionBasedRecommendationGraph2019,
  title = {Session-{{Based Recommendation}} with {{Graph Neural Networks}}},
  author = {Wu, Shu and Tang, Yuyuan and Zhu, Yanqiao and Wang, Liang and Xie, Xing and Tan, Tieniu},
  year = {2019},
  month = jul,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {33},
  number = {01},
  pages = {346--353},
  issn = {2374-3468},
  doi = {10.1609/aaai.v33i01.3301346},
  urldate = {2024-09-24},
  abstract = {The problem of session-based recommendation aims to predict user actions based on anonymous sessions. Previous methods model a session as a sequence and estimate user representations besides item representations to make recommendations. Though achieved promising results, they are insufficient to obtain accurate user vectors in sessions and neglect complex transitions of items. To obtain accurate item embedding and take complex transitions of items into account, we propose a novel method, i.e. Session-based Recommendation with Graph Neural Networks, SR-GNN for brevity. In the proposed method, session sequences are modeled as graphstructured data. Based on the session graph, GNN can capture complex transitions of items, which are difficult to be revealed by previous conventional sequential methods. Each session is then represented as the composition of the global preference and the current interest of that session using an attention network. Extensive experiments conducted on two real datasets show that SR-GNN evidently outperforms the state-of-the-art session-based recommendation methods consistently.},
  copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
  langid = {english}
}

@misc{wuWIPINewWeb2024,
  title = {{{WIPI}}: {{A New Web Threat}} for {{LLM-Driven Web Agents}}},
  shorttitle = {{{WIPI}}},
  author = {Wu, Fangzhou and Wu, Shutong and Cao, Yulong and Xiao, Chaowei},
  year = {2024},
  month = feb,
  number = {arXiv:2402.16965},
  eprint = {2402.16965},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.16965},
  urldate = {2024-08-08},
  abstract = {With the fast development of large language models (LLMs), LLM-driven Web Agents (Web Agents for short) have obtained tons of attention due to their superior capability where LLMs serve as the core part of making decisions like the human brain equipped with multiple web tools to actively interact with external deployed websites. As uncountable Web Agents have been released and such LLM systems are experiencing rapid development and drawing closer to widespread deployment in our daily lives, an essential and pressing question arises: "Are these Web Agents secure?". In this paper, we introduce a novel threat, WIPI, that indirectly controls Web Agent to execute malicious instructions embedded in publicly accessible webpages. To launch a successful WIPI works in a black-box environment. This methodology focuses on the form and content of indirect instructions within external webpages, enhancing the efficiency and stealthiness of the attack. To evaluate the effectiveness of the proposed methodology, we conducted extensive experiments using 7 plugin-based ChatGPT Web Agents, 8 Web GPTs, and 3 different open-source Web Agents. The results reveal that our methodology achieves an average attack success rate (ASR) exceeding 90\% even in pure black-box scenarios. Moreover, through an ablation study examining various user prefix instructions, we demonstrated that the WIPI exhibits strong robustness, maintaining high performance across diverse prefix instructions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security}
}

@inproceedings{xiangSimUserGeneratingUsability2024,
  title = {{{SimUser}}: {{Generating Usability Feedback}} by {{Simulating Various Users Interacting}} with {{Mobile Applications}}},
  shorttitle = {{{SimUser}}},
  booktitle = {Proceedings of the {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Xiang, Wei and Zhu, Hanfei and Lou, Suqi and Chen, Xinli and Pan, Zhenghua and Jin, Yuping and Chen, Shi and Sun, Lingyun},
  year = {2024},
  month = may,
  series = {{{CHI}} '24},
  pages = {1--17},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3613904.3642481},
  urldate = {2024-09-10},
  abstract = {The conflict between the rapid iteration demand of prototyping and the time-consuming nature of user tests has led researchers to adopt AI methods to identify usability issues. However, these AI-driven methods concentrate on evaluating the feasibility of a system, while often overlooking the influence of specified user characteristics and usage contexts. Our work proposes a tool named SimUser based on large language models (LLMs) with the Chain-of-Thought structure and user modeling method. It generates usability feedback by simulating the interaction between users and applications, which is influenced by user characteristics and contextual factors. The empirical study (48 human users and 21 designers) validated that in the context of a simple smartwatch interface, SimUser could generate heuristic usability feedback with the similarity varying from 35.7\% to 100\% according to the user groups and usability category. Our work provides insights into simulating users by LLM to improve future design activities.},
  isbn = {979-8-4007-0330-0}
}

@misc{xiaoCAIL2018LargeScaleLegal2018,
  title = {{{CAIL2018}}: {{A Large-Scale Legal Dataset}} for {{Judgment Prediction}}},
  shorttitle = {{{CAIL2018}}},
  author = {Xiao, Chaojun and Zhong, Haoxi and Guo, Zhipeng and Tu, Cunchao and Liu, Zhiyuan and Sun, Maosong and Feng, Yansong and Han, Xianpei and Hu, Zhen and Wang, Heng and Xu, Jianfeng},
  year = {2018},
  month = jul,
  number = {arXiv:1807.02478},
  eprint = {1807.02478},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/1807.02478},
  urldate = {2023-10-16},
  abstract = {In this paper, we introduce the {\textbackslash}textbf\{C\}hinese {\textbackslash}textbf\{AI\} and {\textbackslash}textbf\{L\}aw challenge dataset (CAIL2018), the first large-scale Chinese legal dataset for judgment prediction. {\textbackslash}dataset contains more than \$2.6\$ million criminal cases published by the Supreme People's Court of China, which are several times larger than other datasets in existing works on judgment prediction. Moreover, the annotations of judgment results are more detailed and rich. It consists of applicable law articles, charges, and prison terms, which are expected to be inferred according to the fact descriptions of cases. For comparison, we implement several conventional text classification baselines for judgment prediction and experimental results show that it is still a challenge for current models to predict the judgment results of legal cases, especially on prison terms. To help the researchers make improvements on legal judgment prediction, both {\textbackslash}dataset and baselines will be released after the CAIL competition{\textbackslash}footnote\{http://cail.cipsc.org.cn/\}.},
  archiveprefix = {arXiv}
}

@misc{xiaoEfficientStreamingLanguage2023,
  title = {Efficient {{Streaming Language Models}} with {{Attention Sinks}}},
  author = {Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  year = {2023},
  month = sep,
  number = {arXiv:2309.17453},
  eprint = {2309.17453},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2309.17453},
  urldate = {2023-10-02},
  abstract = {Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a ``sink'' even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.},
  archiveprefix = {arXiv}
}

@inproceedings{xiaoFreeALHumanFreeActive2023,
  title = {{{FreeAL}}: {{Towards Human-Free Active Learning}} in the {{Era}} of {{Large Language Models}}},
  shorttitle = {{{FreeAL}}},
  booktitle = {Proceedings of the 2023 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Xiao, Ruixuan and Dong, Yiwen and Zhao, Junbo and Wu, Runze and Lin, Minmin and Chen, Gang and Wang, Haobo},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  year = {2023},
  month = dec,
  pages = {14520--14535},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.896},
  urldate = {2024-02-11},
  abstract = {Collecting high-quality labeled data for model training is notoriously time-consuming and labor-intensive for various NLP tasks. While copious solutions, such as active learning for small language models (SLMs) and prevalent in-context learning in the era of large language models (LLMs), have been proposed and alleviate the labeling burden to some extent, their performances are still subject to human intervention. It is still underexplored how to reduce the annotation cost in the LLMs era. To bridge this, we revolutionize traditional active learning and propose an innovative collaborative learning framework FreeAL to interactively distill and filter the task-specific knowledge from LLMs. During collaborative training, an LLM serves as an active annotator inculcating its coarse-grained knowledge, while a downstream SLM is incurred as a student to filter out high-quality in-context samples to feedback LLM for the subsequent label refinery. Extensive experiments on eight benchmark datasets demonstrate that FreeAL largely enhances the zero-shot performances for both SLM and LLM without any human supervision.}
}

@misc{xiaoSimulatingPublicAdministration2023a,
  title = {Simulating {{Public Administration Crisis}}: {{A Novel Generative Agent-Based Simulation System}} to {{Lower Technology Barriers}} in {{Social Science Research}}},
  shorttitle = {Simulating {{Public Administration Crisis}}},
  author = {Xiao, Bushi and Yin, Ziyuan and Shan, Zixuan},
  year = {2023},
  month = nov,
  number = {arXiv:2311.06957},
  eprint = {2311.06957},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.06957},
  urldate = {2024-07-09},
  abstract = {This article proposes a social simulation paradigm based on the GPT-3.5 large language model. It involves constructing Generative Agents that emulate human cognition, memory, and decision-making frameworks, along with establishing a virtual social system capable of stable operation and an insertion mechanism for standardized public events. The project focuses on simulating a township water pollution incident, enabling the comprehensive examination of a virtual government's response to a specific public administration event. Controlled variable experiments demonstrate that the stored memory in generative agents significantly influences both individual decision-making and social networks. The Generative Agent-Based Simulation System introduces a novel approach to social science and public administration research. Agents exhibit personalized customization, and public events are seamlessly incorporated through natural language processing. Its high flexibility and extensive social interaction render it highly applicable in social science investigations. The system effectively reduces the complexity associated with building intricate social simulations while enhancing its interpretability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society}
}

@misc{xiaoSmartInmailDemo2020,
  title = {Smart {{Inmail Demo}}},
  author = {Xiao, Charles},
  year = {2020},
  month = dec,
  url = {https://docs.google.com/presentation/d/1IeINOeVhQn1rnoVJ3_IKfUkntdnBgdPQWFxOWL_lxNk},
  urldate = {2022-08-25}
}

@misc{xiaoUncertaintyQuantificationPretrained2022,
  title = {Uncertainty {{Quantification}} with {{Pre-trained Language Models}}: {{A Large-Scale Empirical Analysis}}},
  shorttitle = {Uncertainty {{Quantification}} with {{Pre-trained Language Models}}},
  author = {Xiao, Yuxin and Liang, Paul Pu and Bhatt, Umang and Neiswanger, Willie and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
  year = {2022},
  month = oct,
  number = {arXiv:2210.04714},
  eprint = {2210.04714},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.04714},
  urldate = {2023-12-20},
  abstract = {Pre-trained language models (PLMs) have gained increasing popularity due to their compelling prediction performance in diverse natural language processing (NLP) tasks. When formulating a PLM-based prediction pipeline for NLP tasks, it is also crucial for the pipeline to minimize the calibration error, especially in safety-critical applications. That is, the pipeline should reliably indicate when we can trust its predictions. In particular, there are various considerations behind the pipeline: (1) the choice and (2) the size of PLM, (3) the choice of uncertainty quantifier, (4) the choice of fine-tuning loss, and many more. Although prior work has looked into some of these considerations, they usually draw conclusions based on a limited scope of empirical studies. There still lacks a holistic analysis on how to compose a well-calibrated PLM-based prediction pipeline. To fill this void, we compare a wide range of popular options for each consideration based on three prevalent NLP classification tasks and the setting of domain shift. In response, we recommend the following: (1) use ELECTRA for PLM encoding, (2) use larger PLMs if possible, (3) use Temp Scaling as the uncertainty quantifier, and (4) use Focal Loss for fine-tuning.},
  archiveprefix = {arXiv}
}

@misc{xiaSelectionGenerationSurvey2025,
  title = {From {{Selection}} to {{Generation}}: {{A Survey}} of {{LLM-based Active Learning}}},
  shorttitle = {From {{Selection}} to {{Generation}}},
  author = {Xia, Yu and Mukherjee, Subhojyoti and Xie, Zhouhang and Wu, Junda and Li, Xintong and Aponte, Ryan and Lyu, Hanjia and Barrow, Joe and Chen, Hongjie and Dernoncourt, Franck and Kveton, Branislav and Yu, Tong and Zhang, Ruiyi and Gu, Jiuxiang and Ahmed, Nesreen K. and Wang, Yu and Chen, Xiang and Deilamsalehy, Hanieh and Kim, Sungchul and Hu, Zhengmian and Zhao, Yue and Lipka, Nedim and Yoon, Seunghyun and Huang, Ting-Hao Kenneth and Wang, Zichao and Mathur, Puneet and Pal, Soumyabrata and Mukherjee, Koyel and Zhang, Zhehao and Park, Namyong and Nguyen, Thien Huu and Luo, Jiebo and Rossi, Ryan A. and McAuley, Julian},
  year = {2025},
  month = feb,
  number = {arXiv:2502.11767},
  eprint = {2502.11767},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.11767},
  urldate = {2025-05-16},
  abstract = {Active Learning (AL) has been a powerful paradigm for improving model efficiency and performance by selecting the most informative data points for labeling and training. In recent active learning frameworks, Large Language Models (LLMs) have been employed not only for selection but also for generating entirely new data instances and providing more cost-effective annotations. Motivated by the increasing importance of high-quality data and efficient model training in the era of LLMs, we present a comprehensive survey on LLM-based Active Learning. We introduce an intuitive taxonomy that categorizes these techniques and discuss the transformative roles LLMs can play in the active learning loop. We further examine the impact of AL on LLM learning paradigms and its applications across various domains. Finally, we identify open challenges and propose future research directions. This survey aims to serve as an up-to-date resource for researchers and practitioners seeking to gain an intuitive understanding of LLM-based AL techniques and deploy them to new applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{xiaShearedLLaMAAccelerating2023,
  title = {Sheared {{LLaMA}}: {{Accelerating Language Model Pre-training}} via {{Structured Pruning}}},
  shorttitle = {Sheared {{LLaMA}}},
  author = {Xia, Mengzhou and Gao, Tianyu and Zeng, Zhiyuan and Chen, Danqi},
  year = {2023},
  month = oct,
  number = {arXiv:2310.06694},
  eprint = {2310.06694},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.06694},
  urldate = {2023-10-12},
  abstract = {The popularity of LLaMA (Touvron et al., 2023a;b) and other recently emerged moderate-sized large language models (LLMs) highlights the potential of building smaller yet powerful LLMs. Regardless, the cost of training such models from scratch on trillions of tokens remains high. In this work, we study structured pruning as an effective means to develop smaller LLMs from pre-trained, larger models. Our approach employs two key techniques: (1) targeted structured pruning, which prunes a larger model to a specified target shape by removing layers, heads, and intermediate and hidden dimensions in an end-to-end manner, and (2) dynamic batch loading, which dynamically updates the composition of sampled data in each training batch based on varying losses across different domains. We demonstrate the efficacy of our approach by presenting the Sheared-LLaMA series, pruning the LLaMA2-7B model down to 1.3B and 2.7B parameters. Sheared-LLaMA models outperform state-of-the-art open-source models of equivalent sizes, such as Pythia, INCITE, and OpenLLaMA models, on a wide range of downstream and instruction tuning evaluations, while requiring only 3\% of compute compared to training such models from scratch. This work provides compelling evidence that leveraging existing LLMs with structured pruning is a far more cost-effective approach for building smaller LLMs.},
  archiveprefix = {arXiv}
}

@misc{xieCanLargeLanguage2024,
  title = {Can {{Large Language Model Agents Simulate Human Trust Behavior}}?},
  author = {Xie, Chengxing and Chen, Canyu and Jia, Feiran and Ye, Ziyu and Lai, Shiyang and Shu, Kai and Gu, Jindong and Bibi, Adel and Hu, Ziniu and Jurgens, David and Evans, James and Torr, Philip and Ghanem, Bernard and Li, Guohao},
  year = {2024},
  month = nov,
  number = {arXiv:2402.04559},
  eprint = {2402.04559},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.04559},
  urldate = {2025-03-21},
  abstract = {Large Language Model (LLM) agents have been increasingly adopted as simulation tools to model humans in social science and role-playing applications. However, one fundamental question remains: can LLM agents really simulate human behavior? In this paper, we focus on one critical and elemental behavior in human interactions, trust, and investigate whether LLM agents can simulate human trust behavior. We first find that LLM agents generally exhibit trust behavior, referred to as agent trust, under the framework of Trust Games, which are widely recognized in behavioral economics. Then, we discover that GPT-4 agents manifest high behavioral alignment with humans in terms of trust behavior, indicating the feasibility of simulating human trust behavior with LLM agents. In addition, we probe the biases of agent trust and differences in agent trust towards other LLM agents and humans. We also explore the intrinsic properties of agent trust under conditions including external manipulations and advanced reasoning strategies. Our study provides new insights into the behaviors of LLM agents and the fundamental analogy between LLMs and humans beyond value alignment. We further illustrate broader implications of our discoveries for applications where trust is paramount.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction}
}

@misc{xieWhenAskHelp2022,
  title = {When to {{Ask}} for {{Help}}: {{Proactive Interventions}} in {{Autonomous Reinforcement Learning}}},
  shorttitle = {When to {{Ask}} for {{Help}}},
  author = {Xie, Annie and Tajwar, Fahim and Sharma, Archit and Finn, Chelsea},
  year = {2022},
  month = oct,
  number = {arXiv:2210.10765},
  eprint = {2210.10765},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.10765},
  urldate = {2025-06-04},
  abstract = {A long-term goal of reinforcement learning is to design agents that can autonomously interact and learn in the world. A critical challenge to such autonomy is the presence of irreversible states which require external assistance to recover from, such as when a robot arm has pushed an object off of a table. While standard agents require constant monitoring to decide when to intervene, we aim to design proactive agents that can request human intervention only when needed. To this end, we propose an algorithm that efficiently learns to detect and avoid states that are irreversible, and proactively asks for help in case the agent does enter them. On a suite of continuous control environments with unknown irreversible states, we find that our algorithm exhibits better sample- and intervention-efficiency compared to existing methods. Our code is publicly available at https://sites.google.com/view/proactive-interventions},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@misc{xuBaizeOpenSourceChat2023,
  title = {Baize: {{An Open-Source Chat Model}} with {{Parameter-Efficient Tuning}} on {{Self-Chat Data}}},
  shorttitle = {Baize},
  author = {Xu, Canwen and Guo, Daya and Duan, Nan and McAuley, Julian},
  year = {2023},
  month = apr,
  number = {arXiv:2304.01196},
  eprint = {2304.01196},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2304.01196},
  urldate = {2023-04-08},
  abstract = {Chat models, such as ChatGPT, have shown impressive capabilities and have been rapidly adopted across numerous domains. However, these models are only accessible through a restricted API, creating barriers for new research and progress in the field. We propose a pipeline that can automatically generate a high-quality multi-turn chat corpus by leveraging ChatGPT to engage in a conversation with itself. Subsequently, we employ parameter-efficient tuning to enhance LLaMA, an open-source large language model. The resulting model, named Baize, demonstrates good performance in multi-turn dialogues with guardrails that minimize potential risks. The Baize models and data are released for research purposes only at https://github.com/project-baize/baize. An online demo is also available at https://huggingface.co/spaces/project-baize/baize-lora-7B.},
  archiveprefix = {arXiv}
}

@misc{xuFantasticQuestionsWhere2022,
  title = {Fantastic {{Questions}} and {{Where}} to {{Find Them}}: {{FairytaleQA}} -- {{An Authentic Dataset}} for {{Narrative Comprehension}}},
  shorttitle = {Fantastic {{Questions}} and {{Where}} to {{Find Them}}},
  author = {Xu, Ying and Wang, Dakuo and Yu, Mo and Ritchie, Daniel and Yao, Bingsheng and Wu, Tongshuang and Zhang, Zheng and Li, Toby Jia-Jun and Bradford, Nora and Sun, Branda and Hoang, Tran Bao and Sang, Yisi and Hou, Yufang and Ma, Xiaojuan and Yang, Diyi and Peng, Nanyun and Yu, Zhou and Warschauer, Mark},
  year = {2022},
  month = mar,
  number = {arXiv:2203.13947},
  eprint = {2203.13947},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2203.13947},
  urldate = {2023-11-08},
  abstract = {Question answering (QA) is a fundamental means to facilitate assessment and training of narrative comprehension skills for both machines and young children, yet there is scarcity of high-quality QA datasets carefully designed to serve this purpose. In particular, existing datasets rarely distinguish fine-grained reading skills, such as the understanding of varying narrative elements. Drawing on the reading education research, we introduce FairytaleQA, a dataset focusing on narrative comprehension of kindergarten to eighth-grade students. Generated by educational experts based on an evidence-based theoretical framework, FairytaleQA consists of 10,580 explicit and implicit questions derived from 278 children-friendly stories, covering seven types of narrative elements or relations. Our dataset is valuable in two folds: First, we ran existing QA models on our dataset and confirmed that this annotation helps assess models' fine-grained learning skills. Second, the dataset supports question generation (QG) task in the education domain. Through benchmarking with QG models, we show that the QG model trained on FairytaleQA is capable of asking high-quality and more diverse questions.},
  archiveprefix = {arXiv}
}

@inproceedings{xuGraphContextualizedSelfAttention2019,
  title = {Graph {{Contextualized Self-Attention Network}} for {{Session-based Recommendation}}},
  booktitle = {Proceedings of the {{Twenty-Eighth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Xu, Chengfeng and Zhao, Pengpeng and Liu, Yanchi and Sheng, Victor S. and Xu, Jiajie and Zhuang, Fuzhen and Fang, Junhua and Zhou, Xiaofang},
  year = {2019},
  month = aug,
  pages = {3940--3946},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  address = {Macao, China},
  doi = {10.24963/ijcai.2019/547},
  urldate = {2024-09-24},
  abstract = {Session-based recommendation, which aims to predict the user's immediate next action based on anonymous sessions, is a key task in many online services (e.g., e-commerce, media streaming). Recently, Self-Attention Network (SAN) has achieved significant success in various sequence modeling tasks without using either recurrent or convolutional network. However, SAN lacks local dependencies that exist over adjacent items and limits its capacity for learning contextualized representations of items in sequences. In this paper, we propose a graph contextualized self-attention model (GC-SAN), which utilizes both graph neural network and self-attention mechanism, for sessionbased recommendation. In GC-SAN, we dynamically construct a graph structure for session sequences and capture rich local dependencies via graph neural network (GNN). Then each session learns long-range dependencies by applying the self-attention mechanism. Finally, each session is represented as a linear combination of the global preference and the current interest of that session. Extensive experiments on two real-world datasets show that GC-SAN outperforms state-of-the-art methods consistently.},
  isbn = {978-0-9992411-4-1},
  langid = {english}
}

@misc{xuMentalLLMLeveragingLarge2023,
  title = {Mental-{{LLM}}: {{Leveraging Large Language Models}} for {{Mental Health Prediction}} via {{Online Text Data}}},
  shorttitle = {Mental-{{LLM}}},
  author = {Xu, Xuhai and Yao, Bingsheng and Dong, Yuanzhe and Gabriel, Saadia and Yu, Hong and Hendler, James and Ghassemi, Marzyeh and Dey, Anind K. and Wang, Dakuo},
  year = {2023},
  month = sep,
  number = {arXiv:2307.14385},
  eprint = {2307.14385},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2307.14385},
  urldate = {2023-10-12},
  abstract = {Advances in large language models (LLMs) have empowered a variety of applications. However, there is still a significant gap in research when it comes to understanding and enhancing the capabilities of LLMs in the field of mental health. In this work, we present the first comprehensive evaluation of multiple LLMs, including Alpaca, Alpaca-LoRA, FLAN-T5, GPT-3.5, and GPT-4, on various mental health prediction tasks via online text data. We conduct a broad range of experiments, covering zero-shot prompting, few-shot prompting, and instruction fine-tuning. The results indicate a promising yet limited performance of LLMs with zero-shot and few-shot prompt designs for the mental health tasks. More importantly, our experiments show that instruction finetuning can significantly boost the performance of LLMs for all tasks simultaneously. Our best-finetuned models, Mental-Alpaca and Mental-FLAN-T5, outperform the best prompt design of GPT-3.5 (25 and 15 times bigger) by 10.9\% on balanced accuracy and the best of GPT-4 (250 and 150 times bigger) by 4.8\%. They further perform on par with the state-of-the-art task-specific language model. We also conduct an exploratory case study on LLMs' capability on the mental health reasoning tasks, illustrating the promising capability of certain models such as GPT-4. We summarize our findings into a set of action guidelines for potential methods to enhance LLMs' capability for mental health tasks. Meanwhile, we also emphasize the important limitations before achieving deployability in real-world mental health settings, such as known racial and gender bias. We highlight the important ethical risks accompanying this line of research.},
  archiveprefix = {arXiv}
}

@misc{xuNECENarrativeEvent2023,
  title = {{{NECE}}: {{Narrative Event Chain Extraction Toolkit}}},
  shorttitle = {{{NECE}}},
  author = {Xu, Guangxuan and Isaza, Paulina Toro and Li, Moshi and Oloko, Akintoye and Yao, Bingsheng and Sanctos, Cassia and Adebiyi, Aminat and Hou, Yufang and Peng, Nanyun and Wang, Dakuo},
  year = {2023},
  month = feb,
  number = {arXiv:2208.08063},
  eprint = {2208.08063},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2208.08063},
  urldate = {2023-05-11},
  abstract = {To understand a narrative, it is essential to comprehend its main characters and the associated major events; however, this can be challenging with lengthy and unstructured narrative texts. To address this, we introduce NECE, an open-access, document-level toolkit that automatically extracts and aligns narrative events in the temporal order of their occurrence using sliding window method. Through extensive human evaluations, we have confirmed the high quality of the NECE toolkit, and external validation has demonstrated its potential for application in downstream tasks such as question answering and bias analysis. The NECE toolkit includes both a Python library and a user-friendly web interface; the latter offers custom visualizations of event chains and easy navigation between graphics and text to improve reading efficiency and experience.},
  archiveprefix = {arXiv}
}

@misc{xuParameterEfficientFineTuningMethods2023,
  title = {Parameter-{{Efficient Fine-Tuning Methods}} for {{Pretrained Language Models}}: {{A Critical Review}} and {{Assessment}}},
  shorttitle = {Parameter-{{Efficient Fine-Tuning Methods}} for {{Pretrained Language Models}}},
  author = {Xu, Lingling and Xie, Haoran and Qin, Si-Zhao Joe and Tao, Xiaohui and Wang, Fu Lee},
  year = {2023},
  month = dec,
  number = {arXiv:2312.12148},
  eprint = {2312.12148},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.12148},
  urldate = {2024-02-13},
  abstract = {With the continuous growth in the number of parameters of transformer-based pretrained language models (PLMs), particularly the emergence of large language models (LLMs) with billions of parameters, many natural language processing (NLP) tasks have demonstrated remarkable success. However, the enormous size and computational demands of these models pose significant challenges for adapting them to specific downstream tasks, especially in environments with limited computational resources. Parameter Efficient Fine-Tuning (PEFT) offers an effective solution by reducing the number of fine-tuning parameters and memory usage while achieving comparable performance to full fine-tuning. The demands for fine-tuning PLMs, especially LLMs, have led to a surge in the development of PEFT methods, as depicted in Fig. 1. In this paper, we present a comprehensive and systematic review of PEFT methods for PLMs. We summarize these PEFT methods, discuss their applications, and outline future directions. Furthermore, we conduct experiments using several representative PEFT methods to better understand their effectiveness in parameter efficiency and memory efficiency. By offering insights into the latest advancements and practical applications, this survey serves as an invaluable resource for researchers and practitioners seeking to navigate the challenges and opportunities presented by PEFT in the context of PLMs.},
  archiveprefix = {arXiv}
}

@misc{xuRAMEHRRetrievalAugmentation2024,
  title = {{{RAM-EHR}}: {{Retrieval Augmentation Meets Clinical Predictions}} on {{Electronic Health Records}}},
  shorttitle = {{{RAM-EHR}}},
  author = {Xu, Ran and Shi, Wenqi and Yu, Yue and Zhuang, Yuchen and Jin, Bowen and Wang, May D. and Ho, Joyce C. and Yang, Carl},
  year = {2024},
  month = feb,
  number = {arXiv:2403.00815},
  eprint = {2403.00815},
  primaryclass = {cs, q-bio},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.00815},
  urldate = {2024-04-05},
  abstract = {We present RAM-EHR, a Retrieval AugMentation pipeline to improve clinical predictions on Electronic Health Records (EHRs). RAM-EHR first collects multiple knowledge sources, converts them into text format, and uses dense retrieval to obtain information related to medical concepts. This strategy addresses the difficulties associated with complex names for the concepts. RAM-EHR then augments the local EHR predictive model co-trained with consistency regularization to capture complementary information from patient visits and summarized knowledge. Experiments on two EHR datasets show the efficacy of RAM-EHR over previous knowledge-enhanced baselines (3.4\% gain in AUROC and 7.2\% gain in AUPR), emphasizing the effectiveness of the summarized knowledge from RAM-EHR for clinical prediction tasks. The code will be published at {\textbackslash}url\{https://github.com/ritaranx/RAM-EHR\}.},
  archiveprefix = {arXiv}
}

@inproceedings{xuTransformerReasoningNetwork2021,
  title = {Transformer {{Reasoning Network}} for {{Personalized Review Summarization}}},
  booktitle = {Proceedings of the 44th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Xu, Hongyan and Liu, Hongtao and Jiao, Pengfei and Wang, Wenjun},
  year = {2021},
  month = jul,
  series = {{{SIGIR}} '21},
  pages = {1452--1461},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3404835.3462854},
  urldate = {2022-07-24},
  abstract = {Review summarization aims to generate condensed text for online product reviews, and has attracted more and more attention in E-commerce platforms. In addition to the input review, the quality of generated summaries is highly related to the characteristics of users and products, e.g., their historical summaries, which could provide useful clues for the target summary generation. However, most previous works ignore the underlying interaction between the given input review and the corresponding historical summaries. Therefore, we aim to explore how to effectively incorporate the history information into the summary generation. In this paper, we propose a novel transformer-based reasoning framework for personalized review summarization. We design an elaborately adapted transformer network containing an encoder and a decoder, to fully infer the important and informative parts among the historical summaries in terms of the input review to generate more comprehensive summaries. In the encoder of our approach, we develop an inter- and intra-attention to involve the history information selectively to learn the personalized representation of the input review. In the decoder part, we propose to incorporate the constructed reasoning memory learning from historical summaries into the original transformer decoder, and design a memory-decoder attention module to retrieve more useful information for the final summary generation. Extensive experiments are conducted and the results show our approach could generate more reasonable summaries for recommendation, and outperform many competitive baseline methods.},
  isbn = {978-1-4503-8037-9}
}

@misc{yamadaLUKEDeepContextualized2020,
  title = {{{LUKE}}: {{Deep Contextualized Entity Representations}} with {{Entity-aware Self-attention}}},
  shorttitle = {{{LUKE}}},
  author = {Yamada, Ikuya and Asai, Akari and Shindo, Hiroyuki and Takeda, Hideaki and Matsumoto, Yuji},
  year = {2020},
  month = oct,
  number = {arXiv:2010.01057},
  eprint = {2010.01057},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2010.01057},
  urldate = {2022-06-29},
  abstract = {Entity representations are useful in natural language tasks involving entities. In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer. The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them. Our model is trained using a new pretraining task based on the masked language model of BERT. The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia. We also propose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens (words or entities) when computing attention scores. The proposed model achieves impressive empirical performance on a wide range of entity-related tasks. In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification), CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question answering). Our source code and pretrained representations are available at https://github.com/studio-ousia/luke.},
  archiveprefix = {arXiv}
}

@article{yamagataCurrentStatusEnhanced2019,
  title = {Current Status of the ``Enhanced Recovery after Surgery'' Program in Gastric Cancer Surgery},
  author = {Yamagata, Yukinori and Yoshikawa, Takaki and Yura, Masahiro and Otsuki, Sho and Morita, Shinji and Katai, Hitoshi and Nishida, Toshiro},
  year = {2019},
  journal = {Annals of Gastroenterological Surgery},
  volume = {3},
  number = {3},
  pages = {231--238},
  issn = {2475-0328},
  doi = {10.1002/ags3.12232},
  urldate = {2024-04-26},
  abstract = {Since the late 1990s, perioperative care through the enhanced recovery after surgery (ERAS, European Society for Clinical Nutrition and Metabolism [ESPEN]) program has spread. ERAS protocols aim to reduce surgical complications, improving postoperative outcomes and thereby saving resources by addressing various clinical elements through a multidisciplinary approach or based on evidence. In the field of gastric cancer, the philosophy of ERAS has gradually become accepted and, in 2014, consensus guidelines for enhanced recovery after gastrectomy were published. These guidelines consist of ``procedure-specific'' guidelines and ``general (not procedure-specific) enhanced recovery items.'' In this review, we focused on the procedure-specific guidelines and tried to update the contents of every element of the procedure-specific guidelines. The procedure-specific guidelines consist of the following eight elements: ``Preoperative nutrition,'' ``Preoperative oral pharmaconutrition,'' ``Access (of gastrectomy),'' ``Wound catheters and transversus abdominis plane block,'' ``Nasogastric/Nasojejunal decompression,'' ``Perianastomotic drains,'' ``Early postoperative diet and artificial nutrition,'' and ``Audit.'' On reviewing papers supporting these elements, it was reconfirmed that the recommendations of the guidelines are pertinent and valid. Four meta-analyses concerning the evaluation of ERAS protocols for gastric cancer were included in this review. Every study showed that the ERAS protocol reduced the cost and duration of hospital stay without increasing surgical complication rates, suggesting that ERAS is effective for gastric cancer surgery. However, it cannot be said that ERAS has achieved full penetration in Japan because most evidence is established in Western countries. Future studies must focus on developing a new ERAS protocols appropriate to Japanese conditions of gastric cancer.},
  copyright = {{\copyright} 2019 The Authors. Annals of Gastroenterological Surgery published by John Wiley \& Sons Australia, Ltd on behalf of The Japanese Society of Gastroenterological Surgery},
  langid = {english},
  keywords = {ERAS,gastric cancer,meta-analysis,perioperative care,review}
}

@misc{yangAgentOccamSimpleStrong2024,
  title = {{{AgentOccam}}: {{A Simple Yet Strong Baseline}} for {{LLM-Based Web Agents}}},
  shorttitle = {{{AgentOccam}}},
  author = {Yang, Ke and Liu, Yao and Chaudhary, Sapana and Fakoor, Rasool and Chaudhari, Pratik and Karypis, George and Rangwala, Huzefa},
  year = {2024},
  month = oct,
  number = {arXiv:2410.13825},
  eprint = {2410.13825},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.13825},
  urldate = {2025-02-17},
  abstract = {Autonomy via agents using large language models (LLMs) for personalized, standardized tasks boosts human efficiency. Automating web tasks (like booking hotels within a budget) is increasingly sought after. Fulfilling practical needs, the web agent also serves as an important proof-of-concept example for various agent grounding scenarios, with its success promising advancements in many future applications. Prior research often handcrafts web agent strategies (e.g., prompting templates, multi-agent systems, search methods, etc.) and the corresponding in-context examples, which may not generalize well across all real-world scenarios. On the other hand, there has been limited study on the misalignment between a web agent's observation/action representation and the pre-training data of the LLM it's based on. This discrepancy is especially notable when LLMs are primarily trained for language completion rather than tasks involving embodied navigation actions and symbolic web elements. Our study enhances an LLM-based web agent by simply refining its observation and action space to better align with the LLM's capabilities. This approach enables our base agent to significantly outperform previous methods on a wide variety of web tasks. Specifically, on WebArena, a benchmark featuring general-purpose web interaction tasks, our agent AgentOccam surpasses the previous state-of-the-art and concurrent work by 9.8 (+29.4\%) and 5.9 (+15.8\%) absolute points respectively, and boosts the success rate by 26.6 points (+161\%) over similar plain web agents with its observation and action space alignment. We achieve this without using in-context examples, new agent roles, online feedback or search strategies. AgentOccam's simple design highlights LLMs' impressive zero-shot performance on web tasks, and underlines the critical role of carefully tuning observation and action spaces for LLM-based agents.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@misc{yangBERTMeetsChinese2019,
  title = {{{BERT Meets Chinese Word Segmentation}}},
  author = {Yang, Haiqin},
  year = {2019},
  month = sep,
  number = {arXiv:1909.09292},
  eprint = {1909.09292},
  primaryclass = {cs, stat},
  institution = {arXiv},
  doi = {10.48550/arXiv.1909.09292},
  urldate = {2022-06-09},
  abstract = {Chinese word segmentation (CWS) is a fundamental task for Chinese language understanding. Recently, neural network-based models have attained superior performance in solving the in-domain CWS task. Last year, Bidirectional Encoder Representation from Transformers (BERT), a new language representation model, has been proposed as a backbone model for many natural language tasks and redefined the corresponding performance. The excellent performance of BERT motivates us to apply it to solve the CWS task. By conducting intensive experiments in the benchmark datasets from the second International Chinese Word Segmentation Bake-off, we obtain several keen observations. BERT can slightly improve the performance even when the datasets contain the issue of labeling inconsistency. When applying sufficiently learned features, Softmax, a simpler classifier, can attain the same performance as that of a more complicated classifier, e.g., Conditional Random Field (CRF). The performance of BERT usually increases as the model size increases. The features extracted by BERT can be also applied as good candidates for other neural network models.},
  archiveprefix = {arXiv}
}

@misc{yangEmpiricalStudyGPT32022,
  title = {An {{Empirical Study}} of {{GPT-3}} for {{Few-Shot Knowledge-Based VQA}}},
  author = {Yang, Zhengyuan and Gan, Zhe and Wang, Jianfeng and Hu, Xiaowei and Lu, Yumao and Liu, Zicheng and Wang, Lijuan},
  year = {2022},
  month = sep,
  number = {arXiv:2109.05014},
  eprint = {2109.05014},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2109.05014},
  urldate = {2023-06-10},
  abstract = {Knowledge-based visual question answering (VQA) involves answering questions that require external knowledge not present in the image. Existing methods first retrieve knowledge from external resources, then reason over the selected knowledge, the input image, and question for answer prediction. However, this two-step approach could lead to mismatches that potentially limit the VQA performance. For example, the retrieved knowledge might be noisy and irrelevant to the question, and the re-embedded knowledge features during reasoning might deviate from their original meanings in the knowledge base (KB). To address this challenge, we propose PICa, a simple yet effective method that Prompts GPT3 via the use of Image Captions, for knowledge-based VQA. Inspired by GPT-3's power in knowledge retrieval and question answering, instead of using structured KBs as in previous work, we treat GPT-3 as an implicit and unstructured KB that can jointly acquire and process relevant knowledge. Specifically, we first convert the image into captions (or tags) that GPT-3 can understand, then adapt GPT-3 to solve the VQA task in a few-shot manner by just providing a few in-context VQA examples. We further boost performance by carefully investigating: (i) what text formats best describe the image content, and (ii) how in-context examples can be better selected and used. PICa unlocks the first use of GPT-3 for multimodal tasks. By using only 16 examples, PICa surpasses the supervised state of the art by an absolute +8.6 points on the OK-VQA dataset. We also benchmark PICa on VQAv2, where PICa also shows a decent few-shot performance.},
  archiveprefix = {arXiv}
}

@inproceedings{yangLegalNLINaturalLanguage2022,
  title = {{{LegalNLI}}: Natural Language Inference for Legal Compliance Inspection},
  shorttitle = {{{LegalNLI}}},
  booktitle = {International {{Conference}} on {{Advanced Algorithms}} and {{Neural Networks}} ({{AANN}} 2022)},
  author = {Yang, Zhanye},
  year = {2022},
  month = jun,
  volume = {12285},
  pages = {144--150},
  publisher = {SPIE},
  doi = {10.1117/12.2637107},
  urldate = {2023-10-12},
  abstract = {Legal compliance inspection (LCI) plays an essential role in the judicial applications of laws and rules but has been neglected in the development of legal artificial intelligence (LegalAI). Currently, few methods in LegalAI can be utilized to solve this task. In this work, we propose to use natural language inference (NLI) framework to solve the LCI problems based on a fundamental fact that the legal judgment should be subject to judicial syllogisms. Specifically, we present LegalNLI -- a specially constructed dataset reformatted from the Chinese legal datasets for other problems. The proposed LegalNLI is a document-level NLI dataset in the legal domain whose premises and hypotheses vary from hundreds to thousands of words in length. In addition, there are few artifacts in LegalNLI that are some clues so as to possibly identify the label by looking only at the hypothesis without observing the premise. Therefore, it is more effective to solve the LCI task by adopting the NLI framework instead of direct text classification methods. Finally, we provide some experiments for evaluating some existing state-of-the-art systems of sentence-level NLI task on the LegalNLI dataset and find it is challenging.}
}

@inproceedings{yangModelingTwoWaySelection2022,
  title = {Modeling {{Two-Way Selection Preference}} for {{Person-Job Fit}}},
  booktitle = {Sixteenth {{ACM Conference}} on {{Recommender Systems}}},
  author = {Yang, Chen and Hou, Yupeng and Song, Yang and Zhang, Tao and Wen, Ji-Rong and Zhao, Wayne Xin},
  year = {2022},
  month = sep,
  eprint = {2208.08612},
  primaryclass = {cs},
  pages = {102--112},
  doi = {10.1145/3523227.3546752},
  urldate = {2023-01-28},
  abstract = {Person-job fit is the core technique of online recruitment platforms, which can improve the efficiency of recruitment by accurately matching the job positions with the job seekers. Existing works mainly focus on modeling the unidirectional process or overall matching. However, recruitment is a two-way selection process, which means that both candidate and employer involved in the interaction should meet the expectation of each other, instead of unilateral satisfaction. In this paper, we propose a dual-perspective graph representation learning approach to model directed interactions between candidates and jobs. To model the two-way selection preference from the dual-perspective of job seekers and employers, we incorporate two different nodes for each candidate (or job) and characterize both successful matching and failed matching via a unified dual-perspective interaction graph. To learn dual-perspective node representations effectively, we design an effective optimization algorithm, which involves a quadruple-based loss and a dual-perspective contrastive learning loss. Extensive experiments on three large real-world recruitment datasets have shown the effectiveness of our approach.},
  archiveprefix = {arXiv}
}

@techreport{yangPerformanceMultimodalGPT4V2023,
  type = {Preprint},
  title = {Performance of {{Multimodal GPT-4V}} on {{USMLE}} with {{Image}}: {{Potential}} for {{Imaging Diagnostic Support}} with {{Explanations}}},
  shorttitle = {Performance of {{Multimodal GPT-4V}} on {{USMLE}} with {{Image}}},
  author = {Yang, Zhichao and Yao, Zonghai and Tasmin, Mahbuba and Vashisht, Parth and Jang, Won Seok and Wang, Beining and Berlowitz, Dan and Yu, Hong},
  year = {2023},
  month = oct,
  institution = {{Radiology and Imaging}},
  doi = {10.1101/2023.10.26.23297629},
  urldate = {2023-10-30},
  abstract = {Objective This study aimed to evaluate the performance of GPT-4V on medical licensing examination questions with images, as well as to analyze interpretability. Design, Setting, and Participants We used 3 sets of multiple-choice questions with images to evaluate GPT-4V's performance. The first set was the United States Medical Licensing Examination (USMLE) from the National Board of Medical Examiners (NBME) sample questions in step1, step2CK, and step3. The second set was derived from AMBOSS, a commonly used question bank for medical students, which also provides statistics on question difficulty and the performance on an exam relative to the user base. The third set was the Diagnostic Radiology Qualifying Core Exam (DRQCE) from the American Board of Radiology. The study (including data analysis) was conducted from September to October 2023. Main Outcomes and Measures The choice accuracy of GPT-4V was compared to two other large language models, GPT-4 and ChatGPT. The GPT-4V explanation was evaluated across 4 qualitative metrics: image misunderstanding, text hallucination, reasoning error, and non-medical error. Results Of the 3 exams with images, NBME, AMBOSS, and DRQCE, GPT-4V achieved accuracies of 86.2\%, 62.0\%, and 73.1\%, respectively. GPT-4V outperformed ChatGPT and GPT-4 by 131.8\% and 64.5\% on average across various data sets. The model demonstrated a decreasing trend in performance as question difficulty increased in the AMBOSS dataset. GPT-4V achieves an accuracy of 90.7\% in the full USMLE exam, outperforming the passing threshold of about 60\% accuracy. Among the incorrect answers, 75.9\% of responses included misinterpretation of the image. However, 39.0\% of them could be easily solved with a short hint. Conclusion In this cross-sectional study, GPT-4V achieved a high accuracy of USMLE that was in the 70th 80th percentile with AMBOSS users preparing for the exam. The results suggest the potential of},
  langid = {english}
}

@misc{yangQuokkaOpensourceLarge2024,
  title = {Quokka: {{An Open-source Large Language Model ChatBot}} for {{Material Science}}},
  shorttitle = {Quokka},
  author = {Yang, Xianjun and Wilson, Stephen D. and Petzold, Linda},
  year = {2024},
  month = jan,
  number = {arXiv:2401.01089},
  eprint = {2401.01089},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2401.01089},
  urldate = {2024-01-20},
  abstract = {This paper presents the development of a specialized chatbot for materials science, leveraging the Llama-2 language model, and continuing pre-training on the expansive research articles in the materials science domain from the S2ORC dataset. The methodology involves an initial pretraining phase on over one million domain-specific papers, followed by an instruction-tuning process to refine the chatbot's capabilities. The chatbot is designed to assist researchers, educators, and students by providing instant, context-aware responses to queries in the field of materials science. We make the four trained checkpoints (7B, 13B, with or without chat ability) freely available to the research community at https://github.com/Xianjun-Yang/Quokka.},
  archiveprefix = {arXiv}
}

@misc{yangRECOVERDesigningLarge2025,
  title = {{{RECOVER}}: {{Designing}} a {{Large Language Model-based Remote Patient Monitoring System}} for {{Postoperative Gastrointestinal Cancer Care}}},
  shorttitle = {{{RECOVER}}},
  author = {Yang, Ziqi and Lu, Yuxuan and Bagdasarian, Jennifer and Swain, Vedant Das and Agarwal, Ritu and Campbell, Collin and {Al-Refaire}, Waddah and {El-Bayoumi}, Jehan and Gao, Guodong and Wang, Dakuo and Yao, Bingsheng and Shara, Nawar},
  year = {2025},
  month = feb,
  number = {arXiv:2502.05740},
  eprint = {2502.05740},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.05740},
  urldate = {2025-04-30},
  abstract = {Cancer surgery is a key treatment for gastrointestinal (GI) cancers, a group of cancers that account for more than 35\% of cancer-related deaths worldwide, but postoperative complications are unpredictable and can be life-threatening. In this paper, we investigate how recent advancements in large language models (LLMs) can benefit remote patient monitoring (RPM) systems through clinical integration by designing RECOVER, an LLM-powered RPM system for postoperative GI cancer care. To closely engage stakeholders in the design process, we first conducted seven participatory design sessions with five clinical staff and interviewed five cancer patients to derive six major design strategies for integrating clinical guidelines and information needs into LLM-based RPM systems. We then designed and implemented RECOVER, which features an LLM-powered conversational agent for cancer patients and an interactive dashboard for clinical staff to enable efficient postoperative RPM. Finally, we used RECOVER as a pilot system to assess the implementation of our design strategies with four clinical staff and five patients, providing design implications by identifying crucial design elements, offering insights on responsible AI, and outlining opportunities for future LLM-powered RPM systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction}
}

@misc{yangUnveilingGeneralizationPower2024,
  title = {Unveiling the {{Generalization Power}} of {{Fine-Tuned Large Language Models}}},
  author = {Yang, Haoran and Zhang, Yumeng and Xu, Jiaqi and Lu, Hongyuan and Heng, Pheng Ann and Lam, Wai},
  year = {2024},
  month = mar,
  journal = {arXiv.org},
  url = {https://arxiv.org/abs/2403.09162v1},
  urldate = {2024-09-05},
  abstract = {While Large Language Models (LLMs) have demonstrated exceptional multitasking abilities, fine-tuning these models on downstream, domain-specific datasets is often necessary to yield superior performance on test sets compared to their counterparts without fine-tuning. However, the comprehensive effects of fine-tuning on the LLMs' generalization ability are not fully understood. This paper delves into the differences between original, unmodified LLMs and their fine-tuned variants. Our primary investigation centers on whether fine-tuning affects the generalization ability intrinsic to LLMs. To elaborate on this, we conduct extensive experiments across five distinct language tasks on various datasets. Our main findings reveal that models fine-tuned on generation and classification tasks exhibit dissimilar behaviors in generalizing to different domains and tasks. Intriguingly, we observe that integrating the in-context learning strategy during fine-tuning on generation tasks can enhance the model's generalization ability. Through this systematic investigation, we aim to contribute valuable insights into the evolving landscape of fine-tuning practices for LLMs.},
  langid = {english}
}

@misc{yangUnveilingGeneralizationPower2024b,
  title = {Unveiling the {{Generalization Power}} of {{Fine-Tuned Large Language Models}}},
  author = {Yang, Haoran and Zhang, Yumeng and Xu, Jiaqi and Lu, Hongyuan and Heng, Pheng Ann and Lam, Wai},
  year = {2024},
  month = mar,
  number = {arXiv:2403.09162},
  eprint = {2403.09162},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2403.09162},
  urldate = {2024-09-05},
  abstract = {While Large Language Models (LLMs) have demonstrated exceptional multitasking abilities, fine-tuning these models on downstream, domain-specific datasets is often necessary to yield superior performance on test sets compared to their counterparts without fine-tuning. However, the comprehensive effects of finetuning on the LLMs' generalization ability are not fully understood. This paper delves into the differences between original, unmodified LLMs and their fine-tuned variants. Our primary investigation centers on whether finetuning affects the generalization ability intrinsic to LLMs. To elaborate on this, we conduct extensive experiments across five distinct language tasks on various datasets. Our main findings reveal that models fine-tuned on generation and classification tasks exhibit dissimilar behaviors in generalizing to different domains and tasks. Intriguingly, we observe that integrating the in-context learning strategy during fine-tuning on generation tasks can enhance the model's generalization ability. Through this systematic investigation, we aim to contribute valuable insights into the evolving landscape of fine-tuning practices for LLMs. The code and data are available at https://github.com/ LHRYANG/Generalization\_of\_FT-LLM.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language}
}

@inproceedings{yaoItAITurn2022,
  title = {It Is {{AI}}'s {{Turn}} to {{Ask Humans}} a {{Question}}: {{Question-Answer Pair Generation}} for {{Children}}'s {{Story Books}}},
  shorttitle = {It Is {{AI}}'s {{Turn}} to {{Ask Humans}} a {{Question}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Yao, Bingsheng and Wang, Dakuo and Wu, Tongshuang and Zhang, Zheng and Li, Toby and Yu, Mo and Xu, Ying},
  year = {2022},
  month = may,
  pages = {731--744},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-long.54},
  urldate = {2023-01-18},
  abstract = {Existing question answering (QA) techniques are created mainly to answer questions asked by humans. But in educational applications, teachers often need to decide what questions they should ask, in order to help students to improve their narrative understanding capabilities. We design an automated question-answer generation (QAG) system for this education scenario: given a story book at the kindergarten to eighth-grade level as input, our system can automatically generate QA pairs that are capable of testing a variety of dimensions of a student's comprehension skills. Our proposed QAG model architecture is demonstrated using a new expert-annotated FairytaleQA dataset, which has 278 child-friendly storybooks with 10,580 QA pairs. Automatic and human evaluations show that our model outperforms state-of-the-art QAG baseline systems. On top of our QAG system, we also start to build an interactive story-telling application for the future real-world deployment in this educational scenario.}
}

@misc{yaoLabelsEmpoweringHuman2023,
  title = {Beyond {{Labels}}: {{Empowering Human Annotators}} with {{Natural Language Explanations}} through a {{Novel Active-Learning Architecture}}},
  shorttitle = {Beyond {{Labels}}},
  author = {Yao, Bingsheng and Jindal, Ishan and Popa, Lucian and Katsis, Yannis and Ghosh, Sayan and He, Lihong and Lu, Yuxuan and Srivastava, Shashank and Li, Yunyao and Hendler, James and Wang, Dakuo},
  year = {2023},
  month = oct,
  number = {arXiv:2305.12710},
  eprint = {2305.12710},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.12710},
  urldate = {2023-11-13},
  abstract = {Real-world domain experts (e.g., doctors) rarely annotate only a decision label in their day-to-day workflow without providing explanations. Yet, existing low-resource learning techniques, such as Active Learning (AL), that aim to support human annotators mostly focus on the label while neglecting the natural language explanation of a data point. This work proposes a novel AL architecture to support experts' real-world need for label and explanation annotations in low-resource scenarios. Our AL architecture leverages an explanation-generation model to produce explanations guided by human explanations, a prediction model that utilizes generated explanations toward prediction faithfully, and a novel data diversity-based AL sampling strategy that benefits from the explanation annotations. Automated and human evaluations demonstrate the effectiveness of incorporating explanations into AL sampling and the improved human annotation efficiency and trustworthiness with our AL architecture. Additional ablation studies illustrate the potential of our AL architecture for transfer learning, generalizability, and integration with large language models (LLMs). While LLMs exhibit exceptional explanation-generation capabilities for relatively simple tasks, their effectiveness in complex real-world tasks warrants further in-depth study.},
  archiveprefix = {arXiv}
}

@inproceedings{yaoLabelsEmpoweringHuman2023a,
  title = {Beyond {{Labels}}: {{Empowering Human Annotators}} with {{Natural Language Explanations}} through a {{Novel Active-Learning Architecture}}},
  shorttitle = {Beyond {{Labels}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2023},
  author = {Yao, Bingsheng and Jindal, Ishan and Popa, Lucian and Katsis, Yannis and Ghosh, Sayan and He, Lihong and Lu, Yuxuan and Srivastava, Shashank and Li, Yunyao and Hendler, James and Wang, Dakuo},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  year = {2023},
  month = dec,
  pages = {11629--11643},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.findings-emnlp.778},
  urldate = {2024-03-29},
  abstract = {Real-world domain experts (e.g., doctors) rarely annotate only a decision label in their day-to-day workflow without providing explanations. Yet, existing low-resource learning techniques, such as Active Learning (AL), that aim to support human annotators mostly focus on the label while neglecting the natural language explanation of a data point. This work proposes a novel AL architecture to support experts' real-world need for label and explanation annotations in low-resource scenarios. Our AL architecture leverages an explanation-generation model to produce explanations guided by human explanations, a prediction model that utilizes generated explanations toward prediction faithfully, and a novel data diversity-based AL sampling strategy that benefits from the explanation annotations. Automated and human evaluations demonstrate the effectiveness of incorporating explanations into AL sampling and the improved human annotation efficiency and trustworthiness with our AL architecture. Additional ablation studies illustrate the potential of our AL architecture for transfer learning, generalizability, and integration with large language models (LLMs). While LLMs exhibit exceptional explanation-generation capabilities for relatively simple tasks, their effectiveness in complex real-world tasks warrants further in-depth study.}
}

@misc{yaoMoreSamplesMore2024,
  title = {More {{Samples}} or {{More Prompts}}? {{Exploring Effective In-Context Sampling}} for {{LLM Few-Shot Prompt Engineering}}},
  shorttitle = {More {{Samples}} or {{More Prompts}}?},
  author = {Yao, Bingsheng and Chen, Guiming and Zou, Ruishi and Lu, Yuxuan and Li, Jiachen and Zhang, Shao and Sang, Yisi and Liu, Sijia and Hendler, James and Wang, Dakuo},
  year = {2024},
  month = apr,
  number = {arXiv:2311.09782},
  eprint = {2311.09782},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.09782},
  urldate = {2025-02-14},
  abstract = {While most existing works on LLM prompting techniques focus only on how to select a better set of data samples inside one single prompt input (In-Context Learning or ICL), why can not we design and leverage multiple prompts together to further improve the LLM's performance? In this work, we propose In-Context Sampling (ICS), a low-resource LLM prompting technique to produce confident predictions by optimizing the construction of multiple ICL prompt inputs. Extensive experiments with three open-source LLMs (FlanT5-XL, Mistral-7B, and Mixtral-8x7B) on four NLI datasets (e-SNLI, Multi-NLI, ANLI, and Contract-NLI) and one QA dataset (CommonsenseQA) illustrate that ICS can consistently enhance LLMs' performance. An in-depth evaluation with three data similarity-based ICS strategies suggests that these strategies can further elevate LLM's performance, which sheds light on a new yet promising future research direction.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{yaoReActSynergizingReasoning2023,
  title = {{{ReAct}}: {{Synergizing Reasoning}} and {{Acting}} in {{Language Models}}},
  shorttitle = {{{ReAct}}},
  author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  year = {2023},
  month = mar,
  number = {arXiv:2210.03629},
  eprint = {2210.03629},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.03629},
  urldate = {2024-08-30},
  abstract = {While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34\% and 10\% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@article{yaoSurveyLargeLanguage2024,
  title = {A Survey on Large Language Model ({{LLM}}) Security and Privacy: {{The Good}}, {{The Bad}}, and {{The Ugly}}},
  shorttitle = {A Survey on Large Language Model ({{LLM}}) Security and Privacy},
  author = {Yao, Yifan and Duan, Jinhao and Xu, Kaidi and Cai, Yuanfang and Sun, Zhibo and Zhang, Yue},
  year = {2024},
  month = jun,
  journal = {High-Confidence Computing},
  volume = {4},
  number = {2},
  pages = {100211},
  issn = {2667-2952},
  doi = {10.1016/j.hcc.2024.100211},
  urldate = {2024-09-12},
  abstract = {Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionized natural language understanding and generation. They possess deep language comprehension, human-like text generation capabilities, contextual awareness, and robust problem-solving skills, making them invaluable in various domains (e.g., search engines, customer support, translation). In the meantime, LLMs have also gained traction in the security community, revealing security vulnerabilities and showcasing their potential in security-related tasks. This paper explores the intersection of LLMs with security and privacy. Specifically, we investigate how LLMs positively impact security and privacy, potential risks and threats associated with their use, and inherent vulnerabilities within LLMs. Through a comprehensive literature review, the paper categorizes the papers into ``The Good'' (beneficial LLM applications), ``The Bad'' (offensive applications), and ``The Ugly'' (vulnerabilities of LLMs and their defenses). We have some interesting findings. For example, LLMs have proven to enhance code security (code vulnerability detection) and data privacy (data confidentiality protection), outperforming traditional methods. However, they can also be harnessed for various attacks (particularly user-level attacks) due to their human-like reasoning abilities. We have identified areas that require further research efforts. For example, Research on model and parameter extraction attacks is limited and often theoretical, hindered by LLM parameter scale and confidentiality. Safe instruction tuning, a recent development, requires more exploration. We hope that our work can shed light on the LLMs' potential to both bolster and jeopardize cybersecurity.},
  keywords = {ChatGPT,Large Language Model (LLM),LLM attacks,LLM privacy,LLM security,LLM vulnerabilities}
}

@inproceedings{yaoWebShopScalableRealWorld2022,
  title = {{{WebShop}}: {{Towards Scalable Real-World Web Interaction}} with {{Grounded Language Agents}}},
  shorttitle = {{{WebShop}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Yao, Shunyu and Chen, Howard and Yang, John and Narasimhan, Karthik R.},
  year = {2022},
  month = oct,
  url = {https://openreview.net/forum?id=R9KnuFlvnU},
  urldate = {2024-07-05},
  abstract = {Most existing benchmarks for grounding language in interactive environments either lack realistic linguistic elements, or prove difficult to scale up due to substantial human involvement in the collection of data or feedback signals. We develop WebShop -- a simulated e-commerce website environment with 1.18 million real-world products and 12,087 crowd-sourced text instructions. In this environment, an agent needs to navigate multiple types of webpages and issue diverse actions to find, customize, and purchase a product given an instruction. WebShop provides several challenges including understanding compositional instructions, query (re-)formulation, dealing with noisy text in webpages, and performing strategic exploration. We collect over 1,600 human trajectories to first validate the benchmark, then train and evaluate a diverse range of agents using reinforcement learning, imitation learning, and pre-trained image and language models. Our best model achieves a task success rate of 29\%, which significantly outperforms rule heuristics but is far lower than expert human performance (59\%). We also analyze agent and human trajectories and ablate various model components to provide insights for developing future agents with stronger language understanding and decision making abilities. Finally, we show our agent trained on WebShop exhibits non-trivial sim-to-real transfer when evaluated on amazon.com and ebay.com, indicating the potential value of our benchmark for developing practical web agents that can operate in the wild.},
  langid = {english}
}

@misc{yasunagaLinkBERTPretrainingLanguage2022,
  title = {{{LinkBERT}}: {{Pretraining Language Models}} with {{Document Links}}},
  shorttitle = {{{LinkBERT}}},
  author = {Yasunaga, Michihiro and Leskovec, Jure and Liang, Percy},
  year = {2022},
  month = mar,
  number = {arXiv:2203.15827},
  eprint = {2203.15827},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2203.15827},
  urldate = {2023-10-16},
  abstract = {Language model (LM) pretraining can learn various knowledge from text corpora, helping downstream tasks. However, existing methods such as BERT model a single document, and do not capture dependencies or knowledge that span across documents. In this work, we propose LinkBERT, an LM pretraining method that leverages links between documents, e.g., hyperlinks. Given a text corpus, we view it as a graph of documents and create LM inputs by placing linked documents in the same context. We then pretrain the LM with two joint self-supervised objectives: masked language modeling and our new proposal, document relation prediction. We show that LinkBERT outperforms BERT on various downstream tasks across two domains: the general domain (pretrained on Wikipedia with hyperlinks) and biomedical domain (pretrained on PubMed with citation links). LinkBERT is especially effective for multi-hop reasoning and few-shot QA (+5\% absolute improvement on HotpotQA and TriviaQA), and our biomedical LinkBERT sets new states of the art on various BioNLP tasks (+7\% on BioASQ and USMLE). We release our pretrained models, LinkBERT and BioLinkBERT, as well as code and data at https://github.com/michiyasunaga/LinkBERT.},
  archiveprefix = {arXiv}
}

@misc{yeMultiWOZ24MultiDomain2022,
  title = {{{MultiWOZ}} 2.4: {{A Multi-Domain Task-Oriented Dialogue Dataset}} with {{Essential Annotation Corrections}} to {{Improve State Tracking Evaluation}}},
  shorttitle = {{{MultiWOZ}} 2.4},
  author = {Ye, Fanghua and Manotumruksa, Jarana and Yilmaz, Emine},
  year = {2022},
  month = jul,
  number = {arXiv:2104.00773},
  eprint = {2104.00773},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2104.00773},
  urldate = {2024-09-30},
  abstract = {The MultiWOZ 2.0 dataset has greatly stimulated the research of task-oriented dialogue systems. However, its state annotations contain substantial noise, which hinders a proper evaluation of model performance. To address this issue, massive efforts were devoted to correcting the annotations. Three improved versions (i.e., MultiWOZ 2.1-2.3) have then been released. Nonetheless, there are still plenty of incorrect and inconsistent annotations. This work introduces MultiWOZ 2.4, which refines the annotations in the validation set and test set of MultiWOZ 2.1. The annotations in the training set remain unchanged (same as MultiWOZ 2.1) to elicit robust and noise-resilient model training. We benchmark eight state-of-the-art dialogue state tracking models on MultiWOZ 2.4. All of them demonstrate much higher performance than on MultiWOZ 2.11.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language}
}

@misc{yeSimpleEffectivePluggable2022,
  title = {A {{Simple}} but {{Effective Pluggable Entity Lookup Table}} for {{Pre-trained Language Models}}},
  author = {Ye, Deming and Lin, Yankai and Li, Peng and Sun, Maosong and Liu, Zhiyuan},
  year = {2022},
  month = may,
  number = {arXiv:2202.13392},
  eprint = {2202.13392},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2202.13392},
  urldate = {2022-06-21},
  abstract = {Pre-trained language models (PLMs) cannot well recall rich factual knowledge of entities exhibited in large-scale corpora, especially those rare entities. In this paper, we propose to build a simple but effective Pluggable Entity Lookup Table (PELT) on demand by aggregating the entity's output representations of multiple occurrences in the corpora. PELT can be compatibly plugged as inputs to infuse supplemental entity knowledge into PLMs. Compared to previous knowledge-enhanced PLMs, PELT only requires 0.2\%-5\% pre-computation with capability of acquiring knowledge from out-of-domain corpora for domain adaptation scenario. The experiments on knowledge-related tasks demonstrate that our method, PELT, can flexibly and effectively transfer entity knowledge from related corpora into PLMs with different architectures.},
  archiveprefix = {arXiv}
}

@article{yildirim-erbasliConversationBasedAssessmentsEducation2023,
  title = {Conversation-{{Based Assessments}} in {{Education}}: {{Design}}, {{Implementation}}, and {{Cognitive Walkthroughs}} for {{Usability Testing}}},
  shorttitle = {Conversation-{{Based Assessments}} in {{Education}}},
  author = {{Yildirim-Erbasli}, Seyma N. and Bulut, Okan and Demmans Epp, Carrie and Cui, Ying},
  year = {2023},
  month = sep,
  journal = {Journal of Educational Technology Systems},
  volume = {52},
  number = {1},
  pages = {27--51},
  publisher = {SAGE Publications Inc},
  issn = {0047-2395},
  doi = {10.1177/00472395231178943},
  urldate = {2025-04-09},
  abstract = {Conversational agents have been widely used in education to support student learning. There have been recent attempts to design and use conversational agents to conduct assessments (i.e., conversation-based assessments: CBA). In this study, we developed CBA with constructed and selected-response tests using Rasa---an artificial intelligence-based tool. CBA was deployed via Google Chat to support formative assessment. We evaluated (1) its performance in answering students' responses and (2) its usability with cognitive walkthroughs conducted by external evaluators. CBA with constructed-response tests consistently matched student responses to the appropriate conversation paths in most cases. In comparison, CBA with selected-response tests demonstrated perfect accuracy between system design and implementation. A cognitive walkthrough of CBA showed its usability as well as several potential issues that could be improved. Participating students did not experience these issues, however, we reported them to help researchers, designers, and practitioners improve the assessment experience for students using CBA.},
  langid = {english}
}

@misc{yildirimMultimodalHealthcareAI2024,
  title = {Multimodal {{Healthcare AI}}: {{Identifying}} and {{Designing Clinically Relevant Vision-Language Applications}} for {{Radiology}}},
  shorttitle = {Multimodal {{Healthcare AI}}},
  author = {Yildirim, Nur and Richardson, Hannah and Wetscherek, Maria T. and Bajwa, Junaid and Jacob, Joseph and Pinnock, Mark A. and Harris, Stephen and {de Castro}, Daniel Coelho and Bannur, Shruthi and Hyland, Stephanie L. and Ghosh, Pratik and Ranjit, Mercy and Bouzid, Kenza and Schwaighofer, Anton and {P{\'e}rez-Garc{\'i}a}, Fernando and Sharma, Harshita and Oktay, Ozan and Lungren, Matthew and {Alvarez-Valle}, Javier and Nori, Aditya and Thieme, Anja},
  year = {2024},
  month = feb,
  eprint = {2402.14252},
  primaryclass = {cs},
  doi = {10.1145/3613904.3642013},
  urldate = {2024-02-28},
  abstract = {Recent advances in AI combine large language models (LLMs) with vision encoders that bring forward unprecedented technical capabilities to leverage for a wide range of healthcare applications. Focusing on the domain of radiology, vision-language models (VLMs) achieve good performance results for tasks such as generating radiology findings based on a patient's medical image, or answering visual questions (e.g., 'Where are the nodules in this chest X-ray?'). However, the clinical utility of potential applications of these capabilities is currently underexplored. We engaged in an iterative, multidisciplinary design process to envision clinically relevant VLM interactions, and co-designed four VLM use concepts: Draft Report Generation, Augmented Report Review, Visual Search and Querying, and Patient Imaging History Highlights. We studied these concepts with 13 radiologists and clinicians who assessed the VLM concepts as valuable, yet articulated many design considerations. Reflecting on our findings, we discuss implications for integrating VLM capabilities in radiology, and for healthcare AI more generally.},
  archiveprefix = {arXiv}
}

@inproceedings{yinSepsisLabEarlySepsis2020,
  title = {{{SepsisLab}}: {{Early Sepsis Prediction}} with {{Uncertainty Quantification}} and {{Active Sensing}}},
  shorttitle = {{{SepsisLab}}},
  booktitle = {Proceedings of the 26th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Yin, Changchang and Chen, Pin-Yu and Yao, Bingsheng and Wang, Dakuo and Caterino, Jeffrey and Zhang, Ping},
  year = {2020},
  month = aug,
  eprint = {2407.16999},
  primaryclass = {cs},
  pages = {862--872},
  doi = {10.1145/3394486.3403129},
  urldate = {2024-09-12},
  abstract = {Sepsis is the leading cause of in-hospital mortality in the USA. Early sepsis onset prediction and diagnosis could significantly improve the survival of sepsis patients. Existing predictive models are usually trained on high-quality data with few missing information, while missing values widely exist in real-world clinical scenarios (especially in the first hours of admissions to the hospital), which causes a significant decrease in accuracy and an increase in uncertainty for the predictive models. The common method to handle missing values is imputation, which replaces the unavailable variables with estimates from the observed data. The uncertainty of imputation results can be propagated to the sepsis prediction outputs, which have not been studied in existing works on either sepsis prediction or uncertainty quantification. In this study, we first define such propagated uncertainty as the variance of prediction output and then introduce uncertainty propagation methods to quantify the propagated uncertainty. Moreover, for the potential high-risk patients with low confidence due to limited observations, we propose a robust active sensing algorithm to increase confidence by actively recommending clinicians to observe the most informative variables. We validate the proposed models in both publicly available data (i.e., MIMIC-III and AmsterdamUMCdb) and proprietary data in The Ohio State University Wexner Medical Center (OSUWMC). The experimental results show that the propagated uncertainty is dominant at the beginning of admissions to hospitals and the proposed algorithm outperforms state-of-the-art active sensing methods. Finally, we implement a SepsisLab system for early sepsis prediction and active sensing based on our pre-trained models. Clinicians and potential sepsis patients can benefit from the system in early prediction and diagnosis of sepsis.},
  archiveprefix = {arXiv},
  keywords = {68T07 (primary) 92C50 (secondary),Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,H.2.8,I.2.1,J.3}
}

@article{yoonSequenceTaggingBiomedical2021,
  title = {Sequence {{Tagging}} for {{Biomedical Extractive Question Answering}}},
  author = {Yoon, Wonjin and Jackson, Richard and Kang, Jaewoo and Lagerberg, Aron},
  year = {2021},
  journal = {arXiv preprint arXiv:2104.07535},
  eprint = {2104.07535},
  archiveprefix = {arXiv}
}

@inproceedings{yoranAssistantBenchCanWeb2024,
  title = {{{AssistantBench}}: {{Can Web Agents Solve Realistic}} and {{Time-Consuming Tasks}}?},
  shorttitle = {{{AssistantBench}}},
  booktitle = {Proceedings of the 2024 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Yoran, Ori and Amouyal, Samuel Joseph and Malaviya, Chaitanya and Bogin, Ben and Press, Ofir and Berant, Jonathan},
  editor = {{Al-Onaizan}, Yaser and Bansal, Mohit and Chen, Yun-Nung},
  year = {2024},
  month = nov,
  pages = {8938--8968},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.505},
  urldate = {2024-12-11},
  abstract = {Language agents, built on top of language models (LMs), are systems that can interact with complex environments, such as the open web. In this work, we examine whether such agents can perform realistic and time-consuming tasks on the web, e.g., monitoring real-estate markets or locating relevant nearby businesses. We introduce AssistantBench, a challenging new benchmark consisting of 214 realistic tasks that can be automatically evaluated, covering different scenarios and domains. We find that AssistantBench exposes the limitations of current systems, including language models and retrieval-augmented language models, as no model reaches an accuracy of more than 25 points. While closed-book LMs perform well in terms of accuracy, they exhibit low precision and tend to hallucinate facts. State-of-the-art web agents reach a score of near zero. Additionally, we introduce SeePlanAct (SPA), a new web agent that significantly outperforms previous agents, and an ensemble of SPA and closed-book models reaches the best overall performance. Moreover, we analyze failures of current systems and highlight that open web navigation remains a major challenge.}
}

@misc{YourProjectsOverleaf,
  title = {Your {{Projects}} - {{Overleaf}}, {{Online LaTeX Editor}}},
  url = {https://www.overleaf.com/project},
  urldate = {2025-03-20}
}

@misc{yuanBatchEvalHumanlikeText2023,
  title = {{{BatchEval}}: {{Towards Human-like Text Evaluation}}},
  shorttitle = {{{BatchEval}}},
  author = {Yuan, Peiwen and Feng, Shaoxiong and Li, Yiwei and Wang, Xinglin and Pan, Boyuan and Wang, Heda and Li, Kan},
  year = {2023},
  month = dec,
  number = {arXiv:2401.00437},
  eprint = {2401.00437},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2401.00437},
  urldate = {2024-01-02},
  abstract = {Significant progress has been made in automatic text evaluation with the introduction of large language models (LLMs) as evaluators. However, current sample-wise evaluation paradigm suffers from the following issues: (1) Sensitive to prompt design; (2) Poor resistance to noise; (3) Inferior ensemble performance with static reference. Inspired by the fact that humans treat both criterion definition and inter sample comparison as references for evaluation, we propose BatchEval, a paradigm that conducts batch-wise evaluation iteratively to alleviate the above problems. We explore variants under this paradigm and confirm the optimal settings are two stage procedure with heterogeneous batch composition strategy and decimal scoring format. Comprehensive experiments across 3 LLMs on 4 text evaluation tasks demonstrate that BatchEval outperforms state-of-the-art methods by 10.5\% on Pearson correlations with only 64\% API cost on average. Further analyses have been conducted to verify the robustness, generalization, and working mechanism of BatchEval.},
  archiveprefix = {arXiv}
}

@inproceedings{yuanColdstartActiveLearning2020,
  title = {Cold-Start {{Active Learning}} through {{Self-supervised Language Modeling}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Yuan, Michelle and Lin, Hsuan-Tien and {Boyd-Graber}, Jordan},
  year = {2020},
  pages = {7935--7948},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.emnlp-main.637},
  urldate = {2024-04-25},
  abstract = {Active learning strives to reduce annotation costs by choosing the most critical examples to label. Typically, the active learning strategy is contingent on the classification model. For instance, uncertainty sampling depends on poorly calibrated model confidence scores. In the cold-start setting, active learning is impractical because of model instability and data scarcity. Fortunately, modern NLP provides an additional source of information: pre-trained language models. The pre-training loss can find examples that surprise the model and should be labeled for efficient fine-tuning. Therefore, we treat the language modeling loss as a proxy for classification uncertainty. With BERT, we develop a simple strategy based on the masked language modeling loss that minimizes labeling costs for text classification. Compared to other baselines, our approach reaches higher accuracy within less sampling iterations and computation time.},
  langid = {american},
  keywords = {Cold Start Active Learning}
}

@article{yuanColdstartActiveLearning2020a,
  title = {Cold-Start {{Active Learning}} through {{Self-supervised Language Modeling}}},
  author = {Yuan, Michelle and Lin, Hsuan-Tien and {Boyd-Graber}, Jordan},
  year = {2020},
  journal = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages = {7935--7948},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.emnlp-main.637},
  urldate = {2024-04-25},
  abstract = {Active learning strives to reduce annotation costs by choosing the most critical examples to label. Typically, the active learning strategy is contingent on the classification model. For instance, uncertainty sampling depends on poorly calibrated model confidence scores. In the cold-start setting, active learning is impractical because of model instability and data scarcity. Fortunately, modern NLP provides an additional source of information: pre-trained language models. The pre-training loss can find examples that surprise the model and should be labeled for efficient fine-tuning. Therefore, we treat the language modeling loss as a proxy for classification uncertainty. With BERT, we develop a simple strategy based on the masked language modeling loss that minimizes labeling costs for text classification. Compared to other baselines, our approach reaches higher accuracy within less sampling iterations and computation time.},
  langid = {american}
}

@misc{yuanColdstartActiveLearning2020b,
  title = {Cold-Start {{Active Learning}} through {{Self-supervised Language Modeling}}},
  author = {Yuan, Michelle and Lin, Hsuan-Tien and {Boyd-Graber}, Jordan},
  year = {2020},
  month = oct,
  number = {arXiv:2010.09535},
  eprint = {2010.09535},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2010.09535},
  urldate = {2024-06-11},
  abstract = {Active learning strives to reduce annotation costs by choosing the most critical examples to label. Typically, the active learning strategy is contingent on the classification model. For instance, uncertainty sampling depends on poorly calibrated model confidence scores. In the cold-start setting, active learning is impractical because of model instability and data scarcity. Fortunately, modern NLP provides an additional source of information: pre-trained language models. The pre-training loss can find examples that surprise the model and should be labeled for efficient fine-tuning. Therefore, we treat the language modeling loss as a proxy for classification uncertainty. With BERT, we develop a simple strategy based on the masked language modeling loss that minimizes labeling costs for text classification. Compared to other baselines, our approach reaches higher accuracy within less sampling iterations and computation time.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{yuanEnhancingVisualGrounding2025,
  title = {Enhancing {{Visual Grounding}} for {{GUI Agents}} via {{Self-Evolutionary Reinforcement Learning}}},
  author = {Yuan, Xinbin and Zhang, Jian and Li, Kaixin and Cai, Zhuoxuan and Yao, Lujian and Chen, Jie and Wang, Enguang and Hou, Qibin and Chen, Jinwei and Jiang, Peng-Tao and Li, Bo},
  year = {2025},
  month = may,
  number = {arXiv:2505.12370},
  eprint = {2505.12370},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.12370},
  urldate = {2025-06-04},
  abstract = {Graphical User Interface (GUI) agents have made substantial strides in understanding and executing user instructions across diverse platforms. Yet, grounding these instructions to precise interface elements remains challenging, especially in complex, high-resolution, professional environments. Traditional supervised finetuning (SFT) methods often require large volumes of diverse data and exhibit weak generalization. To overcome these limitations, we introduce a reinforcement learning (RL) based framework that incorporates three core strategies: (1) seed data curation to ensure high quality training samples, (2) a dense policy gradient that provides continuous feedback based on prediction accuracy, and (3) a self evolutionary reinforcement finetuning mechanism that iteratively refines the model using attention maps. With only 3k training samples, our 7B-parameter model achieves state-of-the-art results among similarly sized models on three grounding benchmarks. Notably, it attains 47.3{\textbackslash}\% accuracy on the ScreenSpot-Pro dataset, outperforming much larger models, such as UI-TARS-72B, by a margin of 24.2{\textbackslash}\%. These findings underscore the effectiveness of RL-based approaches in enhancing GUI agent performance, particularly in high-resolution, complex environments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence}
}

@misc{yueLlamaRecTwoStageRecommendation2023,
  title = {{{LlamaRec}}: {{Two-Stage Recommendation}} Using {{Large Language Models}} for {{Ranking}}},
  shorttitle = {{{LlamaRec}}},
  author = {Yue, Zhenrui and Rabhi, Sara and Moreira, Gabriel de Souza Pereira and Wang, Dong and Oldridge, Even},
  year = {2023},
  month = oct,
  number = {arXiv:2311.02089},
  eprint = {2311.02089},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.02089},
  urldate = {2024-09-24},
  abstract = {Recently, large language models (LLMs) have exhibited significant progress in language understanding and generation. By leveraging textual features, customized LLMs are also applied for recommendation and demonstrate improvements across diverse recommendation scenarios. Yet the majority of existing methods perform training-free recommendation that heavily relies on pretrained knowledge (e.g., movie recommendation). In addition, inference on LLMs is slow due to autoregressive generation, rendering existing methods less effective for real-time recommendation. As such, we propose a two-stage framework using large language models for ranking-based recommendation (LlamaRec). In particular, we use small-scale sequential recommenders to retrieve candidates based on the user interaction history. Then, both history and retrieved items are fed to the LLM in text via a carefully designed prompt template. Instead of generating next-item titles, we adopt a verbalizer-based approach that transforms output logits into probability distributions over the candidate items. Therefore, the proposed LlamaRec can efficiently rank items without generating long text. To validate the effectiveness of the proposed framework, we compare against state-of-the-art baseline methods on benchmark datasets. Our experimental results demonstrate the performance of LlamaRec, which consistently achieves superior performance in both recommendation performance and efficiency.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval}
}

@misc{zamaniJointModelingOptimization2018,
  title = {Joint {{Modeling}} and {{Optimization}} of {{Search}} and {{Recommendation}}},
  author = {Zamani, Hamed and Croft, W. Bruce},
  year = {2018},
  month = jul,
  number = {arXiv:1807.05631},
  eprint = {1807.05631},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1807.05631},
  urldate = {2022-08-31},
  abstract = {Despite the somewhat different techniques used in developing search engines and recommender systems, they both follow the same goal: helping people to get the information they need at the right time. Due to this common goal, search and recommendation models can potentially benefit from each other. The recent advances in neural network technologies make them effective and easily extendable for various tasks, including retrieval and recommendation. This raises the possibility of jointly modeling and optimizing search ranking and recommendation algorithms, with potential benefits to both. In this paper, we present theoretical and practical reasons to motivate joint modeling of search and recommendation as a research direction. We propose a general framework that simultaneously learns a retrieval model and a recommendation model by optimizing a joint loss function. Our preliminary results on a dataset of product data indicate that the proposed joint modeling substantially outperforms the retrieval and recommendation models trained independently. We list a number of future directions for this line of research that can potentially lead to development of state-of-the-art search and recommendation models.},
  archiveprefix = {arXiv}
}

@inproceedings{zamaniLearningJointSearch2020,
  title = {Learning a {{Joint Search}} and {{Recommendation Model}} from {{User-Item Interactions}}},
  booktitle = {Proceedings of the 13th {{International Conference}} on {{Web Search}} and {{Data Mining}}},
  author = {Zamani, Hamed and Croft, W. Bruce},
  year = {2020},
  month = jan,
  series = {{{WSDM}} '20},
  pages = {717--725},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3336191.3371818},
  urldate = {2022-08-29},
  abstract = {Existing learning to rank models for information retrieval are trained based on explicit or implicit query-document relevance information. In this paper, we study the task of learning a retrieval model based on user-item interactions. Our model has potential applications to the systems with rich user-item interaction data, such as browsing and recommendation, in which having an accurate search engine is desired. This includes media streaming services and e-commerce websites among others. Inspired by the neural approaches to collaborative filtering and the language modeling approaches to information retrieval, our model is jointly optimized to predict user-item interactions and reconstruct the item textual descriptions. In more details, our model learns user and item representations such that they can accurately predict future user-item interactions, while generating an effective unigram language model for each item. Our experiments on four diverse datasets in the context of movie and product search and recommendation demonstrate that our model substantially outperforms competitive retrieval baselines, in addition to providing comparable performance to state-of-the-art hybrid recommendation models.},
  isbn = {978-1-4503-6822-3}
}

@misc{zengEvaluatingLargeLanguage2023,
  title = {Evaluating {{Large Language Models}} at {{Evaluating Instruction Following}}},
  author = {Zeng, Zhiyuan and Yu, Jiatong and Gao, Tianyu and Meng, Yu and Goyal, Tanya and Chen, Danqi},
  year = {2023},
  month = oct,
  number = {arXiv:2310.07641},
  eprint = {2310.07641},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.07641},
  urldate = {2023-10-12},
  abstract = {As research in large language models (LLMs) continues to accelerate, LLM-based evaluation has emerged as a scalable and cost-effective alternative to human evaluations for comparing the ever increasing list of models. This paper investigates the efficacy of these "LLM evaluators", particularly in using them to assess instruction following, a metric that gauges how closely generated text adheres to the given instruction. We introduce a challenging meta-evaluation benchmark, LLMBar, designed to test the ability of an LLM evaluator in discerning instruction-following outputs. The authors manually curated 419 pairs of outputs, one adhering to instructions while the other diverging, yet may possess deceptive qualities that mislead an LLM evaluator, e.g., a more engaging tone. Contrary to existing meta-evaluation, we discover that different evaluators (i.e., combinations of LLMs and prompts) exhibit distinct performance on LLMBar and even the highest-scoring ones have substantial room for improvement. We also present a novel suite of prompting strategies that further close the gap between LLM and human evaluators. With LLMBar, we hope to offer more insight into LLM evaluators and foster future research in developing better instruction-following models.},
  archiveprefix = {arXiv}
}

@misc{zhaiActionsSpeakLouder2024,
  title = {Actions {{Speak Louder}} than {{Words}}: {{Trillion-Parameter Sequential Transducers}} for {{Generative Recommendations}}},
  shorttitle = {Actions {{Speak Louder}} than {{Words}}},
  author = {Zhai, Jiaqi and Liao, Lucy and Liu, Xing and Wang, Yueming and Li, Rui and Cao, Xuan and Gao, Leon and Gong, Zhaojie and Gu, Fangda and He, Michael and Lu, Yinghai and Shi, Yu},
  year = {2024},
  month = may,
  number = {arXiv:2402.17152},
  eprint = {2402.17152},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.17152},
  urldate = {2024-09-05},
  abstract = {Large-scale recommendation systems are characterized by their reliance on high cardinality, heterogeneous features and the need to handle tens of billions of user actions on a daily basis. Despite being trained on huge volume of data with thousands of features, most Deep Learning Recommendation Models (DLRMs) in industry fail to scale with compute. Inspired by success achieved by Transformers in language and vision domains, we revisit fundamental design choices in recommendation systems. We reformulate recommendation problems as sequential transduction tasks within a generative modeling framework ("Generative Recommenders"), and propose a new architecture, HSTU, designed for high cardinality, non-stationary streaming recommendation data. HSTU outperforms baselines over synthetic and public datasets by up to 65.8\% in NDCG, and is 5.3x to 15.2x faster than FlashAttention2-based Transformers on 8192 length sequences. HSTU-based Generative Recommenders, with 1.5 trillion parameters, improve metrics in online A/B tests by 12.4\% and have been deployed on multiple surfaces of a large internet platform with billions of users. More importantly, the model quality of Generative Recommenders empirically scales as a power-law of training compute across three orders of magnitude, up to GPT-3/LLaMa-2 scale, which reduces carbon footprint needed for future model developments, and further paves the way for the first foundational models in recommendations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning}
}

@article{zhaiFineTuningLargeVisionLanguage,
  title = {Fine-{{Tuning Large Vision-Language Models}} as {{Decision-Making Agents}} via {{Reinforcement Learning}}},
  author = {Zhai, Yuexiang and Bai, Hao and Lin, Zipeng and Pan, Jiayi and Tong, Shengbang and Zhou, Yifei and Suhr, Alane and Xie, Saining and LeCun, Yann and Ma, Yi and Levine, Sergey},
  langid = {english}
}

@misc{zhaiInvestigatingCatastrophicForgetting2023,
  title = {Investigating the {{Catastrophic Forgetting}} in {{Multimodal Large Language Models}}},
  author = {Zhai, Yuexiang and Tong, Shengbang and Li, Xiao and Cai, Mu and Qu, Qing and Lee, Yong Jae and Ma, Yi},
  year = {2023},
  number = {arXiv:2309.10313},
  eprint = {2309.10313},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.10313},
  urldate = {2024-04-26},
  abstract = {Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-stage fine-tuning on an image dataset improves performance across other image datasets, by enhancing the alignment of text and visual features. However, as fine-tuning proceeds, the MLLMs begin to hallucinate, resulting in a significant loss of generalizability, even when the image encoder remains frozen. Our results suggest that MLLMs have yet to demonstrate performance on par with their vision models on standard image classification tasks and the current MLLM fine-tuning procedure still has room for improvement.},
  archiveprefix = {arXiv},
  langid = {american},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@inproceedings{zhangActiveExampleSelection2022,
  title = {Active {{Example Selection}} for {{In-Context Learning}}},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Zhang, Yiming and Feng, Shi and Tan, Chenhao},
  editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
  year = {2022},
  month = dec,
  pages = {9134--9148},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.emnlp-main.622},
  urldate = {2023-11-14},
  abstract = {With a handful of demonstration examples, large-scale language models demonstrate strong capability to perform various tasks by in-context learning from these examples, without any fine-tuning. We demonstrate that in-context learning performance can be highly unstable across samples of examples, indicating the idiosyncrasies of how language models acquire information. We formulate example selection for in-context learning as a sequential decision problem, and propose a reinforcement learning algorithm for identifying generalizable policies to select demonstration examples. For GPT-2, our learned policies demonstrate strong abilities of generalizing to unseen tasks in training, with a 5.8\% improvement on average. Examples selected from our learned policies can even achieve a small improvement on GPT-3 Ada. However, the improvement diminishes on larger GPT-3 models, suggesting emerging capabilities of large language models.}
}

@misc{zhangAlpaCareInstructiontunedLarge2023,
  title = {{{AlpaCare}}:{{Instruction-tuned Large Language Models}} for {{Medical Application}}},
  shorttitle = {{{AlpaCare}}},
  author = {Zhang, Xinlu and Tian, Chenxin and Yang, Xianjun and Chen, Lichang and Li, Zekun and Petzold, Linda Ruth},
  year = {2023},
  month = oct,
  number = {arXiv:2310.14558},
  eprint = {2310.14558},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2310.14558},
  urldate = {2024-01-20},
  abstract = {Large Language Models (LLMs) have demonstrated significant enhancements in instruction-following abilities through instruction tuning, achieving notable performances across various tasks. Previous research has focused on fine-tuning medical domain-specific LLMs using an extensive array of medical-specific data, incorporating millions of pieces of biomedical literature to augment their medical capabilities. However, existing medical instruction-tuned LLMs have been constrained by the limited scope of tasks and instructions available, restricting the efficacy of instruction tuning and adversely affecting performance in the general domain. In this paper, we fine-tune LLaMA-series models using 52k diverse, machine-generated, medical instruction-following data, MedInstruct-52k, resulting in the model AlpaCare. Comprehensive experimental results on both general and medical-specific domain free-form instruction evaluations showcase AlpaCare's strong medical proficiency and generalizability compared to previous instruction-tuned models in both medical and general domains. We provide public access to our MedInstruct-52k dataset and a clinician-crafted free-form instruction test set, MedInstruct-test, along with our codebase, to foster further research and development. Our project page is available at https://github.com/XZhang97666/AlpaCare.},
  archiveprefix = {arXiv}
}

@misc{zhangAPIAgentsVs2025,
  title = {{{API Agents}} vs. {{GUI Agents}}: {{Divergence}} and {{Convergence}}},
  shorttitle = {{{API Agents}} vs. {{GUI Agents}}},
  author = {Zhang, Chaoyun and He, Shilin and Li, Liqun and Qin, Si and Kang, Yu and Lin, Qingwei and Zhang, Dongmei},
  year = {2025},
  month = mar,
  number = {arXiv:2503.11069},
  eprint = {2503.11069},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.11069},
  urldate = {2025-03-20},
  abstract = {Large language models (LLMs) have evolved beyond simple text generation to power software agents that directly translate natural language commands into tangible actions. While API-based LLM agents initially rose to prominence for their robust automation capabilities and seamless integration with programmatic endpoints, recent progress in multimodal LLM research has enabled GUI-based LLM agents that interact with graphical user interfaces in a human-like manner. Although these two paradigms share the goal of enabling LLM-driven task automation, they diverge significantly in architectural complexity, development workflows, and user interaction models. This paper presents the first comprehensive comparative study of API-based and GUI-based LLM agents, systematically analyzing their divergence and potential convergence. We examine key dimensions and highlight scenarios in which hybrid approaches can harness their complementary strengths. By proposing clear decision criteria and illustrating practical use cases, we aim to guide practitioners and researchers in selecting, combining, or transitioning between these paradigms. Ultimately, we indicate that continuing innovations in LLM-based automation are poised to blur the lines between API- and GUI-driven agents, paving the way for more flexible, adaptive solutions in a wide range of real-world applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction}
}

@inproceedings{zhangAskbeforePlanProactiveLanguage2024,
  title = {Ask-before-{{Plan}}: {{Proactive Language Agents}} for {{Real-World Planning}}},
  shorttitle = {Ask-before-{{Plan}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2024},
  author = {Zhang, Xuan and Deng, Yang and Ren, Zifeng and Ng, See-Kiong and Chua, Tat-Seng},
  editor = {{Al-Onaizan}, Yaser and Bansal, Mohit and Chen, Yun-Nung},
  year = {2024},
  month = nov,
  pages = {10836--10863},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.findings-emnlp.636},
  urldate = {2024-12-11}
}

@article{zhangBioWordVecImprovingBiomedical2019,
  title = {{{BioWordVec}}, Improving Biomedical Word Embeddings with Subword Information and {{MeSH}}},
  author = {Zhang, Yijia and Chen, Qingyu and Yang, Zhihao and Lin, Hongfei and Lu, Zhiyong},
  year = {2019},
  month = dec,
  journal = {Scientific Data},
  volume = {6},
  number = {1},
  pages = {52},
  issn = {2052-4463},
  doi = {10.1038/s41597-019-0055-0},
  urldate = {2021-11-30},
  langid = {english}
}

@inproceedings{zhangConversationalSearchRecommendation2018,
  title = {Towards {{Conversational Search}} and {{Recommendation}}: {{System Ask}}, {{User Respond}}},
  shorttitle = {Towards {{Conversational Search}} and {{Recommendation}}},
  booktitle = {Proceedings of the 27th {{ACM International Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Zhang, Yongfeng and Chen, Xu and Ai, Qingyao and Yang, Liu and Croft, W. Bruce},
  year = {2018},
  month = oct,
  series = {{{CIKM}} '18},
  pages = {177--186},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3269206.3271776},
  urldate = {2022-08-29},
  abstract = {Conversational search and recommendation based on user-system dialogs exhibit major differences from conventional search and recommendation tasks in that 1) the user and system can interact for multiple semantically coherent rounds on a task through natural language dialog, and 2) it becomes possible for the system to understand the user needs or to help users clarify their needs by asking appropriate questions from the users directly. We believe the ability to ask questions so as to actively clarify the user needs is one of the most important advantages of conversational search and recommendation. In this paper, we propose and evaluate a unified conversational search/recommendation framework, in an attempt to make the research problem doable under a standard formalization. Specifically, we propose a System Ask -- User Respond (SAUR) paradigm for conversational search, define the major components of the paradigm, and design a unified implementation of the framework for product search and recommendation in e-commerce. To accomplish this, we propose the Multi-Memory Network (MMN) architecture, which can be trained based on large-scale collections of user reviews in e-commerce. The system is capable of asking aspect-based questions in the right order so as to understand the user needs, while (personalized) search is conducted during the conversation, and results are provided when the system feels confident. Experiments on real-world user purchasing data verified the advantages of conversational search and recommendation against conventional search and recommendation algorithms in terms of standard evaluation measures such as NDCG.},
  isbn = {978-1-4503-6014-2}
}

@inproceedings{zhangCroAnoCrowdAnnotation2021,
  title = {{{CroAno}} : {{A Crowd Annotation Platform}} for {{Improving Label Consistency}} of {{Chinese NER Dataset}}},
  shorttitle = {{{CroAno}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}: {{System Demonstrations}}},
  author = {Zhang, Baoli and Li, Zhucong and Gan, Zhen and Chen, Yubo and Wan, Jing and Liu, Kang and Zhao, Jun and Liu, Shengping and Shi, Yafei},
  year = {2021},
  month = nov,
  pages = {275--282},
  publisher = {Association for Computational Linguistics},
  address = {Online and Punta Cana, Dominican Republic},
  doi = {10.18653/v1/2021.emnlp-demo.32},
  urldate = {2023-08-02},
  abstract = {In this paper, we introduce CroAno, a web-based crowd annotation platform for the Chinese named entity recognition (NER). Besides some basic features for crowd annotation like fast tagging and data management, CroAno provides a systematic solution for improving label consistency of Chinese NER dataset. 1) Disagreement Adjudicator: CroAno uses a multi-dimensional highlight mode to visualize instance-level inconsistent entities and makes the revision process user-friendly. 2) Inconsistency Detector: CroAno employs a detector to locate corpus-level label inconsistency and provides users an interface to correct inconsistent entities in batches. 3) Prediction Error Analyzer: We deconstruct the entity prediction error of the model to six fine-grained entity error types. Users can employ this error system to detect corpus-level inconsistency from a model perspective. To validate the effectiveness of our platform, we use CroAno to revise two public datasets. In the two revised datasets, we get an improvement of +1.96\% and +2.57\% F1 respectively in model performance.}
}

@misc{zhangEnhancingWebAgents2025,
  title = {Enhancing {{Web Agents}} with {{Explicit Rollback Mechanisms}}},
  author = {Zhang, Zhisong and Fang, Tianqing and Ma, Kaixin and Yu, Wenhao and Zhang, Hongming and Mi, Haitao and Yu, Dong},
  year = {2025},
  month = apr,
  number = {arXiv:2504.11788},
  eprint = {2504.11788},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.11788},
  urldate = {2025-04-18},
  abstract = {With recent advancements in large language models, web agents have been greatly improved. However, dealing with complex and dynamic web environments requires more advanced planning and search abilities. Previous studies usually adopt a greedy one-way search strategy, which may struggle to recover from erroneous states. In this work, we enhance web agents with an explicit rollback mechanism, enabling the agent to revert back to a previous state in its navigation trajectory. This mechanism gives the model the flexibility to directly control the search process, leading to an effective and efficient web navigation method. We conduct experiments on two live web navigation benchmarks with zero-shot and fine-tuning settings. The results demonstrate the effectiveness of our proposed approach.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@misc{zhangGraphlessNeuralNetworks2022,
  title = {Graph-Less {{Neural Networks}}: {{Teaching Old MLPs New Tricks}} via {{Distillation}}},
  shorttitle = {Graph-Less {{Neural Networks}}},
  author = {Zhang, Shichang and Liu, Yozen and Sun, Yizhou and Shah, Neil},
  year = {2022},
  month = mar,
  number = {arXiv:2110.08727},
  eprint = {2110.08727},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.08727},
  urldate = {2022-11-16},
  abstract = {Graph Neural Networks (GNNs) are popular for graph machine learning and have shown great results on wide node classification tasks. Yet, they are less popular for practical deployments in the industry owing to their scalability challenges incurred by data dependency. Namely, GNN inference depends on neighbor nodes multiple hops away from the target, and fetching them burdens latency-constrained applications. Existing inference acceleration methods like pruning and quantization can speed up GNNs by reducing Multiplication-and-ACcumulation (MAC) operations, but the improvements are limited given the data dependency is not resolved. Conversely, multi-layer perceptrons (MLPs) have no graph dependency and infer much faster than GNNs, even though they are less accurate than GNNs for node classification in general. Motivated by these complementary strengths and weaknesses, we bring GNNs and MLPs together via knowledge distillation (KD). Our work shows that the performance of MLPs can be improved by large margins with GNN KD. We call the distilled MLPs Graph-less Neural Networks (GLNNs) as they have no inference graph dependency. We show that GLNNs with competitive accuracy infer faster than GNNs by 146X-273X and faster than other acceleration methods by 14X-27X. Under a production setting involving both transductive and inductive predictions across 7 datasets, GLNN accuracies improve over stand-alone MLPs by 12.36\% on average and match GNNs on 6/7 datasets. Comprehensive analysis shows when and why GLNNs can achieve competitive accuracies to GNNs and suggests GLNN as a handy choice for latency-constrained applications.},
  archiveprefix = {arXiv}
}

@article{zhangIntentionCentricLearningDual2024,
  title = {Intention-{{Centric Learning}} via {{Dual Attention}} for {{Sequential Recommendation}}},
  author = {Zhang, Zhigao and Wang, Bin and Xie, Xinqiang},
  year = {2024},
  journal = {IEEE Access},
  volume = {12},
  pages = {2854--2867},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2023.3349188},
  urldate = {2024-09-24},
  abstract = {In sequential recommendation, it is critical to accurately capture the user's intention with limited session information. Previous work concentrates on modeling a single relationship existing between items in an ongoing session, e.g., sequential dependency or graph structures. They lack the exploration of constituent semantic properties of user intention and also lack effective mechanisms binding important features to deduce user intention accurately. In this paper, we present a novel intention detection-enhanced sequential recommendation model, named DASR, which can capture both sequential dependencies and user intent components. We innovatively introduce slot attention to bind the low-level local features extracted by CNN and design multiple rounds of competitive iteration mechanisms to refine the high-level representation of user intent continuously. Finally, these high-level features cooperate with the global dependencies captured by self-attention to achieve sequential recommendations. Extensive experiments are carried out on three benchmark datasets, the experimental results show that DASR outperforms the state-of-the-art baseline methods up to 6.91\%, and 4.73\% on Recall@20, and MRR@20, respectively.},
  copyright = {https://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english}
}

@inproceedings{zhangItsFairGame2024,
  title = {"{{It}}'s a {{Fair Game}}", or {{Is It}}? {{Examining How Users Navigate Disclosure Risks}} and {{Benefits When Using LLM-Based Conversational Agents}}},
  shorttitle = {"{{It}}'s a {{Fair Game}}", or {{Is It}}?},
  booktitle = {Proceedings of the {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Zhang, Zhiping and Jia, Michelle and Lee, Hao-Ping and Yao, Bingsheng and Das, Sauvik and Lerner, Ada and Wang, Dakuo and Li, Tianshi},
  year = {2024},
  month = may,
  eprint = {2309.11653},
  primaryclass = {cs},
  pages = {1--26},
  doi = {10.1145/3613904.3642385},
  urldate = {2024-09-12},
  abstract = {The widespread use of Large Language Model (LLM)-based conversational agents (CAs), especially in high-stakes domains, raises many privacy concerns. Building ethical LLM-based CAs that respect user privacy requires an in-depth understanding of the privacy risks that concern users the most. However, existing research, primarily model-centered, does not provide insight into users' perspectives. To bridge this gap, we analyzed sensitive disclosures in real-world ChatGPT conversations and conducted semi-structured interviews with 19 LLM-based CA users. We found that users are constantly faced with trade-offs between privacy, utility, and convenience when using LLM-based CAs. However, users' erroneous mental models and the dark patterns in system design limited their awareness and comprehension of the privacy risks. Additionally, the human-like interactions encouraged more sensitive disclosures, which complicated users' ability to navigate the trade-offs. We discuss practical design guidelines and the needs for paradigm shifts to protect the privacy of LLM-based CA users.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security,Computer Science - Human-Computer Interaction}
}

@misc{zhangLanguageModelAnnotator2023,
  title = {Language {{Model}} as an {{Annotator}}: {{Unsupervised Context-aware Quality Phrase Generation}}},
  shorttitle = {Language {{Model}} as an {{Annotator}}},
  author = {Zhang, Zhihao and Zuo, Yuan and Lin, Chenghua and Wu, Junjie},
  year = {2023},
  month = dec,
  number = {arXiv:2312.17349},
  eprint = {2312.17349},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.17349},
  urldate = {2024-01-01},
  abstract = {Phrase mining is a fundamental text mining task that aims to identify quality phrases from context. Nevertheless, the scarcity of extensive gold labels datasets, demanding substantial annotation efforts from experts, renders this task exceptionally challenging. Furthermore, the emerging, infrequent, and domain-specific nature of quality phrases presents further challenges in dealing with this task. In this paper, we propose LMPhrase, a novel unsupervised context-aware quality phrase mining framework built upon large pre-trained language models (LMs). Specifically, we first mine quality phrases as silver labels by employing a parameter-free probing technique called Perturbed Masking on the pre-trained language model BERT (coined as Annotator). In contrast to typical statistic-based or distantly-supervised methods, our silver labels, derived from large pre-trained language models, take into account rich contextual information contained in the LMs. As a result, they bring distinct advantages in preserving informativeness, concordance, and completeness of quality phrases. Secondly, training a discriminative span prediction model heavily relies on massive annotated data and is likely to face the risk of overfitting silver labels. Alternatively, we formalize phrase tagging task as the sequence generation problem by directly fine-tuning on the Sequence-to-Sequence pre-trained language model BART with silver labels (coined as Generator). Finally, we merge the quality phrases from both the Annotator and Generator as the final predictions, considering their complementary nature and distinct characteristics. Extensive experiments show that our LMPhrase consistently outperforms all the existing competitors across two different granularity phrase mining tasks, where each task is tested on two different domain datasets.},
  archiveprefix = {arXiv}
}

@misc{zhangLatentPromptTuning2022,
  title = {Latent {{Prompt Tuning}} for {{Text Summarization}}},
  author = {Zhang, Yubo and Zhang, Xingxing and Wang, Xun and Chen, Si-qing and Wei, Furu},
  year = {2022},
  month = dec,
  number = {arXiv:2211.01837},
  eprint = {2211.01837},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.01837},
  urldate = {2023-03-13},
  abstract = {Prompts with different control signals (e.g., length, keywords, etc.) can be used to control text summarization. When control signals are available, they can control the properties of generated summaries and potentially improve summarization quality (since more information are given). Unfortunately, control signals are not already available during inference time. In this paper, we propose Lotus (shorthand for Latent Prompt Tuning for Summarization), which is a single model that can be applied in both controlled and uncontrolled (without control signals) modes. During training, Lotus learns latent prompt representations from prompts with gold control signals using a contrastive learning objective. Experiments show Lotus in uncontrolled mode consistently improves upon strong (uncontrollable) summarization models across four different summarization datasets. We also demonstrate generated summaries can be controlled using prompts with user specified control tokens.},
  archiveprefix = {arXiv}
}

@misc{zhangLeveragingDualProcess2025,
  title = {Leveraging {{Dual Process Theory}} in {{Language Agent Framework}} for {{Real-time Simultaneous Human-AI Collaboration}}},
  author = {Zhang, Shao and Wang, Xihuai and Zhang, Wenhao and Li, Chaoran and Song, Junru and Li, Tingyu and Qiu, Lin and Cao, Xuezhi and Cai, Xunliang and Yao, Wen and Zhang, Weinan and Wang, Xinbing and Wen, Ying},
  year = {2025},
  month = mar,
  number = {arXiv:2502.11882},
  eprint = {2502.11882},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.11882},
  urldate = {2025-04-05},
  abstract = {Agents built on large language models (LLMs) have excelled in turn-by-turn human-AI collaboration but struggle with simultaneous tasks requiring real-time interaction. Latency issues and the challenge of inferring variable human strategies hinder their ability to make autonomous decisions without explicit instructions. Through experiments with current independent System 1 and System 2 methods, we validate the necessity of using Dual Process Theory (DPT) in real-time tasks. We propose DPT-Agent, a novel language agent framework that integrates System 1 and System 2 for efficient real-time simultaneous human-AI collaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and code-as-policy for fast, intuitive, and controllable decision-making. DPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous reflection to infer human intentions and perform reasoning-based autonomous decisions. We demonstrate the effectiveness of DPT-Agent through further experiments with rule-based agents and human collaborators, showing significant improvements over mainstream LLM-based frameworks. DPT-Agent can effectively help LLMs convert correct slow thinking and reasoning into executable actions, thereby improving performance. To the best of our knowledge, DPT-Agent is the first language agent framework that achieves successful real-time simultaneous human-AI collaboration autonomously. Code of DPT-Agent can be found in https://github.com/sjtu-marl/DPT-Agent.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Multiagent Systems}
}

@inproceedings{zhangLLMaAAMakingLarge2023,
  title = {{{LLMaAA}}: {{Making Large Language Models}} as {{Active Annotators}}},
  shorttitle = {{{LLMaAA}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2023},
  author = {Zhang, Ruoyu and Li, Yanzeng and Ma, Yongliang and Zhou, Ming and Zou, Lei},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  year = {2023},
  month = dec,
  pages = {13088--13103},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.findings-emnlp.872},
  urldate = {2024-02-11},
  abstract = {Prevalent supervised learning methods in natural language processing (NLP) are notoriously data-hungry, which demand large amounts of high-quality annotated data. In practice, acquiring such data is a costly endeavor. Recently, the superior few-shot performance of large language models (LLMs) has propelled the development of dataset generation, where the training data are solely synthesized from LLMs. However, such an approach usually suffers from low-quality issues, and requires orders of magnitude more labeled data to achieve satisfactory performance. To fully exploit the potential of LLMs and make use of massive unlabeled data, we propose LLMaAA, which takes LLMs as annotators and puts them into an active learning loop to determine what to annotate efficiently. To learn robustly with pseudo labels, we optimize both the annotation and training processes: (1) we draw k-NN examples from a small demonstration pool as in-context examples, and (2) we adopt the example reweighting technique to assign training samples with learnable weights. Compared with previous approaches, LLMaAA features both efficiency and reliability. We conduct experiments and analysis on two classic NLP tasks, named entity recognition and relation extraction. With LLMaAA, task-specific models trained from LLM-generated labels can outperform the teacher within only hundreds of annotated examples, which is much more cost-effective than other baselines.}
}

@misc{zhangMACSumControllableSummarization2022,
  title = {{{MACSum}}: {{Controllable Summarization}} with {{Mixed Attributes}}},
  shorttitle = {{{MACSum}}},
  author = {Zhang, Yusen and Liu, Yang and Yang, Ziyi and Fang, Yuwei and Chen, Yulong and Radev, Dragomir and Zhu, Chenguang and Zeng, Michael and Zhang, Rui},
  year = {2022},
  month = nov,
  number = {arXiv:2211.05041},
  eprint = {2211.05041},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.05041},
  urldate = {2023-03-14},
  abstract = {Controllable summarization allows users to generate customized summaries with specified attributes. However, due to the lack of designated annotations of controlled summaries, existing works have to craft pseudo datasets by adapting generic summarization benchmarks. Furthermore, most research focuses on controlling single attributes individually (e.g., a short summary or a highly abstractive summary) rather than controlling a mix of attributes together (e.g., a short and highly abstractive summary). In this paper, we propose MACSum, the first human-annotated summarization dataset for controlling mixed attributes. It contains source texts from two domains, news articles and dialogues, with human-annotated summaries controlled by five designed attributes (Length, Extractiveness, Specificity, Topic, and Speaker). We propose two simple and effective parameter-efficient approaches for the new task of mixed controllable summarization based on hard prompt tuning and soft prefix tuning. Results and analysis demonstrate that hard prompt models yield the best performance on all metrics and human evaluations. However, mixed-attribute control is still challenging for summarization tasks. Our dataset and code are available at https://github.com/psunlpgroup/MACSum.},
  archiveprefix = {arXiv}
}

@misc{zhangMultimodalChainofThoughtReasoning2023,
  title = {Multimodal {{Chain-of-Thought Reasoning}} in {{Language Models}}},
  author = {Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Zhao, Hai and Karypis, George and Smola, Alex},
  year = {2023},
  month = feb,
  number = {arXiv:2302.00923},
  eprint = {2302.00923},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2302.00923},
  urldate = {2023-02-24},
  abstract = {Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer. However, existing CoT studies have focused on the language modality. We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference. In this way, answer inference can leverage better generated rationales that are based on multimodal information. With Multimodal-CoT, our model under 1 billion parameters outperforms the previous state-of-the-art LLM (GPT-3.5) by 16 percentage points (75.17\%-{$>$}91.68\% accuracy) on the ScienceQA benchmark and even surpasses human performance. Code is publicly available available at https://github.com/amazon-science/mm-cot.},
  archiveprefix = {arXiv}
}

@misc{zhangOPTOpenPretrained2022,
  title = {{{OPT}}: {{Open Pre-trained Transformer Language Models}}},
  shorttitle = {{{OPT}}},
  author = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Shuster, Kurt and Simig, Daniel and Koura, Punit Singh and Sridhar, Anjali and Wang, Tianlu and Zettlemoyer, Luke},
  year = {2022},
  month = jun,
  number = {arXiv:2205.01068},
  eprint = {2205.01068},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2205.01068},
  urldate = {2022-07-27},
  abstract = {Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.},
  archiveprefix = {arXiv}
}

@inproceedings{zhangPEANUTHumanAICollaborative2023,
  title = {{{PEANUT}}: {{A Human-AI Collaborative Tool}} for {{Annotating Audio-Visual Data}}},
  shorttitle = {{{PEANUT}}},
  booktitle = {Proceedings of the 36th {{Annual ACM Symposium}} on {{User Interface Software}} and {{Technology}}},
  author = {Zhang, Zheng and Ning, Zheng and Xu, Chenliang and Tian, Yapeng and Li, Toby Jia-Jun},
  year = {2023},
  month = oct,
  series = {{{UIST}} '23},
  pages = {1--18},
  publisher = {ACM},
  address = {San Francisco CA USA},
  doi = {10.1145/3586183.3606776},
  urldate = {2023-12-10},
  abstract = {Audio-visual learning seeks to enhance the computer's multi-modal perception leveraging the correlation between the auditory and visual modalities. Despite their many useful downstream tasks, such as video retrieval, AR/VR, and accessibility, the performance and adoption of existing audio-visual models have been impeded by the availability of high-quality datasets. Annotating audio-visual datasets is laborious, expensive, and time-consuming. To address this challenge, we designed and developed an efficient audio-visual annotation tool called Peanut. Peanut's human-AI collaborative pipeline separates the multi-modal task into two single-modal tasks, and utilizes state-of-the-art object detection and sound-tagging models to reduce the annotators' effort to process each frame and the number of manually-annotated frames needed. A within-subject user study with 20 participants found that Peanut can significantly accelerate the audio-visual data annotation process while maintaining high annotation accuracy.},
  isbn = {979-8-4007-0132-0},
  langid = {english}
}

@misc{zhangPrivacyLeakageOvershadowed2025,
  title = {Privacy {{Leakage Overshadowed}} by {{Views}} of {{AI}}: {{A Study}} on {{Human Oversight}} of {{Privacy}} in {{Language Model Agent}}},
  shorttitle = {Privacy {{Leakage Overshadowed}} by {{Views}} of {{AI}}},
  author = {Zhang, Zhiping and Guo, Bingcan and Li, Tianshi},
  year = {2025},
  month = jan,
  number = {arXiv:2411.01344},
  eprint = {2411.01344},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.01344},
  urldate = {2025-02-27},
  abstract = {Language model (LM) agents that act on users' behalf for personal tasks (e.g., replying emails) can boost productivity, but are also susceptible to unintended privacy leakage risks. We present the first study on people's capacity to oversee the privacy implications of the LM agents. By conducting a task-based survey (N=300), we investigate how people react to and assess the response generated by LM agents for asynchronous interpersonal communication tasks, compared with a response they wrote. We found that people may favor the agent response with more privacy leakage over the response they drafted or consider both good, leading to an increased harmful disclosure from 15.7\% to 55.0\%. We further identified six privacy profiles to characterize distinct patterns of concerns, trust, and privacy preferences in LM agents. Our findings shed light on designing agentic systems that enable privacy-preserving interactions and achieve bidirectional alignment on privacy preferences to help users calibrate trust.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security,Computer Science - Human-Computer Interaction}
}

@misc{zhangRAFTAdaptingLanguage2024,
  title = {{{RAFT}}: {{Adapting Language Model}} to {{Domain Specific RAG}}},
  shorttitle = {{{RAFT}}},
  author = {Zhang, Tianjun and Patil, Shishir G. and Jain, Naman and Shen, Sheng and Zaharia, Matei and Stoica, Ion and Gonzalez, Joseph E.},
  year = {2024},
  month = jun,
  number = {arXiv:2403.10131},
  eprint = {2403.10131},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.10131},
  urldate = {2024-06-16},
  abstract = {Pretraining Large Language Models (LLMs) on large corpora of textual data is now a standard paradigm. When using these LLMs for many downstream applications, it is common to additionally bake in new knowledge (e.g., time-critical news, or private domain knowledge) into the pretrained model either through RAG-based-prompting, or fine-tuning. However, the optimal methodology for the model to gain such new knowledge remains an open question. In this paper, we present Retrieval Augmented FineTuning (RAFT), a training recipe that improves the model's ability to answer questions in a "open-book" in-domain settings. In RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don't help in answering the question, which we call, distractor documents. RAFT accomplishes this by citing verbatim the right sequence from the relevant document that would help answer the question. This coupled with RAFT's chain-of-thought-style response helps improve the model's ability to reason. In domain-specific RAG, RAFT consistently improves the model's performance across PubMed, HotpotQA, and Gorilla datasets, presenting a post-training recipe to improve pre-trained LLMs to in-domain RAG. RAFT's code and demo are open-sourced at github.com/ShishirPatil/gorilla.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@inproceedings{zhangRethinkingHumanAICollaboration2024,
  title = {Rethinking {{Human-AI Collaboration}} in {{Complex Medical Decision Making}}: {{A Case Study}} in {{Sepsis Diagnosis}}},
  shorttitle = {Rethinking {{Human-AI Collaboration}} in {{Complex Medical Decision Making}}},
  booktitle = {Proceedings of the {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Zhang, Shao and Yu, Jianing and Xu, Xuhai and Yin, Changchang and Lu, Yuxuan and Yao, Bingsheng and Tory, Melanie and Padilla, Lace M. and Caterino, Jeffrey and Zhang, Ping and Wang, Dakuo},
  year = {2024},
  month = may,
  eprint = {2309.12368},
  primaryclass = {cs},
  pages = {1--18},
  doi = {10.1145/3613904.3642343},
  urldate = {2024-09-12},
  abstract = {Today's AI systems for medical decision support often succeed on benchmark datasets in research papers but fail in real-world deployment. This work focuses on the decision making of sepsis, an acute life-threatening systematic infection that requires an early diagnosis with high uncertainty from the clinician. Our aim is to explore the design requirements for AI systems that can support clinical experts in making better decisions for the early diagnosis of sepsis. The study begins with a formative study investigating why clinical experts abandon an existing AI-powered Sepsis predictive module in their electrical health record (EHR) system. We argue that a human-centered AI system needs to support human experts in the intermediate stages of a medical decision-making process (e.g., generating hypotheses or gathering data), instead of focusing only on the final decision. Therefore, we build SepsisLab based on a state-of-the-art AI algorithm and extend it to predict the future projection of sepsis development, visualize the prediction uncertainty, and propose actionable suggestions (i.e., which additional laboratory tests can be collected) to reduce such uncertainty. Through heuristic evaluation with six clinicians using our prototype system, we demonstrate that SepsisLab enables a promising human-AI collaboration paradigm for the future of AI-assisted sepsis diagnosis and other high-stakes medical decision making.},
  archiveprefix = {arXiv},
  keywords = {68U35,Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,H.5.2,I.2.1}
}

@inproceedings{zhangStoryBuddyHumanAICollaborative2022,
  title = {{{StoryBuddy}}: {{A Human-AI Collaborative Chatbot}} for {{Parent-Child Interactive Storytelling}} with {{Flexible Parental Involvement}}},
  shorttitle = {{{StoryBuddy}}},
  booktitle = {{{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Zhang, Zheng and Xu, Ying and Wang, Yanhao and Yao, Bingsheng and Ritchie, Daniel and Wu, Tongshuang and Yu, Mo and Wang, Dakuo and Li, Toby Jia-Jun},
  year = {2022},
  month = apr,
  eprint = {2202.06205},
  primaryclass = {cs},
  pages = {1--21},
  doi = {10.1145/3491102.3517479},
  urldate = {2023-01-18},
  abstract = {Despite its benefits for children's skill development and parent-child bonding, many parents do not often engage in interactive storytelling by having story-related dialogues with their child due to limited availability or challenges in coming up with appropriate questions. While recent advances made AI generation of questions from stories possible, the fully-automated approach excludes parent involvement, disregards educational goals, and underoptimizes for child engagement. Informed by need-finding interviews and participatory design (PD) results, we developed StoryBuddy, an AI-enabled system for parents to create interactive storytelling experiences. StoryBuddy's design highlighted the need for accommodating dynamic user needs between the desire for parent involvement and parent-child bonding and the goal of minimizing parent intervention when busy. The PD revealed varied assessment and educational goals of parents, which StoryBuddy addressed by supporting configuring question types and tracking child progress. A user study validated StoryBuddy's usability and suggested design insights for future parent-AI collaboration systems.},
  archiveprefix = {arXiv}
}

@misc{zhangTinyLlamaOpenSourceSmall2024,
  title = {{{TinyLlama}}: {{An Open-Source Small Language Model}}},
  shorttitle = {{{TinyLlama}}},
  author = {Zhang, Peiyuan and Zeng, Guangtao and Wang, Tianduo and Lu, Wei},
  year = {2024},
  number = {arXiv:2401.02385},
  eprint = {2401.02385},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.02385},
  urldate = {2024-04-26},
  abstract = {We present TinyLlama, a compact 1.1B language model pretrained on around 1 trillion tokens for approximately 3 epochs. Building on the architecture and tokenizer of Llama 2, TinyLlama leverages various advances contributed by the open-source community (e.g., FlashAttention), achieving better computational efficiency. Despite its relatively small size, TinyLlama demonstrates remarkable performance in a series of downstream tasks. It significantly outperforms existing open-source language models with comparable sizes. Our model checkpoints and code are publicly available on GitHub at https://github.com/jzhang38/TinyLlama.},
  archiveprefix = {arXiv},
  langid = {american},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@misc{zhangWhenYouNeed2020,
  title = {When {{Do You Need Billions}} of {{Words}} of {{Pretraining Data}}?},
  author = {Zhang, Yian and Warstadt, Alex and Li, Haau-Sing and Bowman, Samuel R.},
  year = {2020},
  number = {arXiv:2011.04946},
  eprint = {2011.04946},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2011.04946},
  urldate = {2024-04-25},
  abstract = {NLP is currently dominated by general-purpose pretrained language models like RoBERTa, which achieve strong performance on NLU tasks through pretraining on billions of words. But what exact knowledge or skills do Transformer LMs learn from large-scale pretraining that they cannot learn from less data? We adopt four probing methods---classifier probing, information-theoretic probing, unsupervised relative acceptability judgment, and fine-tuning on NLU tasks---and draw learning curves that track the growth of these different measures of linguistic ability with respect to pretraining data volume using the MiniBERTas, a group of RoBERTa models pretrained on 1M, 10M, 100M and 1B words. We find that LMs require only about 10M or 100M words to learn representations that reliably encode most syntactic and semantic features we test. A much larger quantity of data is needed in order to acquire enough commonsense knowledge and other skills required to master typical downstream NLU tasks. The results suggest that, while the ability to encode linguistic features is almost certainly necessary for language understanding, it is likely that other forms of knowledge are the major drivers of recent improvements in language understanding among large pretrained models.},
  archiveprefix = {arXiv},
  langid = {american}
}

@misc{zhaoEducationalQuestionGeneration2022,
  title = {Educational {{Question Generation}} of {{Children Storybooks}} via {{Question Type Distribution Learning}} and {{Event-Centric Summarization}}},
  author = {Zhao, Zhenjie and Hou, Yufang and Wang, Dakuo and Yu, Mo and Liu, Chengzhong and Ma, Xiaojuan},
  year = {2022},
  month = mar,
  number = {arXiv:2203.14187},
  eprint = {2203.14187},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2203.14187},
  urldate = {2023-01-18},
  abstract = {Generating educational questions of fairytales or storybooks is vital for improving children's literacy ability. However, it is challenging to generate questions that capture the interesting aspects of a fairytale story with educational meaningfulness. In this paper, we propose a novel question generation method that first learns the question type distribution of an input story paragraph, and then summarizes salient events which can be used to generate high-cognitive-demand questions. To train the event-centric summarizer, we finetune a pre-trained transformer-based sequence-to-sequence model using silver samples composed by educational question-answer pairs. On a newly proposed educational question answering dataset FairytaleQA, we show good performance of our method on both automatic and human evaluation metrics. Our work indicates the necessity of decomposing question type distribution learning and event-centric summary generation for educational question generation.},
  archiveprefix = {arXiv}
}

@misc{zhaoKuaiSimComprehensiveSimulator2023,
  title = {{{KuaiSim}}: {{A Comprehensive Simulator}} for {{Recommender Systems}}},
  shorttitle = {{{KuaiSim}}},
  author = {Zhao, Kesen and Liu, Shuchang and Cai, Qingpeng and Zhao, Xiangyu and Liu, Ziru and Zheng, Dong and Jiang, Peng and Gai, Kun},
  year = {2023},
  month = oct,
  number = {arXiv:2309.12645},
  eprint = {2309.12645},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2309.12645},
  urldate = {2024-11-20},
  abstract = {Reinforcement Learning (RL)-based recommender systems (RSs) have garnered considerable attention due to their ability to learn optimal recommendation policies and maximize long-term user rewards. However, deploying RL models directly in online environments and generating authentic data through A/B tests can pose challenges and require substantial resources. Simulators offer an alternative approach by providing training and evaluation environments for RS models, reducing reliance on real-world data. Existing simulators have shown promising results but also have limitations such as simplified user feedback, lacking consistency with real-world data, the challenge of simulator evaluation, and difficulties in migration and expansion across RSs. To address these challenges, we propose KuaiSim, a comprehensive user environment that provides user feedback with multi-behavior and cross-session responses. The resulting simulator can support three levels of recommendation problems: the request level list-wise recommendation task, the whole-session level sequential recommendation task, and the cross-session level retention optimization task. For each task, KuaiSim also provides evaluation protocols and baseline recommendation algorithms that further serve as benchmarks for future research. We also restructure existing competitive simulators on the KuaiRand Dataset and compare them against KuaiSim to future assess their performance and behavioral differences. Furthermore, to showcase KuaiSim's flexibility in accommodating different datasets, we demonstrate its versatility and robustness when deploying it on the ML-1m dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval}
}

@misc{zhaoOuroborosSpeculativeDecoding2024,
  title = {Ouroboros: {{Speculative Decoding}} with {{Large Model Enhanced Drafting}}},
  shorttitle = {Ouroboros},
  author = {Zhao, Weilin and Huang, Yuxiang and Han, Xu and Xiao, Chaojun and Liu, Zhiyuan and Sun, Maosong},
  year = {2024},
  month = feb,
  number = {arXiv:2402.13720},
  eprint = {2402.13720},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2402.13720},
  urldate = {2024-02-26},
  abstract = {Drafting-then-verifying decoding methods such as speculative decoding are widely adopted training-free methods to accelerate the inference of large language models (LLMs). Instead of employing an autoregressive process to decode tokens sequentially, speculative decoding initially creates drafts with an efficient small model. Then LLMs are required to conduct verification and correction in a non-autoregressive fashion to minimize time overhead. Generating longer drafts can lead to even more significant speedups once verified, but also incurs substantial trial and error costs if it fails. Suffering from the high verification failure probability, existing decoding methods cannot draft too much content for verification at one time, achieving sub-optimal inference acceleration. In this paper, we introduce Ouroboros, which constructs a phrase candidate pool from the verification process of LLMs to provide candidates for draft generation of the small model. Thereby, Ouroboros can further improve the efficiency and effectiveness of the initial drafts. The experimental results on typical text generation tasks show that Ouroboros achieves speedups of up to 1.9x and 2.8x compared to lookahead decoding and speculative decoding, respectively. The source code of Ouroboros is available at https://github.com/thunlp/Ouroboros.},
  archiveprefix = {arXiv}
}

@misc{zhaoPyHealthPythonLibrary2021,
  title = {{{PyHealth}}: {{A Python Library}} for {{Health Predictive Models}}},
  shorttitle = {{{PyHealth}}},
  author = {Zhao, Yue and Qiao, Zhi and Xiao, Cao and Glass, Lucas and Sun, Jimeng},
  year = {2021},
  month = jan,
  number = {arXiv:2101.04209},
  eprint = {2101.04209},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2101.04209},
  urldate = {2024-01-14},
  abstract = {Despite the explosion of interest in healthcare AI research, the reproducibility and benchmarking of those research works are often limited due to the lack of standard benchmark datasets and diverse evaluation metrics. To address this reproducibility challenge, we develop PyHealth, an open-source Python toolbox for developing various predictive models on healthcare data. PyHealth consists of data preprocessing module, predictive modeling module, and evaluation module. The target users of PyHealth are both computer science researchers and healthcare data scientists. With PyHealth, they can conduct complex machine learning pipelines on healthcare datasets with fewer than ten lines of code. The data preprocessing module enables the transformation of complex healthcare datasets such as longitudinal electronic health records, medical images, continuous signals (e.g., electrocardiogram), and clinical notes into machine learning friendly formats. The predictive modeling module provides more than 30 machine learning models, including established ensemble trees and deep neural network-based approaches, via a unified but extendable API designed for both researchers and practitioners. The evaluation module provides various evaluation strategies (e.g., cross-validation and train-validation-test split) and predictive model metrics. With robustness and scalability in mind, best practices such as unit testing, continuous integration, code coverage, and interactive examples are introduced in the library's development. PyHealth can be installed through the Python Package Index (PyPI) or https://github.com/yzhao062/PyHealth .},
  archiveprefix = {arXiv}
}

@misc{zhaoPyTorchFSDPExperiences2023,
  title = {{{PyTorch FSDP}}: {{Experiences}} on {{Scaling Fully Sharded Data Parallel}}},
  shorttitle = {{{PyTorch FSDP}}},
  author = {Zhao, Yanli and Gu, Andrew and Varma, Rohan and Luo, Liang and Huang, Chien-Chin and Xu, Min and Wright, Less and Shojanazeri, Hamid and Ott, Myle and Shleifer, Sam and Desmaison, Alban and Balioglu, Can and Damania, Pritam and Nguyen, Bernard and Chauhan, Geeta and Hao, Yuchen and Mathews, Ajit and Li, Shen},
  year = {2023},
  month = sep,
  number = {arXiv:2304.11277},
  eprint = {2304.11277},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.11277},
  urldate = {2025-03-20},
  abstract = {It is widely acknowledged that large models have the potential to deliver superior performance across a broad range of domains. Despite the remarkable progress made in the field of machine learning systems research, which has enabled the development and exploration of large models, such abilities remain confined to a small group of advanced users and industry leaders, resulting in an implicit technical barrier for the wider community to access and leverage these technologies. In this paper, we introduce PyTorch Fully Sharded Data Parallel (FSDP) as an industry-grade solution for large model training. FSDP has been closely co-designed with several key PyTorch core components including Tensor implementation, dispatcher system, and CUDA memory caching allocator, to provide non-intrusive user experiences and high training efficiency. Additionally, FSDP natively incorporates a range of techniques and settings to optimize resource utilization across a variety of hardware configurations. The experimental results demonstrate that FSDP is capable of achieving comparable performance to Distributed Data Parallel while providing support for significantly larger models with near-linear scalability in terms of TFLOPS.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning,Computer Science - Performance}
}

@article{zhaoRecommenderSystemsEra2024,
  title = {Recommender {{Systems}} in the {{Era}} of {{Large Language Models}} ({{LLMs}})},
  author = {Zhao, Zihuai and Fan, Wenqi and Li, Jiatong and Liu, Yunqing and Mei, Xiaowei and Wang, Yiqi and Wen, Zhen and Wang, Fei and Zhao, Xiangyu and Tang, Jiliang and Li, Qing},
  year = {2024},
  month = nov,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {36},
  number = {11},
  pages = {6889--6907},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2024.3392335},
  urldate = {2025-04-21},
  abstract = {With the prosperity of e-commerce and web applications, Recommender Systems (RecSys) have become an indispensable and important component, providing personalized suggestions that cater to user preferences. While Deep Neural Networks (DNNs) have achieved significant advancements in enhancing recommender systems, these DNN-based methods still exhibit some limitations, such as inferior capabilities to effectively capture textual side information about users and items, difficulties in generalization to various recommendation scenarios, and reasoning on their predictions, etc. Meanwhile, the development of Large Language Models (LLMs), such as ChatGPT and GPT-4, has revolutionized the fields of Natural Language Processing (NLP) and Artificial Intelligence (AI), due to their remarkable abilities in fundamental responsibilities of language understanding and generation, as well as impressive generalization capabilities and reasoning skills. As a result, recent studies have actively attempted to harness the power of LLMs to enhance recommender systems. Given the rapid evolution of this research direction in recommender systems, there is a pressing need for a systematic overview that summarizes existing LLM-empowered recommender systems. Therefore, in this survey, we comprehensively review LLM-empowered recommender systems from various perspectives including pre-training, fine-tuning, and prompting paradigms. More specifically, we first introduce the representative methods to learn user and item representations, leveraging LLMs as feature encoders. Then, we systematically review the emerging advanced techniques of LLMs for enhancing recommender systems from three paradigms, namely pre-training, fine-tuning, and prompting. Finally, we comprehensively discuss the promising future directions in this emerging field.},
  keywords = {Electronic mail,History,in-context learning,large language models (LLMs),Motion pictures,pre-training and fine-tuning,prompting,Recommender systems,Reviews,Surveys,Task analysis}
}

@inproceedings{zhaoTalkPapersBringing2020,
  title = {Talk to {{Papers}}: {{Bringing Neural Question Answering}} to {{Academic Search}}},
  shorttitle = {Talk to {{Papers}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{System Demonstrations}}},
  author = {Zhao, Tiancheng and Lee, Kyusong},
  editor = {Celikyilmaz, Asli and Wen, Tsung-Hsien},
  year = {2020},
  month = jul,
  pages = {30--36},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.acl-demos.5},
  urldate = {2024-02-04},
  abstract = {We introduce Talk to Papers, which exploits the recent open-domain question answering (QA) techniques to improve the current experience of academic search. It's designed to enable researchers to use natural language queries to find precise answers and extract insights from a massive amount of academic papers. We present a large improvement over classic search engine baseline on several standard QA datasets and provide the community a collaborative data collection tool to curate the first natural language processing research QA dataset via a community effort.}
}

@article{zhengDGLKETrainingKnowledge2020,
  title = {{{DGL-KE}}: {{Training Knowledge Graph Embeddings}} at {{Scale}}},
  shorttitle = {{{DGL-KE}}},
  author = {Zheng, Da and Song, Xiang and Ma, Chao and Tan, Zeyuan and Ye, Zihao and Dong, Jin and Xiong, Hao and Zhang, Zheng and Karypis, George},
  year = {2020},
  month = apr,
  journal = {arXiv:2004.08532 [cs]},
  eprint = {2004.08532},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2004.08532},
  urldate = {2022-01-25},
  abstract = {Knowledge graphs have emerged as a key abstraction for organizing information in diverse domains and their embeddings are increasingly used to harness their information in various information retrieval and machine learning tasks. However, the ever growing size of knowledge graphs requires computationally efficient algorithms capable of scaling to graphs with millions of nodes and billions of edges. This paper presents DGL-KE, an open-source package to efficiently compute knowledge graph embeddings. DGL-KE introduces various novel optimizations that accelerate training on knowledge graphs with millions of nodes and billions of edges using multi-processing, multi-GPU, and distributed parallelism. These optimizations are designed to increase data locality, reduce communication overhead, overlap computations with memory accesses, and achieve high operation efficiency. Experiments on knowledge graphs consisting of over 86M nodes and 338M edges show that DGL-KE can compute embeddings in 100 minutes on a EC2 instance with 8 GPUs and 30 minutes on an EC2 cluster with 4 machines with 48 cores/machine. These results represent a 2{\texttimes} {$\sim$} 5{\texttimes} speedup over the best competing approaches. DGL-KE is available on https://github.com/awslabs/dgl-ke.},
  archiveprefix = {arXiv},
  langid = {english}
}

@misc{zhengGPT4VisionGeneralistWeb2024,
  title = {{{GPT-4V}}(Ision) Is a {{Generalist Web Agent}}, If {{Grounded}}},
  author = {Zheng, Boyuan and Gou, Boyu and Kil, Jihyung and Sun, Huan and Su, Yu},
  year = {2024},
  month = mar,
  number = {arXiv:2401.01614},
  eprint = {2401.01614},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.01614},
  urldate = {2024-12-04},
  abstract = {The recent development on large multimodal models (LMMs), especially GPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries of multimodal models beyond traditional tasks like image captioning and visual question answering. In this work, we explore the potential of LMMs like GPT-4V as a generalist web agent that can follow natural language instructions to complete tasks on any given website. We propose SEEACT, a generalist web agent that harnesses the power of LMMs for integrated visual understanding and acting on the web. We evaluate on the recent MIND2WEB benchmark. In addition to standard offline evaluation on cached websites, we enable a new online evaluation setting by developing a tool that allows running web agents on live websites. We show that GPT-4V presents a great potential for web agents -- it can successfully complete 51.1 of the tasks on live websites if we manually ground its textual plans into actions on the websites. This substantially outperforms text-only LLMs like GPT-4 or smaller models (FLAN-T5 and BLIP-2) specifically fine-tuned for web agents. However, grounding still remains a major challenge. Existing LMM grounding strategies like set-of-mark prompting turns out to be not effective for web agents, and the best grounding strategy we develop in this paper leverages both the HTML structure and visuals. Yet, there is still a substantial gap with oracle grounding, leaving ample room for further improvement. All code, data, and evaluation tools are available at https://github.com/OSU-NLP-Group/SeeAct.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval}
}

@misc{zhengTopicGuidedAbstractiveText2021,
  title = {Topic-{{Guided Abstractive Text Summarization}}: A {{Joint Learning Approach}}},
  shorttitle = {Topic-{{Guided Abstractive Text Summarization}}},
  author = {Zheng, Chujie and Zhang, Kunpeng and Wang, Harry Jiannan and Fan, Ling and Wang, Zhe},
  year = {2021},
  month = aug,
  number = {arXiv:2010.10323},
  eprint = {2010.10323},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2010.10323},
  urldate = {2022-07-20},
  abstract = {We introduce a new approach for abstractive text summarization, Topic-Guided Abstractive Summarization, which calibrates long-range dependencies from topic-level features with globally salient content. The idea is to incorporate neural topic modeling with a Transformer-based sequence-to-sequence (seq2seq) model in a joint learning framework. This design can learn and preserve the global semantics of the document, which can provide additional contextual guidance for capturing important ideas of the document, thereby enhancing the generation of summary. We conduct extensive experiments on two datasets and the results show that our proposed model outperforms many extractive and abstractive systems in terms of both ROUGE measurements and human evaluation. Our code is available at: https://github.com/chz816/tas.},
  archiveprefix = {arXiv}
}

@inproceedings{zhengWebOlympusOpenPlatform2024,
  title = {{{WebOlympus}}: {{An Open Platform}} for {{Web Agents}} on {{Live Websites}}},
  shorttitle = {{{WebOlympus}}},
  booktitle = {Proceedings of the 2024 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}: {{System Demonstrations}}},
  author = {Zheng, Boyuan and Gou, Boyu and Salisbury, Scott and Du, Zheng and Sun, Huan and Su, Yu},
  editor = {Hernandez Farias, Delia Irazu and Hope, Tom and Li, Manling},
  year = {2024},
  month = nov,
  pages = {187--197},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-demo.20},
  urldate = {2024-12-11},
  abstract = {Web agents are emerging as powerful tools capable of performing complex tasks across diverse web environments. The rapid development of large multimodal models is further enhancing this advancement. However, there is a lack of standardized and user-friendly tools for research and development, as well as experimental platforms on live websites. To address this challenge, we present WebOlympus, an open platform for web agents operating on live websites. WebOlympus offers a Chrome extension-based UI, enabling users without programming experience to easily utilize the platform. It allows users to run web agents with various designs using only a few lines of code or simple clicks on the Chrome extension. To ensure the trustworthiness of web agents, a safety monitor module that prevents harmful actions through human supervision or model-based control is incorporated. WebOlympus supports diverse applications, including annotation interfaces for web agent trajectories and data crawling.}
}

@inproceedings{zhengWhenDoesPretraining2021a,
  title = {When Does Pretraining Help? Assessing Self-Supervised Learning for Law and the {{CaseHOLD}} Dataset of 53,000+ Legal Holdings},
  shorttitle = {When Does Pretraining Help?},
  booktitle = {Proceedings of the {{Eighteenth International Conference}} on {{Artificial Intelligence}} and {{Law}}},
  author = {Zheng, Lucia and Guha, Neel and Anderson, Brandon R. and Henderson, Peter and Ho, Daniel E.},
  year = {2021},
  month = jul,
  series = {{{ICAIL}} '21},
  pages = {159--168},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3462757.3466088},
  urldate = {2023-12-15},
  abstract = {While self-supervised learning has made rapid advances in natural language processing, it remains unclear when researchers should engage in resource-intensive domain-specific pretraining (domain pretraining). The law, puzzlingly, has yielded few documented instances of substantial gains to domain pretraining in spite of the fact that legal language is widely seen to be unique. We hypothesize that these existing results stem from the fact that existing legal NLP tasks are too easy and fail to meet conditions for when domain pretraining can help. To address this, we first present CaseHOLD (Case {$<$}u{$>$}H{$<$}/u{$>$}oldings {$<$}u{$>$}O{$<$}/u{$>$}n {$<$}u{$>$}L{$<$}/u{$>$}egal {$<$}u{$>$}D{$<$}/u{$>$}ecisions), a new dataset comprised of over 53,000+ multiple choice questions to identify the relevant holding of a cited case. This dataset presents a fundamental task to lawyers and is both legally meaningful and difficult from an NLP perspective (F1 of 0.4 with a BiLSTM baseline). Second, we assess performance gains on CaseHOLD and existing legal NLP datasets. While a Transformer architecture (BERT) pretrained on a general corpus (Google Books and Wikipedia) improves performance, domain pretraining (on a corpus of {$\approx$}3.5M decisions across all courts in the U.S. that is larger than BERT's) with a custom legal vocabulary exhibits the most substantial performance gains with CaseHOLD (gain of 7.2\% on F1, representing a 12\% improvement on BERT) and consistent performance gains across two other legal tasks. Third, we show that domain pretraining may be warranted when the task exhibits sufficient similarity to the pretraining corpus: the level of performance increase in three legal tasks was directly tied to the domain specificity of the task. Our findings inform when researchers should engage in resource-intensive pretraining and show that Transformer-based architectures, too, learn embeddings suggestive of distinct legal language.},
  isbn = {978-1-4503-8526-8}
}

@inproceedings{zhengWhenHelpfulAssistant2024,
  title = {When ''{{A Helpful Assistant}}'' {{Is Not Really Helpful}}: {{Personas}} in {{System Prompts Do Not Improve Performances}} of {{Large Language Models}}},
  shorttitle = {When ''{{A Helpful Assistant}}'' {{Is Not Really Helpful}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2024},
  author = {Zheng, Mingqian and Pei, Jiaxin and Logeswaran, Lajanugen and Lee, Moontae and Jurgens, David},
  editor = {{Al-Onaizan}, Yaser and Bansal, Mohit and Chen, Yun-Nung},
  year = {2024},
  month = nov,
  pages = {15126--15154},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.findings-emnlp.888},
  urldate = {2025-04-17},
  abstract = {Prompting serves as the major way humans interact with Large Language Models (LLM). Commercial AI systems commonly define the role of the LLM in system prompts. For example, ChatGPT uses ''You are a helpful assistant'' as part of its default system prompt. Despite current practices of adding personas to system prompts, it remains unclear how different personas affect a model`s performance on objective tasks. In this study, we present a systematic evaluation of personas in system prompts. We curate a list of 162 roles covering 6 types of interpersonal relationships and 8 domains of expertise. Through extensive analysis of 4 popular families of LLMs and 2,410 factual questions, we demonstrate that adding personas in system prompts does not improve model performance across a range of questions compared to the control setting where no persona is added. Nevertheless, further analysis suggests that the gender, type, and domain of the persona can all influence the resulting prediction accuracies. We further experimented with a list of persona search strategies and found that, while aggregating results from the best persona for each question significantly improves prediction accuracy, automatically identifying the best persona is challenging, with predictions often performing no better than random selection. Overall, our findings suggest that while adding a persona may lead to performance gains in certain settings, the effect of each persona can be largely random. \%Our results can help inform the design of system prompts for AI systems. Code and data are available at https://github.com/Jiaxin-Pei/Prompting-with-Social-Roles.}
}

@misc{zhongQMSumNewBenchmark2021,
  title = {{{QMSum}}: {{A New Benchmark}} for {{Query-based Multi-domain Meeting Summarization}}},
  shorttitle = {{{QMSum}}},
  author = {Zhong, Ming and Yin, Da and Yu, Tao and Zaidi, Ahmad and Mutuma, Mutethia and Jha, Rahul and Awadallah, Ahmed Hassan and Celikyilmaz, Asli and Liu, Yang and Qiu, Xipeng and Radev, Dragomir},
  year = {2021},
  month = apr,
  number = {arXiv:2104.05938},
  eprint = {2104.05938},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.05938},
  urldate = {2022-07-27},
  abstract = {Meetings are a key component of human collaboration. As increasing numbers of meetings are recorded and transcribed, meeting summaries have become essential to remind those who may or may not have attended the meetings about the key decisions made and the tasks to be completed. However, it is hard to create a single short summary that covers all the content of a long meeting involving multiple people and topics. In order to satisfy the needs of different types of users, we define a new query-based multi-domain meeting summarization task, where models have to select and summarize relevant spans of meetings in response to a query, and we introduce QMSum, a new benchmark for this task. QMSum consists of 1,808 query-summary pairs over 232 meetings in multiple domains. Besides, we investigate a locate-then-summarize method and evaluate a set of strong summarization baselines on the task. Experimental results and manual analysis reveal that QMSum presents significant challenges in long meeting summarization for future research. Dataset is available at {\textbackslash}url\{https://github.com/Yale-LILY/QMSum\}.},
  archiveprefix = {arXiv}
}

@misc{zhouLeveragingLargeLanguage2023,
  title = {Leveraging {{Large Language Models}} for {{Enhanced Product Descriptions}} in {{eCommerce}}},
  author = {Zhou, Jianghong and Liu, Bo and Hong, Jhalak Nilesh Acharya Yao and Lee, Kuang-chih and Wen, Musen},
  year = {2023},
  month = oct,
  number = {arXiv:2310.18357},
  eprint = {2310.18357},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2310.18357},
  urldate = {2024-01-20},
  abstract = {In the dynamic field of eCommerce, the quality and comprehensiveness of product descriptions are pivotal for enhancing search visibility and customer engagement. Effective product descriptions can address the 'cold start' problem, align with market trends, and ultimately lead to increased click-through rates. Traditional methods for crafting these descriptions often involve significant human effort and may lack both consistency and scalability. This paper introduces a novel methodology for automating product description generation using the LLAMA 2.0 7B language model. We train the model on a dataset of authentic product descriptions from Walmart, one of the largest eCommerce platforms. The model is then fine-tuned for domain-specific language features and eCommerce nuances to enhance its utility in sales and user engagement. We employ multiple evaluation metrics, including NDCG, customer click-through rates, and human assessments, to validate the effectiveness of our approach. Our findings reveal that the system is not only scalable but also significantly reduces the human workload involved in creating product descriptions. This study underscores the considerable potential of large language models like LLAMA 2.0 7B in automating and optimizing various facets of eCommerce platforms, offering significant business impact, including improved search functionality and increased sales.},
  archiveprefix = {arXiv}
}

@misc{zhouLIMALessMore2023,
  title = {{{LIMA}}: {{Less Is More}} for {{Alignment}}},
  shorttitle = {{{LIMA}}},
  author = {Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srini and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and Zhang, Susan and Ghosh, Gargi and Lewis, Mike and Zettlemoyer, Luke and Levy, Omer},
  year = {2023},
  month = may,
  number = {arXiv:2305.11206},
  eprint = {2305.11206},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.11206},
  urldate = {2024-01-23},
  abstract = {Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43\% of cases; this statistic is as high as 58\% when compared to Bard and 65\% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.},
  archiveprefix = {arXiv}
}

@misc{zhouSurveyEfficientInference2024,
  title = {A {{Survey}} on {{Efficient Inference}} for {{Large Language Models}}},
  author = {Zhou, Zixuan and Ning, Xuefei and Hong, Ke and Fu, Tianyu and Xu, Jiaming and Li, Shiyao and Lou, Yuming and Wang, Luning and Yuan, Zhihang and Li, Xiuhong and Yan, Shengen and Dai, Guohao and Zhang, Xiao-Ping and Dong, Yuhan and Wang, Yu},
  year = {2024},
  month = jul,
  number = {arXiv:2404.14294},
  eprint = {2404.14294},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.14294},
  urldate = {2025-04-07},
  abstract = {Large Language Models (LLMs) have attracted extensive attention due to their remarkable performance across various tasks. However, the substantial computational and memory requirements of LLM inference pose challenges for deployment in resource-constrained scenarios. Efforts within the field have been directed towards developing techniques aimed at enhancing the efficiency of LLM inference. This paper presents a comprehensive survey of the existing literature on efficient LLM inference. We start by analyzing the primary causes of the inefficient LLM inference, i.e., the large model size, the quadratic-complexity attention operation, and the auto-regressive decoding approach. Then, we introduce a comprehensive taxonomy that organizes the current literature into data-level, model-level, and system-level optimization. Moreover, the paper includes comparative experiments on representative methods within critical sub-fields to provide quantitative insights. Last but not least, we provide some knowledge summary and discuss future research directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@misc{zhouSWEETRLTrainingMultiTurn2025,
  title = {{{SWEET-RL}}: {{Training Multi-Turn LLM Agents}} on {{Collaborative Reasoning Tasks}}},
  shorttitle = {{{SWEET-RL}}},
  author = {Zhou, Yifei and Jiang, Song and Tian, Yuandong and Weston, Jason and Levine, Sergey and Sukhbaatar, Sainbayar and Li, Xian},
  year = {2025},
  month = mar,
  number = {arXiv:2503.15478},
  eprint = {2503.15478},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.15478},
  urldate = {2025-06-04},
  abstract = {Large language model (LLM) agents need to perform multi-turn interactions in real-world tasks. However, existing multi-turn RL algorithms for optimizing LLM agents fail to perform effective credit assignment over multiple turns while leveraging the generalization capabilities of LLMs and it remains unclear how to develop such algorithms. To study this, we first introduce a new benchmark, ColBench, where an LLM agent interacts with a human collaborator over multiple turns to solve realistic tasks in backend programming and frontend design. Building on this benchmark, we propose a novel RL algorithm, SWEET-RL (RL with Step-WisE Evaluation from Training-time information), that uses a carefully designed optimization objective to train a critic model with access to additional training-time information. The critic provides step-level rewards for improving the policy model. Our experiments demonstrate that SWEET-RL achieves a 6\% absolute improvement in success and win rates on ColBench compared to other state-of-the-art multi-turn RL algorithms, enabling Llama-3.1-8B to match or exceed the performance of GPT4-o in realistic collaborative content creation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@article{zhouTransformerbasedRepresentationlearningModel2023,
  title = {A Transformer-Based Representation-Learning Model with Unified Processing of Multimodal Input for Clinical Diagnostics},
  author = {Zhou, Hong-Yu and Yu, Yizhou and Wang, Chengdi and Zhang, Shu and Gao, Yuanxu and Pan, Jia and Shao, Jun and Lu, Guangming and Zhang, Kang and Li, Weimin},
  year = {2023},
  month = jun,
  journal = {Nature Biomedical Engineering},
  volume = {7},
  number = {6},
  pages = {743--755},
  publisher = {Nature Publishing Group},
  issn = {2157-846X},
  doi = {10.1038/s41551-023-01045-x},
  urldate = {2024-01-25},
  abstract = {During the diagnostic process, clinicians leverage multimodal information, such as the chief complaint, medical images and laboratory test results. Deep-learning models for aiding diagnosis have yet to meet this requirement of leveraging multimodal information. Here we report a transformer-based representation-learning model as a clinical diagnostic aid that processes multimodal input in a unified manner. Rather than learning modality-specific features, the model leverages embedding layers to convert images and unstructured and structured text into visual tokens and text tokens, and uses bidirectional blocks with intramodal and intermodal attention to learn holistic representations of radiographs, the unstructured chief complaint and clinical history, and structured clinical information such as laboratory test results and patient demographic information. The unified model outperformed an image-only model and non-unified multimodal diagnosis models in the identification of pulmonary disease (by 12\% and 9\%, respectively) and in the prediction of adverse clinical outcomes in patients with COVID-19 (by 29\% and 7\%, respectively). Unified multimodal transformer-based models may help streamline the triaging of patients and facilitate the clinical decision-making process.},
  copyright = {2023 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english}
}

@misc{zhouWebArenaRealisticWeb2024,
  title = {{{WebArena}}: {{A Realistic Web Environment}} for {{Building Autonomous Agents}}},
  shorttitle = {{{WebArena}}},
  author = {Zhou, Shuyan and Xu, Frank F. and Zhu, Hao and Zhou, Xuhui and Lo, Robert and Sridhar, Abishek and Cheng, Xianyi and Ou, Tianyue and Bisk, Yonatan and Fried, Daniel and Alon, Uri and Neubig, Graham},
  year = {2024},
  month = apr,
  number = {arXiv:2307.13854},
  eprint = {2307.13854},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2307.13854},
  urldate = {2024-07-12},
  abstract = {With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41\%, significantly lower than the human performance of 78.24\%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress. Our code, data, environment reproduction resources, and video demonstrations are publicly available at https://webarena.dev/.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{zhuangWorkForceAgentR1IncentivizingReasoning2025,
  title = {{{WorkForceAgent-R1}}: {{Incentivizing Reasoning Capability}} in {{LLM-based Web Agents}} via {{Reinforcement Learning}}},
  shorttitle = {{{WorkForceAgent-R1}}},
  author = {Zhuang, Yuchen and Jin, Di and Chen, Jiaao and Shi, Wenqi and Wang, Hanrui and Zhang, Chao},
  year = {2025},
  month = may,
  number = {arXiv:2505.22942},
  eprint = {2505.22942},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.22942},
  urldate = {2025-06-02},
  abstract = {Large language models (LLMs)-empowered web agents enables automating complex, real-time web navigation tasks in enterprise environments. However, existing web agents relying on supervised fine-tuning (SFT) often struggle with generalization and robustness due to insufficient reasoning capabilities when handling the inherently dynamic nature of web interactions. In this study, we introduce WorkForceAgent-R1, an LLM-based web agent trained using a rule-based R1-style reinforcement learning framework designed explicitly to enhance single-step reasoning and planning for business-oriented web navigation tasks. We employ a structured reward function that evaluates both adherence to output formats and correctness of actions, enabling WorkForceAgent-R1 to implicitly learn robust intermediate reasoning without explicit annotations or extensive expert demonstrations. Extensive experiments on the WorkArena benchmark demonstrate that WorkForceAgent-R1 substantially outperforms SFT baselines by 10.26-16.59\%, achieving competitive performance relative to proprietary LLM-based agents (gpt-4o) in workplace-oriented web navigation tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@article{zhuCalibrationLargeLanguage2023,
  title = {On the {{Calibration}} of {{Large Language Models}} and {{Alignment}}},
  author = {Zhu, Chiwei and Xu, Benfeng and Wang, Quan and Zhang, Yongdong and Mao, Zhendong},
  year = {2023},
  month = nov,
  journal = {Findings of the Association for Computational Linguistics: EMNLP 2023},
  eprint = {2311.13240},
  primaryclass = {cs},
  pages = {9778--9795},
  doi = {10.18653/v1/2023.findings-emnlp.654},
  urldate = {2023-12-20},
  abstract = {As large language models attract increasing attention and find widespread application, concurrent challenges of reliability also arise at the same time. Confidence calibration, an effective analysis method for gauging the reliability of deep models, serves as a crucial tool for assessing and improving their reliability. However, such investigation has been comparatively underexplored. In this work, we conduct a systematic examination of the calibration of aligned language models throughout the entire construction process, including pretraining and alignment training. At each stage, we investigate how different training settings, such as parameter scales and training data, affect model calibration. To thoroughly assess model calibration, we evaluate models on three most concerned aspects: generation, factuality and understanding. Our work sheds light on whether popular LLMs are well-calibrated and how the training process influences model calibration.},
  archiveprefix = {arXiv}
}

@misc{zhuGhostMinecraftGenerally2023,
  title = {Ghost in the {{Minecraft}}: {{Generally Capable Agents}} for {{Open-World Environments}} via {{Large Language Models}} with {{Text-based Knowledge}} and {{Memory}}},
  shorttitle = {Ghost in the {{Minecraft}}},
  author = {Zhu, Xizhou and Chen, Yuntao and Tian, Hao and Tao, Chenxin and Su, Weijie and Yang, Chenyu and Huang, Gao and Li, Bin and Lu, Lewei and Wang, Xiaogang and Qiao, Yu and Zhang, Zhaoxiang and Dai, Jifeng},
  year = {2023},
  month = jun,
  number = {arXiv:2305.17144},
  eprint = {2305.17144},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.17144},
  urldate = {2024-08-08},
  abstract = {The captivating realm of Minecraft has attracted substantial research interest in recent years, serving as a rich platform for developing intelligent agents capable of functioning in open-world environments. However, the current research landscape predominantly focuses on specific objectives, such as the popular "ObtainDiamond" task, and has not yet shown effective generalization to a broader spectrum of tasks. Furthermore, the current leading success rate for the "ObtainDiamond" task stands at around 20\%, highlighting the limitations of Reinforcement Learning (RL) based controllers used in existing methods. To tackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel framework integrates Large Language Models (LLMs) with text-based knowledge and memory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These agents, equipped with the logic and common sense capabilities of LLMs, can skillfully navigate complex, sparse-reward environments with text-based interactions. We develop a set of structured actions and leverage LLMs to generate action plans for the agents to execute. The resulting LLM-based agent markedly surpasses previous methods, achieving a remarkable improvement of +47.5\% in success rate on the "ObtainDiamond" task, demonstrating superior robustness compared to traditional RL-based controllers. Notably, our agent is the first to procure all items in the Minecraft Overworld technology tree, demonstrating its extensive capabilities. GITM does not need any GPU for training, but a single CPU node with 32 CPU cores is enough. This research shows the potential of LLMs in developing capable agents for handling long-horizon, complex tasks and adapting to uncertainties in open-world environments. See the project website at https://github.com/OpenGVLab/GITM.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@misc{zhuJudgeLMFinetunedLarge2023,
  title = {{{JudgeLM}}: {{Fine-tuned Large Language Models}} Are {{Scalable Judges}}},
  shorttitle = {{{JudgeLM}}},
  author = {Zhu, Lianghui and Wang, Xinggang and Wang, Xinlong},
  year = {2023},
  month = oct,
  number = {arXiv:2310.17631},
  eprint = {2310.17631},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.17631},
  urldate = {2023-10-30},
  abstract = {Evaluating Large Language Models (LLMs) in open-ended scenarios is challenging because existing benchmarks and metrics can not measure them comprehensively. To address this problem, we propose to fine-tune LLMs as scalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in open-ended benchmarks. We first propose a comprehensive, large-scale, high-quality dataset containing task seeds, LLMs-generated answers, and GPT-4-generated judgments for fine-tuning high-performance judges, as well as a new benchmark for evaluating the judges. We train JudgeLM at different scales from 7B, 13B, to 33B parameters, and conduct a systematic analysis of its capabilities and behaviors. We then analyze the key biases in fine-tuning LLM as a judge and consider them as position bias, knowledge bias, and format bias. To address these issues, JudgeLM introduces a bag of techniques including swap augmentation, reference support, and reference drop, which clearly enhance the judge's performance. JudgeLM obtains the state-of-the-art judge performance on both the existing PandaLM benchmark and our proposed new benchmark. Our JudgeLM is efficient and the JudgeLM-7B only needs 3 minutes to judge 5K samples with 8 A100 GPUs. JudgeLM obtains high agreement with the teacher judge, achieving an agreement exceeding 90\% that even surpasses human-to-human agreement. JudgeLM also demonstrates extended capabilities in being judges of the single answer, multimodal models, multiple answers, and multi-turn chat.},
  archiveprefix = {arXiv}
}

@misc{zhuPromptingLargeLanguage2024,
  title = {Prompting {{Large Language Models}} for {{Zero-Shot Clinical Prediction}} with {{Structured Longitudinal Electronic Health Record Data}}},
  author = {Zhu, Yinghao and Wang, Zixiang and Gao, Junyi and Tong, Yuning and An, Jingkun and Liao, Weibin and Harrison, Ewen M. and Ma, Liantao and Pan, Chengwei},
  year = {2024},
  month = feb,
  number = {arXiv:2402.01713},
  eprint = {2402.01713},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.01713},
  urldate = {2024-02-16},
  abstract = {The inherent complexity of structured longitudinal Electronic Health Records (EHR) data poses a significant challenge when integrated with Large Language Models (LLMs), which are traditionally tailored for natural language processing. Motivated by the urgent need for swift decision-making during new disease outbreaks, where traditional predictive models often fail due to a lack of historical data, this research investigates the adaptability of LLMs, like GPT-4, to EHR data. We particularly focus on their zero-shot capabilities, which enable them to make predictions in scenarios in which they haven't been explicitly trained. In response to the longitudinal, sparse, and knowledge-infused nature of EHR data, our prompting approach involves taking into account specific EHR characteristics such as units and reference ranges, and employing an in-context learning strategy that aligns with clinical contexts. Our comprehensive experiments on the MIMIC-IV and TJH datasets demonstrate that with our elaborately designed prompting framework, LLMs can improve prediction performance in key tasks such as mortality, length-of-stay, and 30-day readmission by about 35{\textbackslash}\%, surpassing ML models in few-shot settings. Our research underscores the potential of LLMs in enhancing clinical decision-making, especially in urgent healthcare situations like the outbreak of emerging diseases with no labeled data. The code is publicly available at https://github.com/yhzhu99/llm4healthcare for reproducibility.},
  archiveprefix = {arXiv}
}

@misc{zhuSurveyModelCompression2023,
  title = {A {{Survey}} on {{Model Compression}} for {{Large Language Models}}},
  author = {Zhu, Xunyu and Li, Jian and Liu, Yong and Ma, Can and Wang, Weiping},
  year = {2023},
  month = sep,
  number = {arXiv:2308.07633},
  eprint = {2308.07633},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.07633},
  urldate = {2024-02-13},
  abstract = {Large Language Models (LLMs) have revolutionized natural language processing tasks with remarkable success. However, their formidable size and computational demands present significant challenges for practical deployment, especially in resource-constrained environments. As these challenges become increasingly pertinent, the field of model compression has emerged as a pivotal research area to alleviate these limitations. This paper presents a comprehensive survey that navigates the landscape of model compression techniques tailored specifically for LLMs. Addressing the imperative need for efficient deployment, we delve into various methodologies, encompassing quantization, pruning, knowledge distillation, and more. Within each of these techniques, we highlight recent advancements and innovative approaches that contribute to the evolving landscape of LLM research. Furthermore, we explore benchmarking strategies and evaluation metrics that are essential for assessing the effectiveness of compressed LLMs. By providing insights into the latest developments and practical implications, this survey serves as an invaluable resource for both researchers and practitioners. As LLMs continue to evolve, this survey aims to facilitate enhanced efficiency and real-world applicability, establishing a foundation for future advancements in the field.},
  archiveprefix = {arXiv}
}

@misc{zhuTransformingWikipediaAugmented2022,
  title = {Transforming {{Wikipedia}} into {{Augmented Data}} for {{Query-Focused Summarization}}},
  author = {Zhu, Haichao and Dong, Li and Wei, Furu and Qin, Bing and Liu, Ting},
  year = {2022},
  month = jul,
  number = {arXiv:1911.03324},
  eprint = {1911.03324},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/1911.03324},
  urldate = {2022-07-27},
  abstract = {The limited size of existing query-focused summarization datasets renders training data-driven summarization models challenging. Meanwhile, the manual construction of a query-focused summarization corpus is costly and time-consuming. In this paper, we use Wikipedia to automatically collect a large query-focused summarization dataset (named WIKIREF) of more than 280, 000 examples, which can serve as a means of data augmentation. We also develop a BERT-based query-focused summarization model (Q-BERT) to extract sentences from the documents as summaries. To better adapt a huge model containing millions of parameters to tiny benchmarks, we identify and fine-tune only a sparse subnetwork, which corresponds to a small fraction of the whole model parameters. Experimental results on three DUC benchmarks show that the model pre-trained on WIKIREF has already achieved reasonable performance. After fine-tuning on the specific benchmark datasets, the model with data augmentation outperforms strong comparison systems. Moreover, both our proposed Q-BERT model and subnetwork fine-tuning further improve the model performance. The dataset is publicly available at https://aka.ms/wikiref.},
  archiveprefix = {arXiv}
}

@article{zliobaiteActiveLearningDrifting2014,
  title = {Active {{Learning With Drifting Streaming Data}}},
  author = {{\v Z}liobait{\.e}, Indr{\.e} and Bifet, Albert and Pfahringer, Bernhard and Holmes, Geoffrey},
  year = {2014},
  month = jan,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {25},
  number = {1},
  pages = {27--39},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2012.2236570},
  urldate = {2024-06-16},
  abstract = {In learning to classify streaming data, obtaining true labels may require major effort and may incur excessive cost. Active learning focuses on carefully selecting as few labeled instances as possible for learning an accurate predictive model. Streaming data poses additional challenges for active learning, since the data distribution may change over time (concept drift) and models need to adapt. Conventional active learning strategies concentrate on querying the most uncertain instances, which are typically concentrated around the decision boundary. Changes occurring further from the boundary may be missed, and models may fail to adapt. This paper presents a theoretically supported framework for active learning from drifting data streams and develops three active learning strategies for streaming data that explicitly handle concept drift. They are based on uncertainty, dynamic allocation of labeling efforts over time, and randomization of the search space. We empirically demonstrate that these strategies react well to changes that can occur anywhere in the instance space and unexpectedly.},
  keywords = {Active learning,Adaptation models,concept drift,data streams,Labeling,Laboratories,Learning systems,Predictive models,Production,Uncertainty,user feedback}
}
